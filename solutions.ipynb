{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a0d1a5",
   "metadata": {},
   "source": [
    "## 1 Полный перебор для 3х недель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a53ee47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Поиск лучшей стратегии для начального состояния «Отличный»\n",
      "================================================================================\n",
      "Всего возможных стратегий: 27\n",
      "\n",
      "Проверка стратегии 1/27: (0, 0, 0)\n",
      "\n",
      "=== Расчёт для стратегии (0, 0, 0) ===\n",
      "Действия: ['3% скидка', '3% скидка', '3% скидка']\n",
      "Начальное состояние: Отличный\n",
      "Начальное распределение вероятностей: [1. 0. 0.]\n",
      "\n",
      "Месяц 1: Действие = 0 (3% скидка)\n",
      "  Текущие вероятности состояний: [1. 0. 0.]\n",
      "  Из состояния 0 (Отличный) с вероятностью 1.000:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 33.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.500, награда 100, вклад: 50.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 70, вклад: 14.000\n",
      "  Вознаграждение за месяц 1: 97.000\n",
      "  Новые вероятности состояний: [0.3 0.5 0.2]\n",
      "  Накопленное вознаграждение: 97.000\n",
      "\n",
      "Месяц 2: Действие = 0 (3% скидка)\n",
      "  Текущие вероятности состояний: [0.3 0.5 0.2]\n",
      "  Из состояния 0 (Отличный) с вероятностью 0.300:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 9.900\n",
      "    → в состояние 1 (Хороший) с вер. 0.500, награда 100, вклад: 15.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 70, вклад: 4.200\n",
      "  Из состояния 1 (Хороший) с вероятностью 0.500:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 10.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 80, вклад: 24.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 50, вклад: 5.000\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 0.200:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 1.600\n",
      "    → в состояние 1 (Хороший) с вер. 0.200, награда 60, вклад: 2.400\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.700, награда 40, вклад: 5.600\n",
      "  Вознаграждение за месяц 2: 77.700\n",
      "  Новые вероятности состояний: [0.21 0.49 0.3 ]\n",
      "  Накопленное вознаграждение: 174.700\n",
      "\n",
      "Месяц 3: Действие = 0 (3% скидка)\n",
      "  Текущие вероятности состояний: [0.21 0.49 0.3 ]\n",
      "  Из состояния 0 (Отличный) с вероятностью 0.210:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 6.930\n",
      "    → в состояние 1 (Хороший) с вер. 0.500, награда 100, вклад: 10.500\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 70, вклад: 2.940\n",
      "  Из состояния 1 (Хороший) с вероятностью 0.490:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 9.800\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 80, вклад: 23.520\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 50, вклад: 4.900\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 0.300:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 2.400\n",
      "    → в состояние 1 (Хороший) с вер. 0.200, награда 60, вклад: 3.600\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.700, награда 40, вклад: 8.400\n",
      "  Вознаграждение за месяц 3: 72.990\n",
      "  Новые вероятности состояний: [0.191 0.459 0.35 ]\n",
      "  Накопленное вознаграждение: 247.690\n",
      "\n",
      "Итоговое ожидаемое вознаграждение для стратегии (0, 0, 0): 247.690\n",
      "Улучшение #1: стратегия (0, 0, 0) = ['3% скидка', '3% скидка', '3% скидка'], вознаграждение: 247.69 (+247.69)\n",
      "Улучшение #2: стратегия (0, 0, 1) = ['3% скидка', '3% скидка', 'Бесплатная доставка'], вознаграждение: 262.75 (+15.06)\n",
      "Улучшение #3: стратегия (0, 1, 1) = ['3% скидка', 'Бесплатная доставка', 'Бесплатная доставка'], вознаграждение: 272.55 (+9.80)\n",
      "Улучшение #4: стратегия (1, 1, 1) = ['Бесплатная доставка', 'Бесплатная доставка', 'Бесплатная доставка'], вознаграждение: 278.40 (+5.85)\n",
      "\n",
      "Проверка стратегии 27/27: (2, 2, 2)\n",
      "\n",
      "=== Расчёт для стратегии (2, 2, 2) ===\n",
      "Действия: ['Ничего', 'Ничего', 'Ничего']\n",
      "Начальное состояние: Отличный\n",
      "Начальное распределение вероятностей: [1. 0. 0.]\n",
      "\n",
      "Месяц 1: Действие = 2 (Ничего)\n",
      "  Текущие вероятности состояний: [1. 0. 0.]\n",
      "  Из состояния 0 (Отличный) с вероятностью 1.000:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 33.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.400, награда 80, вклад: 32.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.300, награда 50, вклад: 15.000\n",
      "  Вознаграждение за месяц 1: 80.000\n",
      "  Новые вероятности состояний: [0.3 0.4 0.3]\n",
      "  Накопленное вознаграждение: 80.000\n",
      "\n",
      "Месяц 2: Действие = 2 (Ничего)\n",
      "  Текущие вероятности состояний: [0.3 0.4 0.3]\n",
      "  Из состояния 0 (Отличный) с вероятностью 0.300:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 9.900\n",
      "    → в состояние 1 (Хороший) с вер. 0.400, награда 80, вклад: 9.600\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.300, награда 50, вклад: 4.500\n",
      "  Из состояния 1 (Хороший) с вероятностью 0.400:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 8.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 60, вклад: 14.400\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 40, вклад: 3.200\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 0.300:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 2.400\n",
      "    → в состояние 1 (Хороший) с вер. 0.300, награда 70, вклад: 6.300\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.600, награда 60, вклад: 10.800\n",
      "  Вознаграждение за месяц 2: 69.100\n",
      "  Новые вероятности состояний: [0.2  0.45 0.35]\n",
      "  Накопленное вознаграждение: 149.100\n",
      "\n",
      "Месяц 3: Действие = 2 (Ничего)\n",
      "  Текущие вероятности состояний: [0.2  0.45 0.35]\n",
      "  Из состояния 0 (Отличный) с вероятностью 0.200:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 6.600\n",
      "    → в состояние 1 (Хороший) с вер. 0.400, награда 80, вклад: 6.400\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.300, награда 50, вклад: 3.000\n",
      "  Из состояния 1 (Хороший) с вероятностью 0.450:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 9.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 60, вклад: 16.200\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 40, вклад: 3.600\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 0.350:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 2.800\n",
      "    → в состояние 1 (Хороший) с вер. 0.300, награда 70, вклад: 7.350\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.600, награда 60, вклад: 12.600\n",
      "  Вознаграждение за месяц 3: 67.550\n",
      "  Новые вероятности состояний: [0.185 0.455 0.36 ]\n",
      "  Накопленное вознаграждение: 216.650\n",
      "\n",
      "Итоговое ожидаемое вознаграждение для стратегии (2, 2, 2): 216.650\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Для начального состояния «Отличный»:\n",
      "  Лучшая стратегия действий: (1, 1, 1) = ['Ничего', 'Ничего', 'Ничего']\n",
      "  Ожидаемое вознаграждение: 278.40\n",
      "  Количество улучшений: 4 из 27 оцененных стратегий\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Поиск лучшей стратегии для начального состояния «Хороший»\n",
      "================================================================================\n",
      "Всего возможных стратегий: 27\n",
      "\n",
      "Проверка стратегии 1/27: (0, 0, 0)\n",
      "\n",
      "=== Расчёт для стратегии (0, 0, 0) ===\n",
      "Действия: ['3% скидка', '3% скидка', '3% скидка']\n",
      "Начальное состояние: Хороший\n",
      "Начальное распределение вероятностей: [0. 1. 0.]\n",
      "\n",
      "Месяц 1: Действие = 0 (3% скидка)\n",
      "  Текущие вероятности состояний: [0. 1. 0.]\n",
      "  Из состояния 1 (Хороший) с вероятностью 1.000:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 20.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 80, вклад: 48.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 50, вклад: 10.000\n",
      "  Вознаграждение за месяц 1: 78.000\n",
      "  Новые вероятности состояний: [0.2 0.6 0.2]\n",
      "  Накопленное вознаграждение: 78.000\n",
      "\n",
      "Месяц 2: Действие = 0 (3% скидка)\n",
      "  Текущие вероятности состояний: [0.2 0.6 0.2]\n",
      "  Из состояния 0 (Отличный) с вероятностью 0.200:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 6.600\n",
      "    → в состояние 1 (Хороший) с вер. 0.500, награда 100, вклад: 10.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 70, вклад: 2.800\n",
      "  Из состояния 1 (Хороший) с вероятностью 0.600:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 12.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 80, вклад: 28.800\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 50, вклад: 6.000\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 0.200:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 1.600\n",
      "    → в состояние 1 (Хороший) с вер. 0.200, награда 60, вклад: 2.400\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.700, награда 40, вклад: 5.600\n",
      "  Вознаграждение за месяц 2: 75.800\n",
      "  Новые вероятности состояний: [0.2 0.5 0.3]\n",
      "  Накопленное вознаграждение: 153.800\n",
      "\n",
      "Месяц 3: Действие = 0 (3% скидка)\n",
      "  Текущие вероятности состояний: [0.2 0.5 0.3]\n",
      "  Из состояния 0 (Отличный) с вероятностью 0.200:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 6.600\n",
      "    → в состояние 1 (Хороший) с вер. 0.500, награда 100, вклад: 10.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 70, вклад: 2.800\n",
      "  Из состояния 1 (Хороший) с вероятностью 0.500:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 10.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 80, вклад: 24.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 50, вклад: 5.000\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 0.300:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 2.400\n",
      "    → в состояние 1 (Хороший) с вер. 0.200, награда 60, вклад: 3.600\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.700, награда 40, вклад: 8.400\n",
      "  Вознаграждение за месяц 3: 72.800\n",
      "  Новые вероятности состояний: [0.19 0.46 0.35]\n",
      "  Накопленное вознаграждение: 226.600\n",
      "\n",
      "Итоговое ожидаемое вознаграждение для стратегии (0, 0, 0): 226.600\n",
      "Улучшение #1: стратегия (0, 0, 0) = ['3% скидка', '3% скидка', '3% скидка'], вознаграждение: 226.60 (+226.60)\n",
      "Улучшение #2: стратегия (0, 0, 1) = ['3% скидка', '3% скидка', 'Бесплатная доставка'], вознаграждение: 241.80 (+15.20)\n",
      "Улучшение #3: стратегия (0, 1, 1) = ['3% скидка', 'Бесплатная доставка', 'Бесплатная доставка'], вознаграждение: 251.80 (+10.00)\n",
      "Улучшение #4: стратегия (1, 1, 1) = ['Бесплатная доставка', 'Бесплатная доставка', 'Бесплатная доставка'], вознаграждение: 257.25 (+5.45)\n",
      "\n",
      "Проверка стратегии 27/27: (2, 2, 2)\n",
      "\n",
      "=== Расчёт для стратегии (2, 2, 2) ===\n",
      "Действия: ['Ничего', 'Ничего', 'Ничего']\n",
      "Начальное состояние: Хороший\n",
      "Начальное распределение вероятностей: [0. 1. 0.]\n",
      "\n",
      "Месяц 1: Действие = 2 (Ничего)\n",
      "  Текущие вероятности состояний: [0. 1. 0.]\n",
      "  Из состояния 1 (Хороший) с вероятностью 1.000:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 20.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 60, вклад: 36.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 40, вклад: 8.000\n",
      "  Вознаграждение за месяц 1: 64.000\n",
      "  Новые вероятности состояний: [0.2 0.6 0.2]\n",
      "  Накопленное вознаграждение: 64.000\n",
      "\n",
      "Месяц 2: Действие = 2 (Ничего)\n",
      "  Текущие вероятности состояний: [0.2 0.6 0.2]\n",
      "  Из состояния 0 (Отличный) с вероятностью 0.200:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 6.600\n",
      "    → в состояние 1 (Хороший) с вер. 0.400, награда 80, вклад: 6.400\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.300, награда 50, вклад: 3.000\n",
      "  Из состояния 1 (Хороший) с вероятностью 0.600:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 12.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 60, вклад: 21.600\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 40, вклад: 4.800\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 0.200:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 1.600\n",
      "    → в состояние 1 (Хороший) с вер. 0.300, награда 70, вклад: 4.200\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.600, награда 60, вклад: 7.200\n",
      "  Вознаграждение за месяц 2: 67.400\n",
      "  Новые вероятности состояний: [0.2 0.5 0.3]\n",
      "  Накопленное вознаграждение: 131.400\n",
      "\n",
      "Месяц 3: Действие = 2 (Ничего)\n",
      "  Текущие вероятности состояний: [0.2 0.5 0.3]\n",
      "  Из состояния 0 (Отличный) с вероятностью 0.200:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 6.600\n",
      "    → в состояние 1 (Хороший) с вер. 0.400, награда 80, вклад: 6.400\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.300, награда 50, вклад: 3.000\n",
      "  Из состояния 1 (Хороший) с вероятностью 0.500:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 10.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 60, вклад: 18.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 40, вклад: 4.000\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 0.300:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 2.400\n",
      "    → в состояние 1 (Хороший) с вер. 0.300, награда 70, вклад: 6.300\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.600, награда 60, вклад: 10.800\n",
      "  Вознаграждение за месяц 3: 67.500\n",
      "  Новые вероятности состояний: [0.19 0.47 0.34]\n",
      "  Накопленное вознаграждение: 198.900\n",
      "\n",
      "Итоговое ожидаемое вознаграждение для стратегии (2, 2, 2): 198.900\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Для начального состояния «Хороший»:\n",
      "  Лучшая стратегия действий: (1, 1, 1) = ['Ничего', 'Ничего', 'Ничего']\n",
      "  Ожидаемое вознаграждение: 257.25\n",
      "  Количество улучшений: 4 из 27 оцененных стратегий\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Поиск лучшей стратегии для начального состояния «Удовлетворительный»\n",
      "================================================================================\n",
      "Всего возможных стратегий: 27\n",
      "\n",
      "Проверка стратегии 1/27: (0, 0, 0)\n",
      "\n",
      "=== Расчёт для стратегии (0, 0, 0) ===\n",
      "Действия: ['3% скидка', '3% скидка', '3% скидка']\n",
      "Начальное состояние: Удовлетворительный\n",
      "Начальное распределение вероятностей: [0. 0. 1.]\n",
      "\n",
      "Месяц 1: Действие = 0 (3% скидка)\n",
      "  Текущие вероятности состояний: [0. 0. 1.]\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 1.000:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 8.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.200, награда 60, вклад: 12.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.700, награда 40, вклад: 28.000\n",
      "  Вознаграждение за месяц 1: 48.000\n",
      "  Новые вероятности состояний: [0.1 0.2 0.7]\n",
      "  Накопленное вознаграждение: 48.000\n",
      "\n",
      "Месяц 2: Действие = 0 (3% скидка)\n",
      "  Текущие вероятности состояний: [0.1 0.2 0.7]\n",
      "  Из состояния 0 (Отличный) с вероятностью 0.100:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 3.300\n",
      "    → в состояние 1 (Хороший) с вер. 0.500, награда 100, вклад: 5.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 70, вклад: 1.400\n",
      "  Из состояния 1 (Хороший) с вероятностью 0.200:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 4.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 80, вклад: 9.600\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 50, вклад: 2.000\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 0.700:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 5.600\n",
      "    → в состояние 1 (Хороший) с вер. 0.200, награда 60, вклад: 8.400\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.700, награда 40, вклад: 19.600\n",
      "  Вознаграждение за месяц 2: 58.900\n",
      "  Новые вероятности состояний: [0.14 0.31 0.55]\n",
      "  Накопленное вознаграждение: 106.900\n",
      "\n",
      "Месяц 3: Действие = 0 (3% скидка)\n",
      "  Текущие вероятности состояний: [0.14 0.31 0.55]\n",
      "  Из состояния 0 (Отличный) с вероятностью 0.140:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 4.620\n",
      "    → в состояние 1 (Хороший) с вер. 0.500, награда 100, вклад: 7.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 70, вклад: 1.960\n",
      "  Из состояния 1 (Хороший) с вероятностью 0.310:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 6.200\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 80, вклад: 14.880\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 50, вклад: 3.100\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 0.550:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 4.400\n",
      "    → в состояние 1 (Хороший) с вер. 0.200, награда 60, вклад: 6.600\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.700, награда 40, вклад: 15.400\n",
      "  Вознаграждение за месяц 3: 64.160\n",
      "  Новые вероятности состояний: [0.159 0.366 0.475]\n",
      "  Накопленное вознаграждение: 171.060\n",
      "\n",
      "Итоговое ожидаемое вознаграждение для стратегии (0, 0, 0): 171.060\n",
      "Улучшение #1: стратегия (0, 0, 0) = ['3% скидка', '3% скидка', '3% скидка'], вознаграждение: 171.06 (+171.06)\n",
      "Улучшение #2: стратегия (0, 0, 1) = ['3% скидка', '3% скидка', 'Бесплатная доставка'], вознаграждение: 187.10 (+16.04)\n",
      "Улучшение #3: стратегия (0, 1, 1) = ['3% скидка', 'Бесплатная доставка', 'Бесплатная доставка'], вознаграждение: 202.05 (+14.95)\n",
      "Улучшение #4: стратегия (1, 0, 1) = ['Бесплатная доставка', '3% скидка', 'Бесплатная доставка'], вознаграждение: 205.10 (+3.05)\n",
      "Улучшение #5: стратегия (1, 1, 1) = ['Бесплатная доставка', 'Бесплатная доставка', 'Бесплатная доставка'], вознаграждение: 220.05 (+14.95)\n",
      "Улучшение #6: стратегия (2, 1, 1) = ['Ничего', 'Бесплатная доставка', 'Бесплатная доставка'], вознаграждение: 222.65 (+2.60)\n",
      "\n",
      "Проверка стратегии 27/27: (2, 2, 2)\n",
      "\n",
      "=== Расчёт для стратегии (2, 2, 2) ===\n",
      "Действия: ['Ничего', 'Ничего', 'Ничего']\n",
      "Начальное состояние: Удовлетворительный\n",
      "Начальное распределение вероятностей: [0. 0. 1.]\n",
      "\n",
      "Месяц 1: Действие = 2 (Ничего)\n",
      "  Текущие вероятности состояний: [0. 0. 1.]\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 1.000:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 8.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.300, награда 70, вклад: 21.000\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.600, награда 60, вклад: 36.000\n",
      "  Вознаграждение за месяц 1: 65.000\n",
      "  Новые вероятности состояний: [0.1 0.3 0.6]\n",
      "  Накопленное вознаграждение: 65.000\n",
      "\n",
      "Месяц 2: Действие = 2 (Ничего)\n",
      "  Текущие вероятности состояний: [0.1 0.3 0.6]\n",
      "  Из состояния 0 (Отличный) с вероятностью 0.100:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 3.300\n",
      "    → в состояние 1 (Хороший) с вер. 0.400, награда 80, вклад: 3.200\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.300, награда 50, вклад: 1.500\n",
      "  Из состояния 1 (Хороший) с вероятностью 0.300:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 6.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 60, вклад: 10.800\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 40, вклад: 2.400\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 0.600:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 4.800\n",
      "    → в состояние 1 (Хороший) с вер. 0.300, награда 70, вклад: 12.600\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.600, награда 60, вклад: 21.600\n",
      "  Вознаграждение за месяц 2: 66.200\n",
      "  Новые вероятности состояний: [0.15 0.4  0.45]\n",
      "  Накопленное вознаграждение: 131.200\n",
      "\n",
      "Месяц 3: Действие = 2 (Ничего)\n",
      "  Текущие вероятности состояний: [0.15 0.4  0.45]\n",
      "  Из состояния 0 (Отличный) с вероятностью 0.150:\n",
      "    → в состояние 0 (Отличный) с вер. 0.300, награда 110, вклад: 4.950\n",
      "    → в состояние 1 (Хороший) с вер. 0.400, награда 80, вклад: 4.800\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.300, награда 50, вклад: 2.250\n",
      "  Из состояния 1 (Хороший) с вероятностью 0.400:\n",
      "    → в состояние 0 (Отличный) с вер. 0.200, награда 100, вклад: 8.000\n",
      "    → в состояние 1 (Хороший) с вер. 0.600, награда 60, вклад: 14.400\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.200, награда 40, вклад: 3.200\n",
      "  Из состояния 2 (Удовлетворительный) с вероятностью 0.450:\n",
      "    → в состояние 0 (Отличный) с вер. 0.100, награда 80, вклад: 3.600\n",
      "    → в состояние 1 (Хороший) с вер. 0.300, награда 70, вклад: 9.450\n",
      "    → в состояние 2 (Удовлетворительный) с вер. 0.600, награда 60, вклад: 16.200\n",
      "  Вознаграждение за месяц 3: 66.850\n",
      "  Новые вероятности состояний: [0.17  0.435 0.395]\n",
      "  Накопленное вознаграждение: 198.050\n",
      "\n",
      "Итоговое ожидаемое вознаграждение для стратегии (2, 2, 2): 198.050\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Для начального состояния «Удовлетворительный»:\n",
      "  Лучшая стратегия действий: (2, 1, 1) = ['Ничего', 'Ничего', 'Ничего']\n",
      "  Ожидаемое вознаграждение: 222.65\n",
      "  Количество улучшений: 6 из 27 оцененных стратегий\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "======================================== Ответ ========================================\n",
      "Для начального состояния «Отличный»:\n",
      "  Лучшая стратегия действий: (1, 1, 1) = ['Бесплатная доставка', 'Бесплатная доставка', 'Бесплатная доставка']\n",
      "  Ожидаемое вознаграждение: 278.40\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Для начального состояния «Хороший»:\n",
      "  Лучшая стратегия действий: (1, 1, 1) = ['Бесплатная доставка', 'Бесплатная доставка', 'Бесплатная доставка']\n",
      "  Ожидаемое вознаграждение: 257.25\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Для начального состояния «Удовлетворительный»:\n",
      "  Лучшая стратегия действий: (2, 1, 1) = ['Ничего', 'Бесплатная доставка', 'Бесплатная доставка']\n",
      "  Ожидаемое вознаграждение: 222.65\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# Матрицы вероятностей переходов P[action][state_from][state_to]\n",
    "transition_probabilities = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "# Матрицы вознаграждений R[action][state_from][state_to]\n",
    "rewards = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "actions = [0, 1, 2]  # Доступные действия (индексы)\n",
    "states = [0, 1, 2]   # Индексы состояний: 0 - «Отличный», 1 - «Хороший», 2 - «Удовлетворительный»\n",
    "months = 3  # Количество месяцев (длительность стратегии)\n",
    "state_names = [\"Отличный\", \"Хороший\", \"Удовлетворительный\"]  # Человекочитаемые имена состояний\n",
    "action_names = [\"3% скидка\", \"Бесплатная доставка\", \"Ничего\"]  # Названия действий\n",
    "\n",
    "def calculate_expected_reward(strategy, initial_state, verbose=False):\n",
    "    \"\"\"\n",
    "    Вычисление ожидаемого вознаграждения для заданной стратегии действий.\n",
    "    \n",
    "    Параметры:\n",
    "    strategy (list[int]): Стратегия действий (порядок действий на каждый месяц).\n",
    "    initial_state (int): Начальное состояние системы.\n",
    "    verbose (bool): Выводить ли промежуточные результаты.\n",
    "    \n",
    "    Возвращает:\n",
    "    float: Ожидаемое вознаграждение для стратегии.\n",
    "    \"\"\"\n",
    "    # Инициализация вероятностей состояний: на старте мы находимся в начальном состоянии\n",
    "    state_probabilities = [0.0, 0.0, 0.0]\n",
    "    state_probabilities[initial_state] = 1.0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n=== Расчёт для стратегии {strategy} ===\")\n",
    "        print(f\"Действия: {[action_names[a] for a in strategy]}\")\n",
    "        print(f\"Начальное состояние: {state_names[initial_state]}\")\n",
    "        print(f\"Начальное распределение вероятностей: {np.round(state_probabilities, 3)}\")\n",
    "    \n",
    "    total_reward = 0.0  # Общее ожидаемое вознаграждение\n",
    "    \n",
    "    # Процесс для каждого действия в стратегии\n",
    "    for month, action in enumerate(strategy):\n",
    "        if verbose:\n",
    "            print(f\"\\nМесяц {month+1}: Действие = {action} ({action_names[action]})\")\n",
    "            print(f\"  Текущие вероятности состояний: {np.round(state_probabilities, 3)}\")\n",
    "        \n",
    "        step_reward = 0.0  # Вознаграждение за текущий месяц\n",
    "        new_state_probabilities = [0.0, 0.0, 0.0]  # Вероятности для нового состояния\n",
    "        \n",
    "        # Проходим по всем возможным переходам между состояниями\n",
    "        for i in range(3):\n",
    "            if state_probabilities[i] > 0:  # Оптимизация: показываем только значимые переходы\n",
    "                if verbose:\n",
    "                    print(f\"  Из состояния {i} ({state_names[i]}) с вероятностью {state_probabilities[i]:.3f}:\")\n",
    "                \n",
    "                for j in range(3):\n",
    "                    transition_probability = transition_probabilities[action][i][j]\n",
    "                    reward = rewards[action][i][j]\n",
    "                    contribution = state_probabilities[i] * transition_probability * reward\n",
    "                    \n",
    "                    if contribution > 0 and verbose:\n",
    "                        print(f\"    → в состояние {j} ({state_names[j]}) с вер. {transition_probability:.3f}, \"\n",
    "                              f\"награда {reward}, вклад: {contribution:.3f}\")\n",
    "                    \n",
    "                    total_reward += contribution\n",
    "                    step_reward += contribution\n",
    "                    new_state_probabilities[j] += state_probabilities[i] * transition_probability\n",
    "        \n",
    "        # Обновляем вероятности состояний для следующего шага\n",
    "        state_probabilities = new_state_probabilities\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Вознаграждение за месяц {month+1}: {step_reward:.3f}\")\n",
    "            print(f\"  Новые вероятности состояний: {np.round(state_probabilities, 3)}\")\n",
    "            print(f\"  Накопленное вознаграждение: {total_reward:.3f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nИтоговое ожидаемое вознаграждение для стратегии {strategy}: {total_reward:.3f}\")\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "best_strategies = {0: None, 1: None, 2: None}  # Словарь для хранения лучших стратегий\n",
    "best_rewards = {0: -1e9, 1: -1e9, 2: -1e9}  # Словарь для хранения лучших вознаграждений\n",
    "\n",
    "# Поиск наилучшей стратегии для каждого начального состояния\n",
    "for initial_state in range(3):\n",
    "    best_strategy = None\n",
    "    best_reward = -1e9  # Начальная очень низкая оценка\n",
    "    strategies_evaluated = 0\n",
    "    improvements = 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Поиск лучшей стратегии для начального состояния «{state_names[initial_state]}»\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Подсчет общего количества стратегий\n",
    "    total_strategies = len(actions) ** months\n",
    "    print(f\"Всего возможных стратегий: {total_strategies}\")\n",
    "    \n",
    "    # Генерация всех возможных стратегий на протяжении заданного количества месяцев\n",
    "    for strategy in itertools.product(actions, repeat=months):\n",
    "        strategies_evaluated += 1\n",
    "        \n",
    "        # Подробный вывод для первой и последней стратегии\n",
    "        verbose = strategies_evaluated == 1 or strategies_evaluated == total_strategies\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nПроверка стратегии {strategies_evaluated}/{total_strategies}: {strategy}\")\n",
    "        \n",
    "        expected_reward = calculate_expected_reward(strategy, initial_state, verbose=verbose)\n",
    "        \n",
    "        # Обновляем лучшую стратегию, если текущая дает большее вознаграждение\n",
    "        if expected_reward > best_reward:\n",
    "            improvement = expected_reward - best_reward if best_reward != -1e9 else expected_reward\n",
    "            improvements += 1\n",
    "            print(f\"Улучшение #{improvements}: стратегия {strategy} = {[action_names[a] for a in strategy]}, \"\n",
    "                  f\"вознаграждение: {expected_reward:.2f} (+{improvement:.2f})\")\n",
    "            best_strategy = strategy\n",
    "            best_reward = expected_reward\n",
    "            best_strategies[initial_state] = best_strategy\n",
    "            best_rewards[initial_state] = best_reward\n",
    "    \n",
    "    # Выводим результаты для каждого начального состояния\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Для начального состояния «{state_names[initial_state]}»:\")\n",
    "    print(f\"  Лучшая стратегия действий: {best_strategy} = {[action_names[a] for a in strategy]}\")\n",
    "    print(f\"  Ожидаемое вознаграждение: {best_reward:.2f}\")\n",
    "    print(f\"  Количество улучшений: {improvements} из {strategies_evaluated} оцененных стратегий\")\n",
    "    print(f\"{'-'*80}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*40} Ответ {'='*40}\")\n",
    "for initial_state in range(3):\n",
    "    print(f\"Для начального состояния «{state_names[initial_state]}»:\")\n",
    "    print(f\"  Лучшая стратегия действий: {best_strategies[initial_state]} = \"\n",
    "          f\"{[action_names[a] for a in best_strategies[initial_state]]}\")\n",
    "    print(f\"  Ожидаемое вознаграждение: {best_rewards[initial_state]:.2f}\")\n",
    "    print(f\"{'-'*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e245d2b6",
   "metadata": {},
   "source": [
    "___\n",
    "## 2 Полный перебор для бесконечного горизонта планирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3473d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего возможных политик: 27\n",
      "\n",
      "================================================================================\n",
      "Политика 1/27: [0, 0, 0] [s0→0(3% скидка), s1→0(3% скидка), s2→0(3% скидка)]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Оценка политики: [0, 0, 0] [s0→a0(3% скидка), s1→a0(3% скидка), s2→a0(3% скидка)]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Состояние 0 (Отличный), выбрано действие 0 (3% скидка):\n",
      "  Вероятности переходов P[0,:] = [0.3 0.5 0.2]\n",
      "  Переход в 0 (Отличный): вероятность = 0.300, награда = 110, вклад = 33.000\n",
      "  Переход в 1 (Хороший): вероятность = 0.500, награда = 100, вклад = 50.000\n",
      "  Переход в 2 (Удовлетворительный): вероятность = 0.200, награда = 70, вклад = 14.000\n",
      "  Суммарное ожидаемое вознаграждение r[0] = 97.000\n",
      "\n",
      "Состояние 1 (Хороший), выбрано действие 0 (3% скидка):\n",
      "  Вероятности переходов P[1,:] = [0.2 0.6 0.2]\n",
      "  Переход в 0 (Отличный): вероятность = 0.200, награда = 100, вклад = 20.000\n",
      "  Переход в 1 (Хороший): вероятность = 0.600, награда = 80, вклад = 48.000\n",
      "  Переход в 2 (Удовлетворительный): вероятность = 0.200, награда = 50, вклад = 10.000\n",
      "  Суммарное ожидаемое вознаграждение r[1] = 78.000\n",
      "\n",
      "Состояние 2 (Удовлетворительный), выбрано действие 0 (3% скидка):\n",
      "  Вероятности переходов P[2,:] = [0.1 0.2 0.7]\n",
      "  Переход в 0 (Отличный): вероятность = 0.100, награда = 80, вклад = 8.000\n",
      "  Переход в 1 (Хороший): вероятность = 0.200, награда = 60, вклад = 12.000\n",
      "  Переход в 2 (Удовлетворительный): вероятность = 0.700, награда = 40, вклад = 28.000\n",
      "  Суммарное ожидаемое вознаграждение r[2] = 48.000\n",
      "\n",
      "Матрица переходов для политики:\n",
      "  [0.3 0.5 0.2]\n",
      "  [0.2 0.6 0.2]\n",
      "  [0.1 0.2 0.7]\n",
      "\n",
      "Вектор вознаграждений для политики:\n",
      "  [97. 78. 48.]\n",
      "\n",
      "Собственные значения матрицы переходов:\n",
      "  λ0 = 1.000000 (|λ0 - 1| = 0.000000)\n",
      "  λ1 = 0.100000 (|λ1 - 1| = 0.900000)\n",
      "  λ2 = 0.500000 (|λ2 - 1| = 0.500000)\n",
      "\n",
      "Индекс собственного значения, ближайшего к 1: 0\n",
      "Соответствующий собственный вектор (ненормированный):\n",
      "  [-0.29231364 -0.69424489 -0.65770569]\n",
      "\n",
      "Стационарное распределение состояний (нормированное):\n",
      "  μ(0) = 0.177778 (Отличный)\n",
      "  μ(1) = 0.422222 (Хороший)\n",
      "  μ(2) = 0.400000 (Удовлетворительный)\n",
      "\n",
      "Расчет среднего дохода:\n",
      "  Состояние 0: μ(0) * r(0) = 0.177778 * 97.000 = 17.244444\n",
      "  Состояние 1: μ(1) * r(1) = 0.422222 * 78.000 = 32.933333\n",
      "  Состояние 2: μ(2) * r(2) = 0.400000 * 48.000 = 19.200000\n",
      "\n",
      "Средний доход g = μ⋅r = 69.377778\n",
      "\n",
      "Политика [0, 0, 0]: средний доход g = 69.377778\n",
      "УЛУЧШЕНИЕ #1: +1000000069.377778 (было -1000000000.000000, стало 69.377778)\n",
      "\n",
      "================================================================================\n",
      "Политика 2/27: [0, 0, 1] [s0→0(3% скидка), s1→0(3% скидка), s2→1(Бесплатная доставка)]\n",
      "\n",
      "Политика [0, 0, 1]: средний доход g = 76.577778\n",
      "УЛУЧШЕНИЕ #2: +7.200000 (было 69.377778, стало 76.577778)\n",
      "\n",
      "================================================================================\n",
      "Политика 3/27: [0, 0, 2] [s0→0(3% скидка), s1→0(3% скидка), s2→2(Ничего)]\n",
      "\n",
      "Политика [0, 0, 2]: средний доход g = 77.185185\n",
      "УЛУЧШЕНИЕ #3: +0.607407 (было 76.577778, стало 77.185185)\n",
      "\n",
      "================================================================================\n",
      "Политика 4/27: [0, 1, 0] [s0→0(3% скидка), s1→1(Бесплатная доставка), s2→0(3% скидка)]\n",
      "\n",
      "Политика [0, 1, 0]: средний доход g = 68.375000\n",
      "\n",
      "================================================================================\n",
      "Политика 5/27: [0, 1, 1] [s0→0(3% скидка), s1→1(Бесплатная доставка), s2→1(Бесплатная доставка)]\n",
      "\n",
      "Политика [0, 1, 1]: средний доход g = 78.781250\n",
      "УЛУЧШЕНИЕ #4: +1.596065 (было 77.185185, стало 78.781250)\n",
      "\n",
      "================================================================================\n",
      "Политика 6/27: [0, 1, 2] [s0→0(3% скидка), s1→1(Бесплатная доставка), s2→2(Ничего)]\n",
      "\n",
      "Политика [0, 1, 2]: средний доход g = 80.194444\n",
      "УЛУЧШЕНИЕ #5: +1.413194 (было 78.781250, стало 80.194444)\n",
      "\n",
      "================================================================================\n",
      "Политика 7/27: [0, 2, 0] [s0→0(3% скидка), s1→2(Ничего), s2→0(3% скидка)]\n",
      "\n",
      "Политика [0, 2, 0]: средний доход g = 63.466667\n",
      "\n",
      "================================================================================\n",
      "Политика 8/27: [0, 2, 1] [s0→0(3% скидка), s1→2(Ничего), s2→1(Бесплатная доставка)]\n",
      "\n",
      "Политика [0, 2, 1]: средний доход g = 70.666667\n",
      "\n",
      "================================================================================\n",
      "Политика 9/27: [0, 2, 2] [s0→0(3% скидка), s1→2(Ничего), s2→2(Ничего)]\n",
      "\n",
      "Политика [0, 2, 2]: средний доход g = 70.444444\n",
      "\n",
      "================================================================================\n",
      "Политика 10/27: [1, 0, 0] [s0→1(Бесплатная доставка), s1→0(3% скидка), s2→0(3% скидка)]\n",
      "\n",
      "Политика [1, 0, 0]: средний доход g = 70.734694\n",
      "\n",
      "================================================================================\n",
      "Политика 11/27: [1, 0, 1] [s0→1(Бесплатная доставка), s1→0(3% скидка), s2→1(Бесплатная доставка)]\n",
      "\n",
      "Политика [1, 0, 1]: средний доход g = 77.346939\n",
      "\n",
      "================================================================================\n",
      "Политика 12/27: [1, 0, 2] [s0→1(Бесплатная доставка), s1→0(3% скидка), s2→2(Ничего)]\n",
      "\n",
      "Политика [1, 0, 2]: средний доход g = 77.932203\n",
      "\n",
      "================================================================================\n",
      "Политика 13/27: [1, 1, 0] [s0→1(Бесплатная доставка), s1→1(Бесплатная доставка), s2→0(3% скидка)]\n",
      "\n",
      "Политика [1, 1, 0]: средний доход g = 69.222222\n",
      "\n",
      "================================================================================\n",
      "Политика 14/27: [1, 1, 1] [s0→1(Бесплатная доставка), s1→1(Бесплатная доставка), s2→1(Бесплатная доставка)]\n",
      "\n",
      "Политика [1, 1, 1]: средний доход g = 79.472222\n",
      "\n",
      "================================================================================\n",
      "Политика 15/27: [1, 1, 2] [s0→1(Бесплатная доставка), s1→1(Бесплатная доставка), s2→2(Ничего)]\n",
      "\n",
      "Политика [1, 1, 2]: средний доход g = 80.864198\n",
      "УЛУЧШЕНИЕ #6: +0.669753 (было 80.194444, стало 80.864198)\n",
      "\n",
      "================================================================================\n",
      "Политика 16/27: [1, 2, 0] [s0→1(Бесплатная доставка), s1→2(Ничего), s2→0(3% скидка)]\n",
      "\n",
      "Политика [1, 2, 0]: средний доход g = 64.163265\n",
      "\n",
      "================================================================================\n",
      "Политика 17/27: [1, 2, 1] [s0→1(Бесплатная доставка), s1→2(Ничего), s2→1(Бесплатная доставка)]\n",
      "\n",
      "Политика [1, 2, 1]: средний доход g = 70.775510\n",
      "\n",
      "================================================================================\n",
      "Политика 18/27: [1, 2, 2] [s0→1(Бесплатная доставка), s1→2(Ничего), s2→2(Ничего)]\n",
      "\n",
      "Политика [1, 2, 2]: средний доход g = 70.576271\n",
      "\n",
      "================================================================================\n",
      "Политика 19/27: [2, 0, 0] [s0→2(Ничего), s1→0(3% скидка), s2→0(3% скидка)]\n",
      "\n",
      "Политика [2, 0, 0]: средний доход g = 65.304348\n",
      "\n",
      "================================================================================\n",
      "Политика 20/27: [2, 0, 1] [s0→2(Ничего), s1→0(3% скидка), s2→1(Бесплатная доставка)]\n",
      "\n",
      "Политика [2, 0, 1]: средний доход g = 73.130435\n",
      "\n",
      "================================================================================\n",
      "Политика 21/27: [2, 0, 2] [s0→2(Ничего), s1→0(3% скидка), s2→2(Ничего)]\n",
      "\n",
      "Политика [2, 0, 2]: средний доход g = 73.636364\n",
      "\n",
      "================================================================================\n",
      "Политика 22/27: [2, 1, 0] [s0→2(Ничего), s1→1(Бесплатная доставка), s2→0(3% скидка)]\n",
      "\n",
      "Политика [2, 1, 0]: средний доход g = 65.500000\n",
      "\n",
      "================================================================================\n",
      "Политика 23/27: [2, 1, 1] [s0→2(Ничего), s1→1(Бесплатная доставка), s2→1(Бесплатная доставка)]\n",
      "\n",
      "Политика [2, 1, 1]: средний доход g = 76.187500\n",
      "\n",
      "================================================================================\n",
      "Политика 24/27: [2, 1, 2] [s0→2(Ничего), s1→1(Бесплатная доставка), s2→2(Ничего)]\n",
      "\n",
      "Политика [2, 1, 2]: средний доход g = 77.638889\n",
      "\n",
      "================================================================================\n",
      "Политика 25/27: [2, 2, 0] [s0→2(Ничего), s1→2(Ничего), s2→0(3% скидка)]\n",
      "\n",
      "Политика [2, 2, 0]: средний доход g = 59.826087\n",
      "\n",
      "================================================================================\n",
      "Политика 26/27: [2, 2, 1] [s0→2(Ничего), s1→2(Ничего), s2→1(Бесплатная доставка)]\n",
      "\n",
      "Политика [2, 2, 1]: средний доход g = 67.652174\n",
      "\n",
      "================================================================================\n",
      "Политика 27/27: [2, 2, 2] [s0→2(Ничего), s1→2(Ничего), s2→2(Ничего)]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Оценка политики: [2, 2, 2] [s0→a2(Ничего), s1→a2(Ничего), s2→a2(Ничего)]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Состояние 0 (Отличный), выбрано действие 2 (Ничего):\n",
      "  Вероятности переходов P[0,:] = [0.3 0.4 0.3]\n",
      "  Переход в 0 (Отличный): вероятность = 0.300, награда = 110, вклад = 33.000\n",
      "  Переход в 1 (Хороший): вероятность = 0.400, награда = 80, вклад = 32.000\n",
      "  Переход в 2 (Удовлетворительный): вероятность = 0.300, награда = 50, вклад = 15.000\n",
      "  Суммарное ожидаемое вознаграждение r[0] = 80.000\n",
      "\n",
      "Состояние 1 (Хороший), выбрано действие 2 (Ничего):\n",
      "  Вероятности переходов P[1,:] = [0.2 0.6 0.2]\n",
      "  Переход в 0 (Отличный): вероятность = 0.200, награда = 100, вклад = 20.000\n",
      "  Переход в 1 (Хороший): вероятность = 0.600, награда = 60, вклад = 36.000\n",
      "  Переход в 2 (Удовлетворительный): вероятность = 0.200, награда = 40, вклад = 8.000\n",
      "  Суммарное ожидаемое вознаграждение r[1] = 64.000\n",
      "\n",
      "Состояние 2 (Удовлетворительный), выбрано действие 2 (Ничего):\n",
      "  Вероятности переходов P[2,:] = [0.1 0.3 0.6]\n",
      "  Переход в 0 (Отличный): вероятность = 0.100, награда = 80, вклад = 8.000\n",
      "  Переход в 1 (Хороший): вероятность = 0.300, награда = 70, вклад = 21.000\n",
      "  Переход в 2 (Удовлетворительный): вероятность = 0.600, награда = 60, вклад = 36.000\n",
      "  Суммарное ожидаемое вознаграждение r[2] = 65.000\n",
      "\n",
      "Матрица переходов для политики:\n",
      "  [0.3 0.4 0.3]\n",
      "  [0.2 0.6 0.2]\n",
      "  [0.1 0.3 0.6]\n",
      "\n",
      "Вектор вознаграждений для политики:\n",
      "  [80. 64. 65.]\n",
      "\n",
      "Собственные значения матрицы переходов:\n",
      "  λ0 = 1.000000 (|λ0 - 1| = 0.000000)\n",
      "  λ1 = 0.138197 (|λ1 - 1| = 0.861803)\n",
      "  λ2 = 0.361803 (|λ2 - 1| = 0.638197)\n",
      "\n",
      "Индекс собственного значения, ближайшего к 1: 0\n",
      "Соответствующий собственный вектор (ненормированный):\n",
      "  [0.2981424  0.74535599 0.59628479]\n",
      "\n",
      "Стационарное распределение состояний (нормированное):\n",
      "  μ(0) = 0.181818 (Отличный)\n",
      "  μ(1) = 0.454545 (Хороший)\n",
      "  μ(2) = 0.363636 (Удовлетворительный)\n",
      "\n",
      "Расчет среднего дохода:\n",
      "  Состояние 0: μ(0) * r(0) = 0.181818 * 80.000 = 14.545455\n",
      "  Состояние 1: μ(1) * r(1) = 0.454545 * 64.000 = 29.090909\n",
      "  Состояние 2: μ(2) * r(2) = 0.363636 * 65.000 = 23.636364\n",
      "\n",
      "Средний доход g = μ⋅r = 67.272727\n",
      "\n",
      "Политика [2, 2, 2]: средний доход g = 67.272727\n",
      "\n",
      "\n",
      "\n",
      "======================================== Ответ ========================================\n",
      "Лучшая стационарная стратегия для состояний (0-Отличный, 1-Хороший, 2-Удовлетворительный):\n",
      "  [1, 1, 2] [s0→a1(Бесплатная доставка), s1→a1(Бесплатная доставка), s2→a2(Ничего)]\n",
      "\n",
      "Стационарное распределение состояний μ:\n",
      "  μ(0) = 0.111111 (Отличный)\n",
      "  μ(1) = 0.382716 (Хороший)\n",
      "  μ(2) = 0.506173 (Удовлетворительный)\n",
      "\n",
      "Средний доход g при лучшей стратегии: 80.864198\n",
      "Количество улучшений: 6 из 27 проверенных политик\n",
      "\n",
      "\n",
      "\n",
      "======================================== Лучшая политика ========================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Оценка политики: [1, 1, 2] [s0→a1(Бесплатная доставка), s1→a1(Бесплатная доставка), s2→a2(Ничего)]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Состояние 0 (Отличный), выбрано действие 1 (Бесплатная доставка):\n",
      "  Вероятности переходов P[0,:] = [0.2 0.7 0.1]\n",
      "  Переход в 0 (Отличный): вероятность = 0.200, награда = 120, вклад = 24.000\n",
      "  Переход в 1 (Хороший): вероятность = 0.700, награда = 100, вклад = 70.000\n",
      "  Переход в 2 (Удовлетворительный): вероятность = 0.100, награда = 70, вклад = 7.000\n",
      "  Суммарное ожидаемое вознаграждение r[0] = 101.000\n",
      "\n",
      "Состояние 1 (Хороший), выбрано действие 1 (Бесплатная доставка):\n",
      "  Вероятности переходов P[1,:] = [0.1 0.4 0.5]\n",
      "  Переход в 0 (Отличный): вероятность = 0.100, награда = 110, вклад = 11.000\n",
      "  Переход в 1 (Хороший): вероятность = 0.400, награда = 100, вклад = 40.000\n",
      "  Переход в 2 (Удовлетворительный): вероятность = 0.500, награда = 90, вклад = 45.000\n",
      "  Суммарное ожидаемое вознаграждение r[1] = 96.000\n",
      "\n",
      "Состояние 2 (Удовлетворительный), выбрано действие 2 (Ничего):\n",
      "  Вероятности переходов P[2,:] = [0.1 0.3 0.6]\n",
      "  Переход в 0 (Отличный): вероятность = 0.100, награда = 80, вклад = 8.000\n",
      "  Переход в 1 (Хороший): вероятность = 0.300, награда = 70, вклад = 21.000\n",
      "  Переход в 2 (Удовлетворительный): вероятность = 0.600, награда = 60, вклад = 36.000\n",
      "  Суммарное ожидаемое вознаграждение r[2] = 65.000\n",
      "\n",
      "Матрица переходов для политики:\n",
      "  [0.2 0.7 0.1]\n",
      "  [0.1 0.4 0.5]\n",
      "  [0.1 0.3 0.6]\n",
      "\n",
      "Вектор вознаграждений для политики:\n",
      "  [101.  96.  65.]\n",
      "\n",
      "Собственные значения матрицы переходов:\n",
      "  λ0 = 1.000000 (|λ0 - 1| = 0.000000)\n",
      "  λ1 = 0.100000 (|λ1 - 1| = 0.900000)\n",
      "  λ2 = 0.100000 (|λ2 - 1| = 0.900000)\n",
      "\n",
      "Индекс собственного значения, ближайшего к 1: 0\n",
      "Соответствующий собственный вектор (ненормированный):\n",
      "  [0.17247204 0.59407034 0.78570594]\n",
      "\n",
      "Стационарное распределение состояний (нормированное):\n",
      "  μ(0) = 0.111111 (Отличный)\n",
      "  μ(1) = 0.382716 (Хороший)\n",
      "  μ(2) = 0.506173 (Удовлетворительный)\n",
      "\n",
      "Расчет среднего дохода:\n",
      "  Состояние 0: μ(0) * r(0) = 0.111111 * 101.000 = 11.222222\n",
      "  Состояние 1: μ(1) * r(1) = 0.382716 * 96.000 = 36.740741\n",
      "  Состояние 2: μ(2) * r(2) = 0.506173 * 65.000 = 32.901235\n",
      "\n",
      "Средний доход g = μ⋅r = 80.864198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80.8641975308642, array([0.11111111, 0.38271605, 0.50617284]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# Матрицы переходов и доходов для каждого действия\n",
    "transition_matrix = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "reward_matrix = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "actions = [0, 1, 2]  # Индексы доступных действий\n",
    "states = [0, 1, 2]   # Состояния: 0 - «Отличный», 1 - «Хороший», 2 - «Удовлетворительный»\n",
    "state_names = [\"Отличный\", \"Хороший\", \"Удовлетворительный\"]\n",
    "action_names = [\"3% скидка\", \"Бесплатная доставка\", \"Ничего\"]\n",
    "\n",
    "def evaluate_policy(policy, verbose=True):\n",
    "    \"\"\"\n",
    "    Оценка политики: вычисление среднего дохода и стационарного распределения состояний.\n",
    "    \n",
    "    Параметры:\n",
    "    policy (list[int]): Стратегия действий для каждого состояния.\n",
    "    verbose (bool): Выводить ли детальные промежуточные результаты.\n",
    "    \n",
    "    Возвращает:\n",
    "    float: Средний доход от применения политики.\n",
    "    numpy.ndarray: Стационарное распределение состояний.\n",
    "    \"\"\"\n",
    "    # Инициализация матрицы переходов и вектора вознаграждений для данной политики\n",
    "    transition_matrix_policy = np.zeros((3, 3))\n",
    "    reward_vector_policy = np.zeros(3)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'-'*80}\")\n",
    "        policy_str = \", \".join([f\"s{i}→a{a}({action_names[a]})\" for i, a in enumerate(policy)])\n",
    "        print(f\"Оценка политики: {policy} [{policy_str}]\")\n",
    "        print(f\"{'-'*80}\")\n",
    "    \n",
    "    # Для каждого состояния, вычисляем ожидаемое вознаграждение и вероятности перехода\n",
    "    for state in states:\n",
    "        action = policy[state]\n",
    "        transition_matrix_policy[state, :] = transition_matrix[action][state]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nСостояние {state} ({state_names[state]}), выбрано действие {action} ({action_names[action]}):\")\n",
    "            print(f\"  Вероятности переходов P[{state},:] = {transition_matrix_policy[state, :]}\")\n",
    "        \n",
    "        state_reward = 0\n",
    "        for next_state in states:\n",
    "            transition_prob = transition_matrix[action][state][next_state]\n",
    "            reward = reward_matrix[action][state][next_state]\n",
    "            contrib = transition_prob * reward\n",
    "            state_reward += contrib\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Переход в {next_state} ({state_names[next_state]}): вероятность = {transition_prob:.3f}, \"\n",
    "                      f\"награда = {reward}, вклад = {contrib:.3f}\")\n",
    "        \n",
    "        reward_vector_policy[state] = state_reward\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Суммарное ожидаемое вознаграждение r[{state}] = {state_reward:.3f}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nМатрица переходов для политики:\")\n",
    "        for i in range(3):\n",
    "            print(f\"  {transition_matrix_policy[i, :]}\")\n",
    "        print(\"\\nВектор вознаграждений для политики:\")\n",
    "        print(f\"  {reward_vector_policy}\")\n",
    "\n",
    "    # Решение задачи с собственными значениями для нахождения стационарного распределения\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(transition_matrix_policy.T)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nСобственные значения матрицы переходов:\")\n",
    "        for i, ev in enumerate(eigenvalues):\n",
    "            print(f\"  λ{i} = {ev:.6f} (|λ{i} - 1| = {abs(ev - 1.0):.6f})\")\n",
    "    \n",
    "    stationary_state_idx = np.argmin(np.abs(eigenvalues - 1.0))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nИндекс собственного значения, ближайшего к 1: {stationary_state_idx}\")\n",
    "        print(f\"Соответствующий собственный вектор (ненормированный):\")\n",
    "        print(f\"  {np.real(eigenvectors[:, stationary_state_idx])}\")\n",
    "    \n",
    "    stationary_distribution = np.real(eigenvectors[:, stationary_state_idx])\n",
    "    \n",
    "    # Нормировка стационарного распределения\n",
    "    stationary_distribution /= stationary_distribution.sum()\n",
    "\n",
    "    # Рассчитываем средний доход\n",
    "    average_reward = float(np.dot(stationary_distribution, reward_vector_policy))\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nСтационарное распределение состояний (нормированное):\")\n",
    "        for i, prob in enumerate(stationary_distribution):\n",
    "            print(f\"  μ({i}) = {prob:.6f} ({state_names[i]})\")\n",
    "        \n",
    "        print(\"\\nРасчет среднего дохода:\")\n",
    "        for i, (prob, reward) in enumerate(zip(stationary_distribution, reward_vector_policy)):\n",
    "            print(f\"  Состояние {i}: μ({i}) * r({i}) = {prob:.6f} * {reward:.3f} = {prob * reward:.6f}\")\n",
    "        \n",
    "        print(f\"\\nСредний доход g = μ⋅r = {average_reward:.6f}\")\n",
    "    \n",
    "    return average_reward, stationary_distribution\n",
    "\n",
    "# Инициализация переменных для поиска лучшей политики\n",
    "best_policy = None\n",
    "best_gain = -1e9  # Начальное значение для максимального дохода\n",
    "best_stationary_distribution = None\n",
    "\n",
    "# Общее количество политик\n",
    "total_policies = len(actions) ** len(states)\n",
    "print(f\"Всего возможных политик: {total_policies}\")\n",
    "\n",
    "# Перебор всех возможных политик и выбор лучшей\n",
    "policy_count = 0\n",
    "improvements = 0\n",
    "\n",
    "for policy in itertools.product(actions, repeat=3):\n",
    "    policy_count += 1\n",
    "    policy = list(policy)  # Преобразование кортежа в список для удобства\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Политика {policy_count}/{total_policies}: {policy} \" + \n",
    "          f\"[{', '.join([f's{i}→{a}({action_names[a]})' for i, a in enumerate(policy)])}]\")\n",
    "    \n",
    "    # Подробный вывод только для первой, лучшей и последней политики\n",
    "    verbose = (policy_count == 1) or (policy_count == total_policies)\n",
    "    gain, stationary_distribution = evaluate_policy(policy, verbose=verbose)\n",
    "    \n",
    "    print(f\"\\nПолитика {policy}: средний доход g = {gain:.6f}\")\n",
    "    \n",
    "    if gain > best_gain:\n",
    "        improvements += 1\n",
    "        improvement = gain - best_gain\n",
    "        print(f\"УЛУЧШЕНИЕ #{improvements}: +{improvement:.6f} (было {best_gain:.6f}, стало {gain:.6f})\")\n",
    "        best_gain = gain\n",
    "        best_policy = policy\n",
    "        best_stationary_distribution = stationary_distribution\n",
    "\n",
    "print(f\"\\n\\n\\n{'='*40} Ответ {'='*40}\")\n",
    "print(\"Лучшая стационарная стратегия для состояний (0-Отличный, 1-Хороший, 2-Удовлетворительный):\")\n",
    "policy_str = \", \".join([f\"s{i}→a{a}({action_names[a]})\" for i, a in enumerate(best_policy)])\n",
    "print(f\"  {best_policy} [{policy_str}]\")\n",
    "print(\"\\nСтационарное распределение состояний μ:\")\n",
    "for i, prob in enumerate(best_stationary_distribution):\n",
    "    print(f\"  μ({i}) = {prob:.6f} ({state_names[i]})\")\n",
    "print(f\"\\nСредний доход g при лучшей стратегии: {best_gain:.6f}\")\n",
    "print(f\"Количество улучшений: {improvements} из {policy_count} проверенных политик\")\n",
    "\n",
    "print(f\"\\n\\n\\n{'='*40} Лучшая политика {'='*40}\")\n",
    "evaluate_policy(best_policy, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaadd10",
   "metadata": {},
   "source": [
    "___\n",
    "## 3 Метод итерации по стратегиям без дисконтирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3bb237d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Начальная политика:\n",
      "  Состояние 0 (Отличный) → Действие 0 (3% скидка)\n",
      "  Состояние 1 (Хороший) → Действие 0 (3% скидка)\n",
      "  Состояние 2 (Удовлетворительный) → Действие 0 (3% скидка)\n",
      "\n",
      "**************************************** Итерация 1 ****************************************\n",
      "\n",
      "Текущая политика:\n",
      "  0 (Отличный) → 0 (3% скидка)\n",
      "  1 (Хороший) → 0 (3% скидка)\n",
      "  2 (Удовлетворительный) → 0 (3% скидка)\n",
      "\n",
      "======================================================================\n",
      "ОЦЕНКА ПОЛИТИКИ: [0, 0, 0] (['3% скидка', '3% скидка', '3% скидка'])\n",
      "======================================================================\n",
      "\n",
      "1) Собираем P_π и r_π на основе текущей политики:\n",
      "\n",
      "  Состояние 0 (Отличный) → действие 0 (3% скидка):\n",
      "    Переход в состояние 0 (Отличный): P[0][0][0] = 0.300, R[0][0][0] = 110, вклад в r_π[0] += 33.000\n",
      "    Переход в состояние 1 (Хороший): P[0][0][1] = 0.500, R[0][0][1] = 100, вклад в r_π[0] += 50.000\n",
      "    Переход в состояние 2 (Удовлетворительный): P[0][0][2] = 0.200, R[0][0][2] = 70, вклад в r_π[0] += 14.000\n",
      "\n",
      "  Состояние 1 (Хороший) → действие 0 (3% скидка):\n",
      "    Переход в состояние 0 (Отличный): P[0][1][0] = 0.200, R[0][1][0] = 100, вклад в r_π[1] += 20.000\n",
      "    Переход в состояние 1 (Хороший): P[0][1][1] = 0.600, R[0][1][1] = 80, вклад в r_π[1] += 48.000\n",
      "    Переход в состояние 2 (Удовлетворительный): P[0][1][2] = 0.200, R[0][1][2] = 50, вклад в r_π[1] += 10.000\n",
      "\n",
      "  Состояние 2 (Удовлетворительный) → действие 0 (3% скидка):\n",
      "    Переход в состояние 0 (Отличный): P[0][2][0] = 0.100, R[0][2][0] = 80, вклад в r_π[2] += 8.000\n",
      "    Переход в состояние 1 (Хороший): P[0][2][1] = 0.200, R[0][2][1] = 60, вклад в r_π[2] += 12.000\n",
      "    Переход в состояние 2 (Удовлетворительный): P[0][2][2] = 0.700, R[0][2][2] = 40, вклад в r_π[2] += 28.000\n",
      "\n",
      "  Итоговая матрица переходов P_π:\n",
      "    [0.3 0.5 0.2]\n",
      "    [0.2 0.6 0.2]\n",
      "    [0.1 0.2 0.7]\n",
      "\n",
      "  Итоговый вектор вознаграждений r_π:\n",
      "    [97. 78. 48.]\n",
      "\n",
      "2) Строим систему уравнений (m+1)×(m+1) для нахождения [h₀, h₁, h₂, g]:\n",
      "   (I - P_π)h + g·1 = r_π, с дополнительным условием h[m-1] = 0\n",
      "    Уравнение 1: 0.700·h[0] -0.500·h[1] -0.200·h[2] + g = 97.000\n",
      "    Уравнение 2: -0.200·h[0] +0.400·h[1] -0.200·h[2] + g = 78.000\n",
      "    Уравнение 3: -0.100·h[0] -0.200·h[1] +0.300·h[2] + g = 48.000\n",
      "    Дополнительное условие: h[2] = 0\n",
      "\n",
      "  Матрица системы A:\n",
      "    [ 0.7 -0.5 -0.2  1. ]\n",
      "    [-0.2  0.4 -0.2  1. ]\n",
      "    [-0.1 -0.2  0.3  1. ]\n",
      "    [0. 0. 1. 0.]\n",
      "\n",
      "  Вектор правых частей b:\n",
      "    [97. 78. 48.  0.]\n",
      "\n",
      "3) Решение системы уравнений:\n",
      "  Вектор смещений h = [85.33333333 64.22222222  0.        ]\n",
      "  Средний доход g = 69.377778\n",
      "\n",
      "4) Проверка решения (должно быть приблизительно равно r_π):\n",
      "  Уравнение 1: h[0] - ∑_j P_π[0,2]·h[j] + g = 97.000000, r_π[0] = 97.000000, ошибка = 0.000000e+00\n",
      "  Уравнение 2: h[1] - ∑_j P_π[1,2]·h[j] + g = 78.000000, r_π[1] = 78.000000, ошибка = 0.000000e+00\n",
      "  Уравнение 3: h[2] - ∑_j P_π[2,2]·h[j] + g = 48.000000, r_π[2] = 48.000000, ошибка = 0.000000e+00\n",
      "\n",
      "Оценка текущей политики:\n",
      "  Средний доход g = 69.3778\n",
      "\n",
      "========================================Улучшение политики========================================\n",
      "\n",
      "Текущая политика: [0, 0, 0] (['3% скидка', '3% скидка', '3% скидка'])\n",
      "Вектор смещений h = [85.33333333 64.22222222  0.        ]\n",
      "\n",
      "Для состояния 0 (Отличный) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(0,0) = P[0][0][0]·R[0][0][0] = 0.300·110 = 33.000 + P[0][0][1]·R[0][0][1] = 0.500·100 = 50.000 + P[0][0][2]·R[0][0][2] = 0.200·70 = 14.000 = 97.000\n",
      "    Bias term = P[0][0][0]·h[0] = 0.300·85.333 = 25.600 + P[0][0][1]·h[1] = 0.500·64.222 = 32.111 + P[0][0][2]·h[2] = 0.200·0.000 = 0.000 = 57.711\n",
      "    Q(0,0) = 97.000 + 57.711 = 154.711\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    r(0,1) = P[1][0][0]·R[1][0][0] = 0.200·120 = 24.000 + P[1][0][1]·R[1][0][1] = 0.700·100 = 70.000 + P[1][0][2]·R[1][0][2] = 0.100·70 = 7.000 = 101.000\n",
      "    Bias term = P[1][0][0]·h[0] = 0.200·85.333 = 17.067 + P[1][0][1]·h[1] = 0.700·64.222 = 44.956 + P[1][0][2]·h[2] = 0.100·0.000 = 0.000 = 62.022\n",
      "    Q(0,1) = 101.000 + 62.022 = 163.022\n",
      "    Лучше предыдущего действия 0 с Q = 154.711, обновляем.\n",
      "  Действие 2 (Ничего):\n",
      "    r(0,2) = P[2][0][0]·R[2][0][0] = 0.300·110 = 33.000 + P[2][0][1]·R[2][0][1] = 0.400·80 = 32.000 + P[2][0][2]·R[2][0][2] = 0.300·50 = 15.000 = 80.000\n",
      "    Bias term = P[2][0][0]·h[0] = 0.300·85.333 = 25.600 + P[2][0][1]·h[1] = 0.400·64.222 = 25.689 + P[2][0][2]·h[2] = 0.300·0.000 = 0.000 = 51.289\n",
      "    Q(0,2) = 80.000 + 51.289 = 131.289\n",
      "    Хуже текущего лучшего действия 1 с Q = 163.022, пропускаем.\n",
      "  Лучшее действие для состояния 0: 1 (Бесплатная доставка), Q = 163.022\n",
      "  >>> Политика ИЗМЕНЕНА для состояния 0: 0 → 1\n",
      "\n",
      "Для состояния 1 (Хороший) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(1,0) = P[0][1][0]·R[0][1][0] = 0.200·100 = 20.000 + P[0][1][1]·R[0][1][1] = 0.600·80 = 48.000 + P[0][1][2]·R[0][1][2] = 0.200·50 = 10.000 = 78.000\n",
      "    Bias term = P[0][1][0]·h[0] = 0.200·85.333 = 17.067 + P[0][1][1]·h[1] = 0.600·64.222 = 38.533 + P[0][1][2]·h[2] = 0.200·0.000 = 0.000 = 55.600\n",
      "    Q(1,0) = 78.000 + 55.600 = 133.600\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    r(1,1) = P[1][1][0]·R[1][1][0] = 0.100·110 = 11.000 + P[1][1][1]·R[1][1][1] = 0.400·100 = 40.000 + P[1][1][2]·R[1][1][2] = 0.500·90 = 45.000 = 96.000\n",
      "    Bias term = P[1][1][0]·h[0] = 0.100·85.333 = 8.533 + P[1][1][1]·h[1] = 0.400·64.222 = 25.689 + P[1][1][2]·h[2] = 0.500·0.000 = 0.000 = 34.222\n",
      "    Q(1,1) = 96.000 + 34.222 = 130.222\n",
      "    Хуже текущего лучшего действия 0 с Q = 133.600, пропускаем.\n",
      "  Действие 2 (Ничего):\n",
      "    r(1,2) = P[2][1][0]·R[2][1][0] = 0.200·100 = 20.000 + P[2][1][1]·R[2][1][1] = 0.600·60 = 36.000 + P[2][1][2]·R[2][1][2] = 0.200·40 = 8.000 = 64.000\n",
      "    Bias term = P[2][1][0]·h[0] = 0.200·85.333 = 17.067 + P[2][1][1]·h[1] = 0.600·64.222 = 38.533 + P[2][1][2]·h[2] = 0.200·0.000 = 0.000 = 55.600\n",
      "    Q(1,2) = 64.000 + 55.600 = 119.600\n",
      "    Хуже текущего лучшего действия 0 с Q = 133.600, пропускаем.\n",
      "  Лучшее действие для состояния 1: 0 (3% скидка), Q = 133.600\n",
      "  >>> Политика НЕ ИЗМЕНЕНА для состояния 1: остается 0\n",
      "\n",
      "Для состояния 2 (Удовлетворительный) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(2,0) = P[0][2][0]·R[0][2][0] = 0.100·80 = 8.000 + P[0][2][1]·R[0][2][1] = 0.200·60 = 12.000 + P[0][2][2]·R[0][2][2] = 0.700·40 = 28.000 = 48.000\n",
      "    Bias term = P[0][2][0]·h[0] = 0.100·85.333 = 8.533 + P[0][2][1]·h[1] = 0.200·64.222 = 12.844 + P[0][2][2]·h[2] = 0.700·0.000 = 0.000 = 21.378\n",
      "    Q(2,0) = 48.000 + 21.378 = 69.378\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    r(2,1) = P[1][2][0]·R[1][2][0] = 0.100·100 = 10.000 + P[1][2][1]·R[1][2][1] = 0.200·70 = 14.000 + P[1][2][2]·R[1][2][2] = 0.700·60 = 42.000 = 66.000\n",
      "    Bias term = P[1][2][0]·h[0] = 0.100·85.333 = 8.533 + P[1][2][1]·h[1] = 0.200·64.222 = 12.844 + P[1][2][2]·h[2] = 0.700·0.000 = 0.000 = 21.378\n",
      "    Q(2,1) = 66.000 + 21.378 = 87.378\n",
      "    Лучше предыдущего действия 0 с Q = 69.378, обновляем.\n",
      "  Действие 2 (Ничего):\n",
      "    r(2,2) = P[2][2][0]·R[2][2][0] = 0.100·80 = 8.000 + P[2][2][1]·R[2][2][1] = 0.300·70 = 21.000 + P[2][2][2]·R[2][2][2] = 0.600·60 = 36.000 = 65.000\n",
      "    Bias term = P[2][2][0]·h[0] = 0.100·85.333 = 8.533 + P[2][2][1]·h[1] = 0.300·64.222 = 19.267 + P[2][2][2]·h[2] = 0.600·0.000 = 0.000 = 27.800\n",
      "    Q(2,2) = 65.000 + 27.800 = 92.800\n",
      "    Лучше предыдущего действия 1 с Q = 87.378, обновляем.\n",
      "  Лучшее действие для состояния 2: 2 (Ничего), Q = 92.800\n",
      "  >>> Политика ИЗМЕНЕНА для состояния 2: 0 → 2\n",
      "\n",
      "Итог улучшения политики:\n",
      "  Было: [0, 0, 0] (['3% скидка', '3% скидка', '3% скидка'])\n",
      "  Стало: [1, 0, 2] (['Бесплатная доставка', '3% скидка', 'Ничего'])\n",
      "  Политика ИЗМЕНИЛАСЬ - требуется продолжить итерации.\n",
      "\n",
      "**************************************** Итерация 2 ****************************************\n",
      "\n",
      "Текущая политика:\n",
      "  0 (Отличный) → 1 (Бесплатная доставка)\n",
      "  1 (Хороший) → 0 (3% скидка)\n",
      "  2 (Удовлетворительный) → 2 (Ничего)\n",
      "\n",
      "======================================================================\n",
      "ОЦЕНКА ПОЛИТИКИ: [1, 0, 2] (['Бесплатная доставка', '3% скидка', 'Ничего'])\n",
      "======================================================================\n",
      "\n",
      "1) Собираем P_π и r_π на основе текущей политики:\n",
      "\n",
      "  Состояние 0 (Отличный) → действие 1 (Бесплатная доставка):\n",
      "    Переход в состояние 0 (Отличный): P[1][0][0] = 0.200, R[1][0][0] = 120, вклад в r_π[0] += 24.000\n",
      "    Переход в состояние 1 (Хороший): P[1][0][1] = 0.700, R[1][0][1] = 100, вклад в r_π[0] += 70.000\n",
      "    Переход в состояние 2 (Удовлетворительный): P[1][0][2] = 0.100, R[1][0][2] = 70, вклад в r_π[0] += 7.000\n",
      "\n",
      "  Состояние 1 (Хороший) → действие 0 (3% скидка):\n",
      "    Переход в состояние 0 (Отличный): P[0][1][0] = 0.200, R[0][1][0] = 100, вклад в r_π[1] += 20.000\n",
      "    Переход в состояние 1 (Хороший): P[0][1][1] = 0.600, R[0][1][1] = 80, вклад в r_π[1] += 48.000\n",
      "    Переход в состояние 2 (Удовлетворительный): P[0][1][2] = 0.200, R[0][1][2] = 50, вклад в r_π[1] += 10.000\n",
      "\n",
      "  Состояние 2 (Удовлетворительный) → действие 2 (Ничего):\n",
      "    Переход в состояние 0 (Отличный): P[2][2][0] = 0.100, R[2][2][0] = 80, вклад в r_π[2] += 8.000\n",
      "    Переход в состояние 1 (Хороший): P[2][2][1] = 0.300, R[2][2][1] = 70, вклад в r_π[2] += 21.000\n",
      "    Переход в состояние 2 (Удовлетворительный): P[2][2][2] = 0.600, R[2][2][2] = 60, вклад в r_π[2] += 36.000\n",
      "\n",
      "  Итоговая матрица переходов P_π:\n",
      "    [0.2 0.7 0.1]\n",
      "    [0.2 0.6 0.2]\n",
      "    [0.1 0.3 0.6]\n",
      "\n",
      "  Итоговый вектор вознаграждений r_π:\n",
      "    [101.  78.  65.]\n",
      "\n",
      "2) Строим систему уравнений (m+1)×(m+1) для нахождения [h₀, h₁, h₂, g]:\n",
      "   (I - P_π)h + g·1 = r_π, с дополнительным условием h[m-1] = 0\n",
      "    Уравнение 1: 0.800·h[0] -0.700·h[1] -0.100·h[2] + g = 101.000\n",
      "    Уравнение 2: -0.200·h[0] +0.400·h[1] -0.200·h[2] + g = 78.000\n",
      "    Уравнение 3: -0.100·h[0] -0.300·h[1] +0.400·h[2] + g = 65.000\n",
      "    Дополнительное условие: h[2] = 0\n",
      "\n",
      "  Матрица системы A:\n",
      "    [ 0.8 -0.7 -0.1  1. ]\n",
      "    [-0.2  0.4 -0.2  1. ]\n",
      "    [-0.1 -0.3  0.4  1. ]\n",
      "    [0. 0. 1. 0.]\n",
      "\n",
      "  Вектор правых частей b:\n",
      "    [101.  78.  65.   0.]\n",
      "\n",
      "3) Решение системы уравнений:\n",
      "  Вектор смещений h = [51.52542373 25.93220339  0.        ]\n",
      "  Средний доход g = 77.932203\n",
      "\n",
      "4) Проверка решения (должно быть приблизительно равно r_π):\n",
      "  Уравнение 1: h[0] - ∑_j P_π[0,2]·h[j] + g = 101.000000, r_π[0] = 101.000000, ошибка = 0.000000e+00\n",
      "  Уравнение 2: h[1] - ∑_j P_π[1,2]·h[j] + g = 78.000000, r_π[1] = 78.000000, ошибка = 2.842171e-14\n",
      "  Уравнение 3: h[2] - ∑_j P_π[2,2]·h[j] + g = 65.000000, r_π[2] = 65.000000, ошибка = 0.000000e+00\n",
      "\n",
      "Оценка текущей политики:\n",
      "  Средний доход g = 77.9322\n",
      "\n",
      "========================================Улучшение политики========================================\n",
      "\n",
      "Текущая политика: [1, 0, 2] (['Бесплатная доставка', '3% скидка', 'Ничего'])\n",
      "Вектор смещений h = [51.52542373 25.93220339  0.        ]\n",
      "\n",
      "Для состояния 0 (Отличный) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(0,0) = P[0][0][0]·R[0][0][0] = 0.300·110 = 33.000 + P[0][0][1]·R[0][0][1] = 0.500·100 = 50.000 + P[0][0][2]·R[0][0][2] = 0.200·70 = 14.000 = 97.000\n",
      "    Bias term = P[0][0][0]·h[0] = 0.300·51.525 = 15.458 + P[0][0][1]·h[1] = 0.500·25.932 = 12.966 + P[0][0][2]·h[2] = 0.200·0.000 = 0.000 = 28.424\n",
      "    Q(0,0) = 97.000 + 28.424 = 125.424\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    r(0,1) = P[1][0][0]·R[1][0][0] = 0.200·120 = 24.000 + P[1][0][1]·R[1][0][1] = 0.700·100 = 70.000 + P[1][0][2]·R[1][0][2] = 0.100·70 = 7.000 = 101.000\n",
      "    Bias term = P[1][0][0]·h[0] = 0.200·51.525 = 10.305 + P[1][0][1]·h[1] = 0.700·25.932 = 18.153 + P[1][0][2]·h[2] = 0.100·0.000 = 0.000 = 28.458\n",
      "    Q(0,1) = 101.000 + 28.458 = 129.458\n",
      "    Лучше предыдущего действия 0 с Q = 125.424, обновляем.\n",
      "  Действие 2 (Ничего):\n",
      "    r(0,2) = P[2][0][0]·R[2][0][0] = 0.300·110 = 33.000 + P[2][0][1]·R[2][0][1] = 0.400·80 = 32.000 + P[2][0][2]·R[2][0][2] = 0.300·50 = 15.000 = 80.000\n",
      "    Bias term = P[2][0][0]·h[0] = 0.300·51.525 = 15.458 + P[2][0][1]·h[1] = 0.400·25.932 = 10.373 + P[2][0][2]·h[2] = 0.300·0.000 = 0.000 = 25.831\n",
      "    Q(0,2) = 80.000 + 25.831 = 105.831\n",
      "    Хуже текущего лучшего действия 1 с Q = 129.458, пропускаем.\n",
      "  Лучшее действие для состояния 0: 1 (Бесплатная доставка), Q = 129.458\n",
      "  >>> Политика НЕ ИЗМЕНЕНА для состояния 0: остается 1\n",
      "\n",
      "Для состояния 1 (Хороший) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(1,0) = P[0][1][0]·R[0][1][0] = 0.200·100 = 20.000 + P[0][1][1]·R[0][1][1] = 0.600·80 = 48.000 + P[0][1][2]·R[0][1][2] = 0.200·50 = 10.000 = 78.000\n",
      "    Bias term = P[0][1][0]·h[0] = 0.200·51.525 = 10.305 + P[0][1][1]·h[1] = 0.600·25.932 = 15.559 + P[0][1][2]·h[2] = 0.200·0.000 = 0.000 = 25.864\n",
      "    Q(1,0) = 78.000 + 25.864 = 103.864\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    r(1,1) = P[1][1][0]·R[1][1][0] = 0.100·110 = 11.000 + P[1][1][1]·R[1][1][1] = 0.400·100 = 40.000 + P[1][1][2]·R[1][1][2] = 0.500·90 = 45.000 = 96.000\n",
      "    Bias term = P[1][1][0]·h[0] = 0.100·51.525 = 5.153 + P[1][1][1]·h[1] = 0.400·25.932 = 10.373 + P[1][1][2]·h[2] = 0.500·0.000 = 0.000 = 15.525\n",
      "    Q(1,1) = 96.000 + 15.525 = 111.525\n",
      "    Лучше предыдущего действия 0 с Q = 103.864, обновляем.\n",
      "  Действие 2 (Ничего):\n",
      "    r(1,2) = P[2][1][0]·R[2][1][0] = 0.200·100 = 20.000 + P[2][1][1]·R[2][1][1] = 0.600·60 = 36.000 + P[2][1][2]·R[2][1][2] = 0.200·40 = 8.000 = 64.000\n",
      "    Bias term = P[2][1][0]·h[0] = 0.200·51.525 = 10.305 + P[2][1][1]·h[1] = 0.600·25.932 = 15.559 + P[2][1][2]·h[2] = 0.200·0.000 = 0.000 = 25.864\n",
      "    Q(1,2) = 64.000 + 25.864 = 89.864\n",
      "    Хуже текущего лучшего действия 1 с Q = 111.525, пропускаем.\n",
      "  Лучшее действие для состояния 1: 1 (Бесплатная доставка), Q = 111.525\n",
      "  >>> Политика ИЗМЕНЕНА для состояния 1: 0 → 1\n",
      "\n",
      "Для состояния 2 (Удовлетворительный) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(2,0) = P[0][2][0]·R[0][2][0] = 0.100·80 = 8.000 + P[0][2][1]·R[0][2][1] = 0.200·60 = 12.000 + P[0][2][2]·R[0][2][2] = 0.700·40 = 28.000 = 48.000\n",
      "    Bias term = P[0][2][0]·h[0] = 0.100·51.525 = 5.153 + P[0][2][1]·h[1] = 0.200·25.932 = 5.186 + P[0][2][2]·h[2] = 0.700·0.000 = 0.000 = 10.339\n",
      "    Q(2,0) = 48.000 + 10.339 = 58.339\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    r(2,1) = P[1][2][0]·R[1][2][0] = 0.100·100 = 10.000 + P[1][2][1]·R[1][2][1] = 0.200·70 = 14.000 + P[1][2][2]·R[1][2][2] = 0.700·60 = 42.000 = 66.000\n",
      "    Bias term = P[1][2][0]·h[0] = 0.100·51.525 = 5.153 + P[1][2][1]·h[1] = 0.200·25.932 = 5.186 + P[1][2][2]·h[2] = 0.700·0.000 = 0.000 = 10.339\n",
      "    Q(2,1) = 66.000 + 10.339 = 76.339\n",
      "    Лучше предыдущего действия 0 с Q = 58.339, обновляем.\n",
      "  Действие 2 (Ничего):\n",
      "    r(2,2) = P[2][2][0]·R[2][2][0] = 0.100·80 = 8.000 + P[2][2][1]·R[2][2][1] = 0.300·70 = 21.000 + P[2][2][2]·R[2][2][2] = 0.600·60 = 36.000 = 65.000\n",
      "    Bias term = P[2][2][0]·h[0] = 0.100·51.525 = 5.153 + P[2][2][1]·h[1] = 0.300·25.932 = 7.780 + P[2][2][2]·h[2] = 0.600·0.000 = 0.000 = 12.932\n",
      "    Q(2,2) = 65.000 + 12.932 = 77.932\n",
      "    Лучше предыдущего действия 1 с Q = 76.339, обновляем.\n",
      "  Лучшее действие для состояния 2: 2 (Ничего), Q = 77.932\n",
      "  >>> Политика НЕ ИЗМЕНЕНА для состояния 2: остается 2\n",
      "\n",
      "Итог улучшения политики:\n",
      "  Было: [1, 0, 2] (['Бесплатная доставка', '3% скидка', 'Ничего'])\n",
      "  Стало: [1, 1, 2] (['Бесплатная доставка', 'Бесплатная доставка', 'Ничего'])\n",
      "  Политика ИЗМЕНИЛАСЬ - требуется продолжить итерации.\n",
      "\n",
      "**************************************** Итерация 3 ****************************************\n",
      "\n",
      "Текущая политика:\n",
      "  0 (Отличный) → 1 (Бесплатная доставка)\n",
      "  1 (Хороший) → 1 (Бесплатная доставка)\n",
      "  2 (Удовлетворительный) → 2 (Ничего)\n",
      "\n",
      "======================================================================\n",
      "ОЦЕНКА ПОЛИТИКИ: [1, 1, 2] (['Бесплатная доставка', 'Бесплатная доставка', 'Ничего'])\n",
      "======================================================================\n",
      "\n",
      "1) Собираем P_π и r_π на основе текущей политики:\n",
      "\n",
      "  Состояние 0 (Отличный) → действие 1 (Бесплатная доставка):\n",
      "    Переход в состояние 0 (Отличный): P[1][0][0] = 0.200, R[1][0][0] = 120, вклад в r_π[0] += 24.000\n",
      "    Переход в состояние 1 (Хороший): P[1][0][1] = 0.700, R[1][0][1] = 100, вклад в r_π[0] += 70.000\n",
      "    Переход в состояние 2 (Удовлетворительный): P[1][0][2] = 0.100, R[1][0][2] = 70, вклад в r_π[0] += 7.000\n",
      "\n",
      "  Состояние 1 (Хороший) → действие 1 (Бесплатная доставка):\n",
      "    Переход в состояние 0 (Отличный): P[1][1][0] = 0.100, R[1][1][0] = 110, вклад в r_π[1] += 11.000\n",
      "    Переход в состояние 1 (Хороший): P[1][1][1] = 0.400, R[1][1][1] = 100, вклад в r_π[1] += 40.000\n",
      "    Переход в состояние 2 (Удовлетворительный): P[1][1][2] = 0.500, R[1][1][2] = 90, вклад в r_π[1] += 45.000\n",
      "\n",
      "  Состояние 2 (Удовлетворительный) → действие 2 (Ничего):\n",
      "    Переход в состояние 0 (Отличный): P[2][2][0] = 0.100, R[2][2][0] = 80, вклад в r_π[2] += 8.000\n",
      "    Переход в состояние 1 (Хороший): P[2][2][1] = 0.300, R[2][2][1] = 70, вклад в r_π[2] += 21.000\n",
      "    Переход в состояние 2 (Удовлетворительный): P[2][2][2] = 0.600, R[2][2][2] = 60, вклад в r_π[2] += 36.000\n",
      "\n",
      "  Итоговая матрица переходов P_π:\n",
      "    [0.2 0.7 0.1]\n",
      "    [0.1 0.4 0.5]\n",
      "    [0.1 0.3 0.6]\n",
      "\n",
      "  Итоговый вектор вознаграждений r_π:\n",
      "    [101.  96.  65.]\n",
      "\n",
      "2) Строим систему уравнений (m+1)×(m+1) для нахождения [h₀, h₁, h₂, g]:\n",
      "   (I - P_π)h + g·1 = r_π, с дополнительным условием h[m-1] = 0\n",
      "    Уравнение 1: 0.800·h[0] -0.700·h[1] -0.100·h[2] + g = 101.000\n",
      "    Уравнение 2: -0.100·h[0] +0.600·h[1] -0.500·h[2] + g = 96.000\n",
      "    Уравнение 3: -0.100·h[0] -0.300·h[1] +0.400·h[2] + g = 65.000\n",
      "    Дополнительное условие: h[2] = 0\n",
      "\n",
      "  Матрица системы A:\n",
      "    [ 0.8 -0.7 -0.1  1. ]\n",
      "    [-0.1  0.6 -0.5  1. ]\n",
      "    [-0.1 -0.3  0.4  1. ]\n",
      "    [0. 0. 1. 0.]\n",
      "\n",
      "  Вектор правых частей b:\n",
      "    [101.  96.  65.   0.]\n",
      "\n",
      "3) Решение системы уравнений:\n",
      "  Вектор смещений h = [55.30864198 34.44444444  0.        ]\n",
      "  Средний доход g = 80.864198\n",
      "\n",
      "4) Проверка решения (должно быть приблизительно равно r_π):\n",
      "  Уравнение 1: h[0] - ∑_j P_π[0,2]·h[j] + g = 101.000000, r_π[0] = 101.000000, ошибка = 0.000000e+00\n",
      "  Уравнение 2: h[1] - ∑_j P_π[1,2]·h[j] + g = 96.000000, r_π[1] = 96.000000, ошибка = 0.000000e+00\n",
      "  Уравнение 3: h[2] - ∑_j P_π[2,2]·h[j] + g = 65.000000, r_π[2] = 65.000000, ошибка = 1.421085e-14\n",
      "\n",
      "Оценка текущей политики:\n",
      "  Средний доход g = 80.8642\n",
      "\n",
      "========================================Улучшение политики========================================\n",
      "\n",
      "Текущая политика: [1, 1, 2] (['Бесплатная доставка', 'Бесплатная доставка', 'Ничего'])\n",
      "Вектор смещений h = [55.30864198 34.44444444  0.        ]\n",
      "\n",
      "Для состояния 0 (Отличный) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(0,0) = P[0][0][0]·R[0][0][0] = 0.300·110 = 33.000 + P[0][0][1]·R[0][0][1] = 0.500·100 = 50.000 + P[0][0][2]·R[0][0][2] = 0.200·70 = 14.000 = 97.000\n",
      "    Bias term = P[0][0][0]·h[0] = 0.300·55.309 = 16.593 + P[0][0][1]·h[1] = 0.500·34.444 = 17.222 + P[0][0][2]·h[2] = 0.200·0.000 = 0.000 = 33.815\n",
      "    Q(0,0) = 97.000 + 33.815 = 130.815\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    r(0,1) = P[1][0][0]·R[1][0][0] = 0.200·120 = 24.000 + P[1][0][1]·R[1][0][1] = 0.700·100 = 70.000 + P[1][0][2]·R[1][0][2] = 0.100·70 = 7.000 = 101.000\n",
      "    Bias term = P[1][0][0]·h[0] = 0.200·55.309 = 11.062 + P[1][0][1]·h[1] = 0.700·34.444 = 24.111 + P[1][0][2]·h[2] = 0.100·0.000 = 0.000 = 35.173\n",
      "    Q(0,1) = 101.000 + 35.173 = 136.173\n",
      "    Лучше предыдущего действия 0 с Q = 130.815, обновляем.\n",
      "  Действие 2 (Ничего):\n",
      "    r(0,2) = P[2][0][0]·R[2][0][0] = 0.300·110 = 33.000 + P[2][0][1]·R[2][0][1] = 0.400·80 = 32.000 + P[2][0][2]·R[2][0][2] = 0.300·50 = 15.000 = 80.000\n",
      "    Bias term = P[2][0][0]·h[0] = 0.300·55.309 = 16.593 + P[2][0][1]·h[1] = 0.400·34.444 = 13.778 + P[2][0][2]·h[2] = 0.300·0.000 = 0.000 = 30.370\n",
      "    Q(0,2) = 80.000 + 30.370 = 110.370\n",
      "    Хуже текущего лучшего действия 1 с Q = 136.173, пропускаем.\n",
      "  Лучшее действие для состояния 0: 1 (Бесплатная доставка), Q = 136.173\n",
      "  >>> Политика НЕ ИЗМЕНЕНА для состояния 0: остается 1\n",
      "\n",
      "Для состояния 1 (Хороший) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(1,0) = P[0][1][0]·R[0][1][0] = 0.200·100 = 20.000 + P[0][1][1]·R[0][1][1] = 0.600·80 = 48.000 + P[0][1][2]·R[0][1][2] = 0.200·50 = 10.000 = 78.000\n",
      "    Bias term = P[0][1][0]·h[0] = 0.200·55.309 = 11.062 + P[0][1][1]·h[1] = 0.600·34.444 = 20.667 + P[0][1][2]·h[2] = 0.200·0.000 = 0.000 = 31.728\n",
      "    Q(1,0) = 78.000 + 31.728 = 109.728\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    r(1,1) = P[1][1][0]·R[1][1][0] = 0.100·110 = 11.000 + P[1][1][1]·R[1][1][1] = 0.400·100 = 40.000 + P[1][1][2]·R[1][1][2] = 0.500·90 = 45.000 = 96.000\n",
      "    Bias term = P[1][1][0]·h[0] = 0.100·55.309 = 5.531 + P[1][1][1]·h[1] = 0.400·34.444 = 13.778 + P[1][1][2]·h[2] = 0.500·0.000 = 0.000 = 19.309\n",
      "    Q(1,1) = 96.000 + 19.309 = 115.309\n",
      "    Лучше предыдущего действия 0 с Q = 109.728, обновляем.\n",
      "  Действие 2 (Ничего):\n",
      "    r(1,2) = P[2][1][0]·R[2][1][0] = 0.200·100 = 20.000 + P[2][1][1]·R[2][1][1] = 0.600·60 = 36.000 + P[2][1][2]·R[2][1][2] = 0.200·40 = 8.000 = 64.000\n",
      "    Bias term = P[2][1][0]·h[0] = 0.200·55.309 = 11.062 + P[2][1][1]·h[1] = 0.600·34.444 = 20.667 + P[2][1][2]·h[2] = 0.200·0.000 = 0.000 = 31.728\n",
      "    Q(1,2) = 64.000 + 31.728 = 95.728\n",
      "    Хуже текущего лучшего действия 1 с Q = 115.309, пропускаем.\n",
      "  Лучшее действие для состояния 1: 1 (Бесплатная доставка), Q = 115.309\n",
      "  >>> Политика НЕ ИЗМЕНЕНА для состояния 1: остается 1\n",
      "\n",
      "Для состояния 2 (Удовлетворительный) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(2,0) = P[0][2][0]·R[0][2][0] = 0.100·80 = 8.000 + P[0][2][1]·R[0][2][1] = 0.200·60 = 12.000 + P[0][2][2]·R[0][2][2] = 0.700·40 = 28.000 = 48.000\n",
      "    Bias term = P[0][2][0]·h[0] = 0.100·55.309 = 5.531 + P[0][2][1]·h[1] = 0.200·34.444 = 6.889 + P[0][2][2]·h[2] = 0.700·0.000 = 0.000 = 12.420\n",
      "    Q(2,0) = 48.000 + 12.420 = 60.420\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    r(2,1) = P[1][2][0]·R[1][2][0] = 0.100·100 = 10.000 + P[1][2][1]·R[1][2][1] = 0.200·70 = 14.000 + P[1][2][2]·R[1][2][2] = 0.700·60 = 42.000 = 66.000\n",
      "    Bias term = P[1][2][0]·h[0] = 0.100·55.309 = 5.531 + P[1][2][1]·h[1] = 0.200·34.444 = 6.889 + P[1][2][2]·h[2] = 0.700·0.000 = 0.000 = 12.420\n",
      "    Q(2,1) = 66.000 + 12.420 = 78.420\n",
      "    Лучше предыдущего действия 0 с Q = 60.420, обновляем.\n",
      "  Действие 2 (Ничего):\n",
      "    r(2,2) = P[2][2][0]·R[2][2][0] = 0.100·80 = 8.000 + P[2][2][1]·R[2][2][1] = 0.300·70 = 21.000 + P[2][2][2]·R[2][2][2] = 0.600·60 = 36.000 = 65.000\n",
      "    Bias term = P[2][2][0]·h[0] = 0.100·55.309 = 5.531 + P[2][2][1]·h[1] = 0.300·34.444 = 10.333 + P[2][2][2]·h[2] = 0.600·0.000 = 0.000 = 15.864\n",
      "    Q(2,2) = 65.000 + 15.864 = 80.864\n",
      "    Лучше предыдущего действия 1 с Q = 78.420, обновляем.\n",
      "  Лучшее действие для состояния 2: 2 (Ничего), Q = 80.864\n",
      "  >>> Политика НЕ ИЗМЕНЕНА для состояния 2: остается 2\n",
      "\n",
      "Итог улучшения политики:\n",
      "  Было: [1, 1, 2] (['Бесплатная доставка', 'Бесплатная доставка', 'Ничего'])\n",
      "  Стало: [1, 1, 2] (['Бесплатная доставка', 'Бесплатная доставка', 'Ничего'])\n",
      "  Политика НЕ ИЗМЕНИЛАСЬ - достигнута оптимальная политика.\n",
      "\n",
      "================================================================================\n",
      "Политика не изменилась. Алгоритм завершён.\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================== Ответ ========================================\n",
      "  Состояние 0 (Отличный) → Действие 1 (Бесплатная доставка)\n",
      "  Состояние 1 (Хороший) → Действие 1 (Бесплатная доставка)\n",
      "  Состояние 2 (Удовлетворительный) → Действие 2 (Ничего)\n",
      "\n",
      "Оптимальный средний доход g* = 80.8642\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Задаём P и R по условию задачи\n",
    "P = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "R = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "num_states = 3\n",
    "actions = [0, 1, 2]  # 0=скидка, 1=доставка, 2=ничего\n",
    "state_names = [\"Отличный\", \"Хороший\", \"Удовлетворительный\"]\n",
    "action_names = [\"3% скидка\", \"Бесплатная доставка\", \"Ничего\"]\n",
    "\n",
    "def evaluate_policy(policy, verbose=True):\n",
    "    \"\"\"Policy evaluation (gain g и bias h)\"\"\"\n",
    "    m = num_states\n",
    "    # 1) Собираем P_pi и r_pi\n",
    "    P_pi = np.zeros((m, m))\n",
    "    r_pi = np.zeros(m)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"ОЦЕНКА ПОЛИТИКИ: {policy} ({[action_names[a] for a in policy]})\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n1) Собираем P_π и r_π на основе текущей политики:\")\n",
    "    \n",
    "    for i in range(m):\n",
    "        a = policy[i]\n",
    "        if verbose:\n",
    "            print(f\"\\n  Состояние {i} ({state_names[i]}) → действие {a} ({action_names[a]}):\")\n",
    "        \n",
    "        # Заполнение строк матрицы переходов P_pi\n",
    "        for j in range(m):\n",
    "            P_pi[i, j] = P[a][i][j]\n",
    "            r_pi[i] += P[a][i][j] * R[a][i][j]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"    Переход в состояние {j} ({state_names[j]}): P[{a}][{i}][{j}] = {P[a][i][j]:.3f}, \" + \n",
    "                      f\"R[{a}][{i}][{j}] = {R[a][i][j]}, вклад в r_π[{i}] += {P[a][i][j] * R[a][i][j]:.3f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n  Итоговая матрица переходов P_π:\")\n",
    "        for i in range(m):\n",
    "            print(f\"    {P_pi[i, :]}\")\n",
    "        print(\"\\n  Итоговый вектор вознаграждений r_π:\")\n",
    "        print(f\"    {r_pi}\")\n",
    "\n",
    "    # 2) Строим и решаем систему (m+1)x(m+1) на [h0..h_{m-1}, g]\n",
    "    A = np.zeros((m + 1, m + 1))\n",
    "    b = np.zeros(m + 1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n2) Строим систему уравнений (m+1)×(m+1) для нахождения [h₀, h₁, h₂, g]:\")\n",
    "        print(\"   (I - P_π)h + g·1 = r_π, с дополнительным условием h[m-1] = 0\")\n",
    "    \n",
    "    for i in range(m):\n",
    "        A[i, i] = 1.0\n",
    "        A[i, :m] -= P_pi[i, :]\n",
    "        A[i, m] = 1.0\n",
    "        b[i] = r_pi[i]\n",
    "        \n",
    "        if verbose:\n",
    "            eq_parts = []\n",
    "            for j in range(m):\n",
    "                if j == i:\n",
    "                    coef = 1.0 - P_pi[i][j]\n",
    "                else:\n",
    "                    coef = -P_pi[i][j]\n",
    "                \n",
    "                if abs(coef) > 1e-6:  # Если коэффициент не слишком мал\n",
    "                    sign = '+' if coef >= 0 and j > 0 else ''\n",
    "                    eq_parts.append(f\"{sign}{coef:.3f}·h[{j}]\")\n",
    "            \n",
    "            eq_parts.append(\"+ g\")\n",
    "            eq = \" \".join(eq_parts)\n",
    "            print(f\"    Уравнение {i+1}: {eq} = {r_pi[i]:.3f}\")\n",
    "    \n",
    "    # фиксация h[m-1] = 0\n",
    "    A[m, m - 1] = 1.0\n",
    "    b[m] = 0.0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"    Дополнительное условие: h[{m-1}] = 0\")\n",
    "        print(\"\\n  Матрица системы A:\")\n",
    "        for i in range(m+1):\n",
    "            print(f\"    {A[i, :]}\")\n",
    "        print(\"\\n  Вектор правых частей b:\")\n",
    "        print(f\"    {b}\")\n",
    "\n",
    "    # Решаем систему\n",
    "    x = np.linalg.solve(A, b)\n",
    "    h = x[:m]\n",
    "    g = x[m]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n3) Решение системы уравнений:\")\n",
    "        print(f\"  Вектор смещений h = {h}\")\n",
    "        print(f\"  Средний доход g = {g:.6f}\")\n",
    "        \n",
    "        # Проверка решения\n",
    "        print(\"\\n4) Проверка решения (должно быть приблизительно равно r_π):\")\n",
    "        for i in range(m):\n",
    "            check_sum = 0\n",
    "            for j in range(m):\n",
    "                check_sum += P_pi[i, j] * h[j]\n",
    "            check_val = h[i] - check_sum + g\n",
    "            error = abs(check_val - r_pi[i])\n",
    "            print(f\"  Уравнение {i+1}: h[{i}] - ∑_j P_π[{i},{j}]·h[j] + g = {check_val:.6f}, r_π[{i}] = {r_pi[i]:.6f}, ошибка = {error:.6e}\")\n",
    "\n",
    "    return g, h\n",
    "\n",
    "def improve_policy(policy, h, verbose=True):\n",
    "    \"\"\"Policy improvement\"\"\"\n",
    "    m = num_states\n",
    "    new_pol = policy.copy()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n\" + \"=\"*40 + \"Улучшение политики\" + \"=\"*40)\n",
    "        print(f\"\\nТекущая политика: {policy} ({[action_names[a] for a in policy]})\")\n",
    "        print(f\"Вектор смещений h = {h}\")\n",
    "    \n",
    "    for i in range(m):\n",
    "        if verbose:\n",
    "            print(f\"\\nДля состояния {i} ({state_names[i]}) ищем оптимальное действие:\")\n",
    "        \n",
    "        best_q, best_a = -1e9, None\n",
    "        for a in actions:\n",
    "            # считаем Q(i,a) = r(i,a) + sum_j P[a][i][j]·h[j]\n",
    "            r_ia = 0.0\n",
    "            bias_term = 0.0\n",
    "            \n",
    "            for j in range(m):\n",
    "                r_ia += P[a][i][j] * R[a][i][j]\n",
    "                bias_term += P[a][i][j] * h[j]\n",
    "            \n",
    "            q = r_ia + bias_term\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Действие {a} ({action_names[a]}):\")\n",
    "                # Детально показываем расчет r(i,a)\n",
    "                print(f\"    r({i},{a}) = \", end=\"\")\n",
    "                for j in range(m):\n",
    "                    print(f\"P[{a}][{i}][{j}]·R[{a}][{i}][{j}] = {P[a][i][j]:.3f}·{R[a][i][j]} = {P[a][i][j] * R[a][i][j]:.3f}\", end=\"\")\n",
    "                    if j < m-1:\n",
    "                        print(\" + \", end=\"\")\n",
    "                print(f\" = {r_ia:.3f}\")\n",
    "                \n",
    "                # Детально показываем расчет bias-терма\n",
    "                print(f\"    Bias term = \", end=\"\")\n",
    "                for j in range(m):\n",
    "                    print(f\"P[{a}][{i}][{j}]·h[{j}] = {P[a][i][j]:.3f}·{h[j]:.3f} = {P[a][i][j] * h[j]:.3f}\", end=\"\")\n",
    "                    if j < m-1:\n",
    "                        print(\" + \", end=\"\")\n",
    "                print(f\" = {bias_term:.3f}\")\n",
    "                \n",
    "                # Итоговый Q-value\n",
    "                print(f\"    Q({i},{a}) = {r_ia:.3f} + {bias_term:.3f} = {q:.3f}\")\n",
    "            \n",
    "            if q > best_q:\n",
    "                if verbose and best_a is not None:\n",
    "                    print(f\"    Лучше предыдущего действия {best_a} с Q = {best_q:.3f}, обновляем.\")\n",
    "                best_q, best_a = q, a\n",
    "            elif verbose:\n",
    "                print(f\"    Хуже текущего лучшего действия {best_a} с Q = {best_q:.3f}, пропускаем.\")\n",
    "        \n",
    "        new_pol[i] = best_a\n",
    "        \n",
    "        if verbose:\n",
    "            action_changed = policy[i] != new_pol[i]\n",
    "            print(f\"  Лучшее действие для состояния {i}: {best_a} ({action_names[best_a]}), Q = {best_q:.3f}\")\n",
    "            if action_changed:\n",
    "                print(f\"  >>> Политика ИЗМЕНЕНА для состояния {i}: {policy[i]} → {new_pol[i]}\")\n",
    "            else:\n",
    "                print(f\"  >>> Политика НЕ ИЗМЕНЕНА для состояния {i}: остается {policy[i]}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nИтог улучшения политики:\")\n",
    "        print(f\"  Было: {policy} ({[action_names[a] for a in policy]})\")\n",
    "        print(f\"  Стало: {new_pol} ({[action_names[a] for a in new_pol]})\")\n",
    "        if policy == new_pol:\n",
    "            print(\"  Политика НЕ ИЗМЕНИЛАСЬ - достигнута оптимальная политика.\")\n",
    "        else:\n",
    "            print(\"  Политика ИЗМЕНИЛАСЬ - требуется продолжить итерации.\")\n",
    "            \n",
    "    return new_pol\n",
    "\n",
    "def policy_iteration():\n",
    "    policy = [0] * num_states  # стартуем, например, всегда со \"скидки\"\n",
    "    iteration = 0\n",
    "    \n",
    "    print(\"\\nНачальная политика:\")\n",
    "    for i, a in enumerate(policy):\n",
    "        print(f\"  Состояние {i} ({state_names[i]}) → Действие {a} ({action_names[a]})\")\n",
    "    \n",
    "    while True:\n",
    "        print(f\"\\n{'*'*40} Итерация {iteration+1} {'*'*40}\")\n",
    "        \n",
    "        print(f\"\\nТекущая политика:\")\n",
    "        for i, a in enumerate(policy):\n",
    "            print(f\"  {i} ({state_names[i]}) → {a} ({action_names[a]})\")\n",
    "\n",
    "        # Оценка этой политики\n",
    "        g, h = evaluate_policy(policy)\n",
    "        print(f\"\\nОценка текущей политики:\")\n",
    "        print(f\"  Средний доход g = {g:.4f}\")\n",
    "\n",
    "        # Улучшаем политику\n",
    "        new_pol = improve_policy(policy, h)\n",
    "\n",
    "        # Если не изменилось — готово\n",
    "        if new_pol == policy:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"Политика не изменилась. Алгоритм завершён.\")\n",
    "            print(\"=\"*80)\n",
    "            break\n",
    "\n",
    "        policy = new_pol\n",
    "        iteration += 1\n",
    "\n",
    "    return policy, g, h\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt_policy, opt_gain, opt_h = policy_iteration()\n",
    "    print(f\"\\n\\n\\n{'='*40} Ответ {'='*40}\")\n",
    "\n",
    "    for i, a in enumerate(opt_policy):\n",
    "        print(f\"  Состояние {i} ({state_names[i]}) → Действие {a} ({action_names[a]})\")\n",
    "    print(f\"\\nОптимальный средний доход g* = {opt_gain:.4f}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e1ec5",
   "metadata": {},
   "source": [
    "___\n",
    "## 4 Метод итерации по стратегии с дисконтированием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "669a51ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Коэффициент дисконтирования γ = 0.7\n",
      "\n",
      "Начальная политика:\n",
      "  Состояние 0 (Отл.) → Действие 0 (3% скидка)\n",
      "  Состояние 1 (Хор.) → Действие 0 (3% скидка)\n",
      "  Состояние 2 (Уд.) → Действие 0 (3% скидка)\n",
      "\n",
      "**************************************** Итерация 1 ****************************************\n",
      "\n",
      "Текущая политика:\n",
      "  0 (Отл.) → 0 (3% скидка)\n",
      "  1 (Хор.) → 0 (3% скидка)\n",
      "  2 (Уд.) → 0 (3% скидка)\n",
      "\n",
      "========================================ОЦЕНКА ПОЛИТИКИ========================================\n",
      "Действия политики: ['3% скидка', '3% скидка', '3% скидка']\n",
      "\n",
      "1) Создаём матрицу переходов Pπ и вектор вознаграждений rπ для текущей политики:\n",
      "\n",
      "  Состояние 0 (Отл.) → действие 0 (3% скидка):\n",
      "    Переход в 0 (Отл.): P[0][0][0] = 0.300, R[0][0][0] = 110, вклад в rπ[0]: 33.000\n",
      "    Переход в 1 (Хор.): P[0][0][1] = 0.500, R[0][0][1] = 100, вклад в rπ[0]: 50.000\n",
      "    Переход в 2 (Уд.): P[0][0][2] = 0.200, R[0][0][2] = 70, вклад в rπ[0]: 14.000\n",
      "\n",
      "  Состояние 1 (Хор.) → действие 0 (3% скидка):\n",
      "    Переход в 0 (Отл.): P[0][1][0] = 0.200, R[0][1][0] = 100, вклад в rπ[1]: 20.000\n",
      "    Переход в 1 (Хор.): P[0][1][1] = 0.600, R[0][1][1] = 80, вклад в rπ[1]: 48.000\n",
      "    Переход в 2 (Уд.): P[0][1][2] = 0.200, R[0][1][2] = 50, вклад в rπ[1]: 10.000\n",
      "\n",
      "  Состояние 2 (Уд.) → действие 0 (3% скидка):\n",
      "    Переход в 0 (Отл.): P[0][2][0] = 0.100, R[0][2][0] = 80, вклад в rπ[2]: 8.000\n",
      "    Переход в 1 (Хор.): P[0][2][1] = 0.200, R[0][2][1] = 60, вклад в rπ[2]: 12.000\n",
      "    Переход в 2 (Уд.): P[0][2][2] = 0.700, R[0][2][2] = 40, вклад в rπ[2]: 28.000\n",
      "\n",
      "  Итоговая матрица переходов Pπ:\n",
      "    [0.3 0.5 0.2]\n",
      "    [0.2 0.6 0.2]\n",
      "    [0.1 0.2 0.7]\n",
      "\n",
      "  Итоговый вектор вознаграждений rπ:\n",
      "    [97. 78. 48.]\n",
      "\n",
      "2) Формируем систему уравнений (I - γ·Pπ)·V = rπ:\n",
      "\n",
      "  Матрица γ·Pπ (γ = 0.7):\n",
      "    [0.21 0.35 0.14]\n",
      "    [0.14 0.42 0.14]\n",
      "    [0.07 0.14 0.49]\n",
      "\n",
      "  Матрица системы A = I - γ·Pπ:\n",
      "    [ 0.79 -0.35 -0.14]\n",
      "    [-0.14  0.58 -0.14]\n",
      "    [-0.07 -0.14  0.51]\n",
      "\n",
      "  Система уравнений:\n",
      "    0.790·V[0] -0.350·V[1] -0.140·V[2] = 97.000\n",
      "    -0.140·V[0] +0.580·V[1] -0.140·V[2] = 78.000\n",
      "    -0.070·V[0] -0.140·V[1] +0.510·V[2] = 48.000\n",
      "\n",
      "3) Решение системы дает значения функции ценности V:\n",
      "    V[0] (Отл.) = 267.398952\n",
      "    V[1] (Хор.) = 246.968845\n",
      "    V[2] (Уд.) = 198.614833\n",
      "\n",
      "4) Проверка решения (A·V должно быть ≈ rπ):\n",
      "    Уравнение 1: 97.000000 ≈ 97.000000, ошибка = 1.421085e-14\n",
      "    Уравнение 2: 78.000000 ≈ 78.000000, ошибка = 1.421085e-14\n",
      "    Уравнение 3: 48.000000 ≈ 48.000000, ошибка = 0.000000e+00\n",
      "\n",
      "Результат оценки политики:\n",
      "  V = [267.399 246.969 198.615]\n",
      "\n",
      "========================================Улучшение политики========================================\n",
      "\n",
      "Текущая политика: [0, 0, 0] (['3% скидка', '3% скидка', '3% скидка'])\n",
      "Текущие значения V: [267.399  246.9688 198.6148]\n",
      "\n",
      "Для состояния 0 (Отл.) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(0,0) = P[0][0][0]·R[0][0][0] = 0.300·110 = 33.000 + P[0][0][1]·R[0][0][1] = 0.500·100 = 50.000 + P[0][0][2]·R[0][0][2] = 0.200·70 = 14.000 = 97.000\n",
      "    γ·∑_j P[0][0][j]·V[j] = γ·P[0][0][0]·V[0] = 0.7·0.300·267.399 = 56.154 + γ·P[0][0][1]·V[1] = 0.7·0.500·246.969 = 86.439 + γ·P[0][0][2]·V[2] = 0.7·0.200·198.615 = 27.806 = 170.399\n",
      "    Q(0,0) = 97.000 + 170.399 = 267.399\n",
      "  Действие 1 (доставка):\n",
      "    r(0,1) = P[1][0][0]·R[1][0][0] = 0.200·120 = 24.000 + P[1][0][1]·R[1][0][1] = 0.700·100 = 70.000 + P[1][0][2]·R[1][0][2] = 0.100·70 = 7.000 = 101.000\n",
      "    γ·∑_j P[1][0][j]·V[j] = γ·P[1][0][0]·V[0] = 0.7·0.200·267.399 = 37.436 + γ·P[1][0][1]·V[1] = 0.7·0.700·246.969 = 121.015 + γ·P[1][0][2]·V[2] = 0.7·0.100·198.615 = 13.903 = 172.354\n",
      "    Q(0,1) = 101.000 + 172.354 = 273.354\n",
      "    Лучше предыдущего действия 0 с Q=267.399, обновляем.\n",
      "  Действие 2 (ничего):\n",
      "    r(0,2) = P[2][0][0]·R[2][0][0] = 0.300·110 = 33.000 + P[2][0][1]·R[2][0][1] = 0.400·80 = 32.000 + P[2][0][2]·R[2][0][2] = 0.300·50 = 15.000 = 80.000\n",
      "    γ·∑_j P[2][0][j]·V[j] = γ·P[2][0][0]·V[0] = 0.7·0.300·267.399 = 56.154 + γ·P[2][0][1]·V[1] = 0.7·0.400·246.969 = 69.151 + γ·P[2][0][2]·V[2] = 0.7·0.300·198.615 = 41.709 = 167.014\n",
      "    Q(0,2) = 80.000 + 167.014 = 247.014\n",
      "    Хуже текущего лучшего действия 1 с Q=273.354, пропускаем.\n",
      "  Лучшее действие для состояния 0: 1 (доставка), Q=273.354\n",
      "  Q-значения всех действий: [267.399 273.354 247.014]\n",
      "  >>> Политика ИЗМЕНЕНА для состояния 0: 0 → 1\n",
      "\n",
      "Для состояния 1 (Хор.) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(1,0) = P[0][1][0]·R[0][1][0] = 0.200·100 = 20.000 + P[0][1][1]·R[0][1][1] = 0.600·80 = 48.000 + P[0][1][2]·R[0][1][2] = 0.200·50 = 10.000 = 78.000\n",
      "    γ·∑_j P[0][1][j]·V[j] = γ·P[0][1][0]·V[0] = 0.7·0.200·267.399 = 37.436 + γ·P[0][1][1]·V[1] = 0.7·0.600·246.969 = 103.727 + γ·P[0][1][2]·V[2] = 0.7·0.200·198.615 = 27.806 = 168.969\n",
      "    Q(1,0) = 78.000 + 168.969 = 246.969\n",
      "  Действие 1 (доставка):\n",
      "    r(1,1) = P[1][1][0]·R[1][1][0] = 0.100·110 = 11.000 + P[1][1][1]·R[1][1][1] = 0.400·100 = 40.000 + P[1][1][2]·R[1][1][2] = 0.500·90 = 45.000 = 96.000\n",
      "    γ·∑_j P[1][1][j]·V[j] = γ·P[1][1][0]·V[0] = 0.7·0.100·267.399 = 18.718 + γ·P[1][1][1]·V[1] = 0.7·0.400·246.969 = 69.151 + γ·P[1][1][2]·V[2] = 0.7·0.500·198.615 = 69.515 = 157.384\n",
      "    Q(1,1) = 96.000 + 157.384 = 253.384\n",
      "    Лучше предыдущего действия 0 с Q=246.969, обновляем.\n",
      "  Действие 2 (ничего):\n",
      "    r(1,2) = P[2][1][0]·R[2][1][0] = 0.200·100 = 20.000 + P[2][1][1]·R[2][1][1] = 0.600·60 = 36.000 + P[2][1][2]·R[2][1][2] = 0.200·40 = 8.000 = 64.000\n",
      "    γ·∑_j P[2][1][j]·V[j] = γ·P[2][1][0]·V[0] = 0.7·0.200·267.399 = 37.436 + γ·P[2][1][1]·V[1] = 0.7·0.600·246.969 = 103.727 + γ·P[2][1][2]·V[2] = 0.7·0.200·198.615 = 27.806 = 168.969\n",
      "    Q(1,2) = 64.000 + 168.969 = 232.969\n",
      "    Хуже текущего лучшего действия 1 с Q=253.384, пропускаем.\n",
      "  Лучшее действие для состояния 1: 1 (доставка), Q=253.384\n",
      "  Q-значения всех действий: [246.969 253.384 232.969]\n",
      "  >>> Политика ИЗМЕНЕНА для состояния 1: 0 → 1\n",
      "\n",
      "Для состояния 2 (Уд.) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(2,0) = P[0][2][0]·R[0][2][0] = 0.100·80 = 8.000 + P[0][2][1]·R[0][2][1] = 0.200·60 = 12.000 + P[0][2][2]·R[0][2][2] = 0.700·40 = 28.000 = 48.000\n",
      "    γ·∑_j P[0][2][j]·V[j] = γ·P[0][2][0]·V[0] = 0.7·0.100·267.399 = 18.718 + γ·P[0][2][1]·V[1] = 0.7·0.200·246.969 = 34.576 + γ·P[0][2][2]·V[2] = 0.7·0.700·198.615 = 97.321 = 150.615\n",
      "    Q(2,0) = 48.000 + 150.615 = 198.615\n",
      "  Действие 1 (доставка):\n",
      "    r(2,1) = P[1][2][0]·R[1][2][0] = 0.100·100 = 10.000 + P[1][2][1]·R[1][2][1] = 0.200·70 = 14.000 + P[1][2][2]·R[1][2][2] = 0.700·60 = 42.000 = 66.000\n",
      "    γ·∑_j P[1][2][j]·V[j] = γ·P[1][2][0]·V[0] = 0.7·0.100·267.399 = 18.718 + γ·P[1][2][1]·V[1] = 0.7·0.200·246.969 = 34.576 + γ·P[1][2][2]·V[2] = 0.7·0.700·198.615 = 97.321 = 150.615\n",
      "    Q(2,1) = 66.000 + 150.615 = 216.615\n",
      "    Лучше предыдущего действия 0 с Q=198.615, обновляем.\n",
      "  Действие 2 (ничего):\n",
      "    r(2,2) = P[2][2][0]·R[2][2][0] = 0.100·80 = 8.000 + P[2][2][1]·R[2][2][1] = 0.300·70 = 21.000 + P[2][2][2]·R[2][2][2] = 0.600·60 = 36.000 = 65.000\n",
      "    γ·∑_j P[2][2][j]·V[j] = γ·P[2][2][0]·V[0] = 0.7·0.100·267.399 = 18.718 + γ·P[2][2][1]·V[1] = 0.7·0.300·246.969 = 51.863 + γ·P[2][2][2]·V[2] = 0.7·0.600·198.615 = 83.418 = 154.000\n",
      "    Q(2,2) = 65.000 + 154.000 = 219.000\n",
      "    Лучше предыдущего действия 1 с Q=216.615, обновляем.\n",
      "  Лучшее действие для состояния 2: 2 (ничего), Q=219.000\n",
      "  Q-значения всех действий: [198.615 216.615 219.   ]\n",
      "  >>> Политика ИЗМЕНЕНА для состояния 2: 0 → 2\n",
      "\n",
      "Итог улучшения политики:\n",
      "  Было: [0, 0, 0] (['3% скидка', '3% скидка', '3% скидка'])\n",
      "  Стало: [1, 1, 2] (['доставка', 'доставка', 'ничего'])\n",
      "  Политика ИЗМЕНИЛАСЬ - требуется продолжить итерации.\n",
      "\n",
      "Изменения в политике:\n",
      "  Состояние 0 (Отл.): 0 (3% скидка) → 1 (доставка)\n",
      "  Состояние 1 (Хор.): 0 (3% скидка) → 1 (доставка)\n",
      "  Состояние 2 (Уд.): 0 (3% скидка) → 2 (ничего)\n",
      "\n",
      "**************************************** Итерация 2 ****************************************\n",
      "\n",
      "Текущая политика:\n",
      "  0 (Отл.) → 1 (доставка)\n",
      "  1 (Хор.) → 1 (доставка)\n",
      "  2 (Уд.) → 2 (ничего)\n",
      "\n",
      "========================================ОЦЕНКА ПОЛИТИКИ========================================\n",
      "Действия политики: ['доставка', 'доставка', 'ничего']\n",
      "\n",
      "1) Создаём матрицу переходов Pπ и вектор вознаграждений rπ для текущей политики:\n",
      "\n",
      "  Состояние 0 (Отл.) → действие 1 (доставка):\n",
      "    Переход в 0 (Отл.): P[1][0][0] = 0.200, R[1][0][0] = 120, вклад в rπ[0]: 24.000\n",
      "    Переход в 1 (Хор.): P[1][0][1] = 0.700, R[1][0][1] = 100, вклад в rπ[0]: 70.000\n",
      "    Переход в 2 (Уд.): P[1][0][2] = 0.100, R[1][0][2] = 70, вклад в rπ[0]: 7.000\n",
      "\n",
      "  Состояние 1 (Хор.) → действие 1 (доставка):\n",
      "    Переход в 0 (Отл.): P[1][1][0] = 0.100, R[1][1][0] = 110, вклад в rπ[1]: 11.000\n",
      "    Переход в 1 (Хор.): P[1][1][1] = 0.400, R[1][1][1] = 100, вклад в rπ[1]: 40.000\n",
      "    Переход в 2 (Уд.): P[1][1][2] = 0.500, R[1][1][2] = 90, вклад в rπ[1]: 45.000\n",
      "\n",
      "  Состояние 2 (Уд.) → действие 2 (ничего):\n",
      "    Переход в 0 (Отл.): P[2][2][0] = 0.100, R[2][2][0] = 80, вклад в rπ[2]: 8.000\n",
      "    Переход в 1 (Хор.): P[2][2][1] = 0.300, R[2][2][1] = 70, вклад в rπ[2]: 21.000\n",
      "    Переход в 2 (Уд.): P[2][2][2] = 0.600, R[2][2][2] = 60, вклад в rπ[2]: 36.000\n",
      "\n",
      "  Итоговая матрица переходов Pπ:\n",
      "    [0.2 0.7 0.1]\n",
      "    [0.1 0.4 0.5]\n",
      "    [0.1 0.3 0.6]\n",
      "\n",
      "  Итоговый вектор вознаграждений rπ:\n",
      "    [101.  96.  65.]\n",
      "\n",
      "2) Формируем систему уравнений (I - γ·Pπ)·V = rπ:\n",
      "\n",
      "  Матрица γ·Pπ (γ = 0.7):\n",
      "    [0.14 0.49 0.07]\n",
      "    [0.07 0.28 0.35]\n",
      "    [0.07 0.21 0.42]\n",
      "\n",
      "  Матрица системы A = I - γ·Pπ:\n",
      "    [ 0.86 -0.49 -0.07]\n",
      "    [-0.07  0.72 -0.35]\n",
      "    [-0.07 -0.21  0.58]\n",
      "\n",
      "  Система уравнений:\n",
      "    0.860·V[0] -0.490·V[1] -0.070·V[2] = 101.000\n",
      "    -0.070·V[0] +0.720·V[1] -0.350·V[2] = 96.000\n",
      "    -0.070·V[0] -0.210·V[1] +0.580·V[2] = 65.000\n",
      "\n",
      "3) Решение системы дает значения функции ценности V:\n",
      "    V[0] (Отл.) = 300.119474\n",
      "    V[1] (Хор.) = 284.707288\n",
      "    V[2] (Уд.) = 251.373955\n",
      "\n",
      "4) Проверка решения (A·V должно быть ≈ rπ):\n",
      "    Уравнение 1: 101.000000 ≈ 101.000000, ошибка = 1.421085e-14\n",
      "    Уравнение 2: 96.000000 ≈ 96.000000, ошибка = 1.421085e-14\n",
      "    Уравнение 3: 65.000000 ≈ 65.000000, ошибка = 0.000000e+00\n",
      "\n",
      "Результат оценки политики:\n",
      "  V = [300.119 284.707 251.374]\n",
      "\n",
      "========================================Улучшение политики========================================\n",
      "\n",
      "Текущая политика: [1, 1, 2] (['доставка', 'доставка', 'ничего'])\n",
      "Текущие значения V: [300.1195 284.7073 251.374 ]\n",
      "\n",
      "Для состояния 0 (Отл.) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(0,0) = P[0][0][0]·R[0][0][0] = 0.300·110 = 33.000 + P[0][0][1]·R[0][0][1] = 0.500·100 = 50.000 + P[0][0][2]·R[0][0][2] = 0.200·70 = 14.000 = 97.000\n",
      "    γ·∑_j P[0][0][j]·V[j] = γ·P[0][0][0]·V[0] = 0.7·0.300·300.119 = 63.025 + γ·P[0][0][1]·V[1] = 0.7·0.500·284.707 = 99.648 + γ·P[0][0][2]·V[2] = 0.7·0.200·251.374 = 35.192 = 197.865\n",
      "    Q(0,0) = 97.000 + 197.865 = 294.865\n",
      "  Действие 1 (доставка):\n",
      "    r(0,1) = P[1][0][0]·R[1][0][0] = 0.200·120 = 24.000 + P[1][0][1]·R[1][0][1] = 0.700·100 = 70.000 + P[1][0][2]·R[1][0][2] = 0.100·70 = 7.000 = 101.000\n",
      "    γ·∑_j P[1][0][j]·V[j] = γ·P[1][0][0]·V[0] = 0.7·0.200·300.119 = 42.017 + γ·P[1][0][1]·V[1] = 0.7·0.700·284.707 = 139.507 + γ·P[1][0][2]·V[2] = 0.7·0.100·251.374 = 17.596 = 199.119\n",
      "    Q(0,1) = 101.000 + 199.119 = 300.119\n",
      "    Лучше предыдущего действия 0 с Q=294.865, обновляем.\n",
      "  Действие 2 (ничего):\n",
      "    r(0,2) = P[2][0][0]·R[2][0][0] = 0.300·110 = 33.000 + P[2][0][1]·R[2][0][1] = 0.400·80 = 32.000 + P[2][0][2]·R[2][0][2] = 0.300·50 = 15.000 = 80.000\n",
      "    γ·∑_j P[2][0][j]·V[j] = γ·P[2][0][0]·V[0] = 0.7·0.300·300.119 = 63.025 + γ·P[2][0][1]·V[1] = 0.7·0.400·284.707 = 79.718 + γ·P[2][0][2]·V[2] = 0.7·0.300·251.374 = 52.789 = 195.532\n",
      "    Q(0,2) = 80.000 + 195.532 = 275.532\n",
      "    Хуже текущего лучшего действия 1 с Q=300.119, пропускаем.\n",
      "  Лучшее действие для состояния 0: 1 (доставка), Q=300.119\n",
      "  Q-значения всех действий: [294.865 300.119 275.532]\n",
      "  >>> Политика НЕ ИЗМЕНЕНА для состояния 0: остается 1\n",
      "\n",
      "Для состояния 1 (Хор.) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(1,0) = P[0][1][0]·R[0][1][0] = 0.200·100 = 20.000 + P[0][1][1]·R[0][1][1] = 0.600·80 = 48.000 + P[0][1][2]·R[0][1][2] = 0.200·50 = 10.000 = 78.000\n",
      "    γ·∑_j P[0][1][j]·V[j] = γ·P[0][1][0]·V[0] = 0.7·0.200·300.119 = 42.017 + γ·P[0][1][1]·V[1] = 0.7·0.600·284.707 = 119.577 + γ·P[0][1][2]·V[2] = 0.7·0.200·251.374 = 35.192 = 196.786\n",
      "    Q(1,0) = 78.000 + 196.786 = 274.786\n",
      "  Действие 1 (доставка):\n",
      "    r(1,1) = P[1][1][0]·R[1][1][0] = 0.100·110 = 11.000 + P[1][1][1]·R[1][1][1] = 0.400·100 = 40.000 + P[1][1][2]·R[1][1][2] = 0.500·90 = 45.000 = 96.000\n",
      "    γ·∑_j P[1][1][j]·V[j] = γ·P[1][1][0]·V[0] = 0.7·0.100·300.119 = 21.008 + γ·P[1][1][1]·V[1] = 0.7·0.400·284.707 = 79.718 + γ·P[1][1][2]·V[2] = 0.7·0.500·251.374 = 87.981 = 188.707\n",
      "    Q(1,1) = 96.000 + 188.707 = 284.707\n",
      "    Лучше предыдущего действия 0 с Q=274.786, обновляем.\n",
      "  Действие 2 (ничего):\n",
      "    r(1,2) = P[2][1][0]·R[2][1][0] = 0.200·100 = 20.000 + P[2][1][1]·R[2][1][1] = 0.600·60 = 36.000 + P[2][1][2]·R[2][1][2] = 0.200·40 = 8.000 = 64.000\n",
      "    γ·∑_j P[2][1][j]·V[j] = γ·P[2][1][0]·V[0] = 0.7·0.200·300.119 = 42.017 + γ·P[2][1][1]·V[1] = 0.7·0.600·284.707 = 119.577 + γ·P[2][1][2]·V[2] = 0.7·0.200·251.374 = 35.192 = 196.786\n",
      "    Q(1,2) = 64.000 + 196.786 = 260.786\n",
      "    Хуже текущего лучшего действия 1 с Q=284.707, пропускаем.\n",
      "  Лучшее действие для состояния 1: 1 (доставка), Q=284.707\n",
      "  Q-значения всех действий: [274.786 284.707 260.786]\n",
      "  >>> Политика НЕ ИЗМЕНЕНА для состояния 1: остается 1\n",
      "\n",
      "Для состояния 2 (Уд.) ищем оптимальное действие:\n",
      "  Действие 0 (3% скидка):\n",
      "    r(2,0) = P[0][2][0]·R[0][2][0] = 0.100·80 = 8.000 + P[0][2][1]·R[0][2][1] = 0.200·60 = 12.000 + P[0][2][2]·R[0][2][2] = 0.700·40 = 28.000 = 48.000\n",
      "    γ·∑_j P[0][2][j]·V[j] = γ·P[0][2][0]·V[0] = 0.7·0.100·300.119 = 21.008 + γ·P[0][2][1]·V[1] = 0.7·0.200·284.707 = 39.859 + γ·P[0][2][2]·V[2] = 0.7·0.700·251.374 = 123.173 = 184.041\n",
      "    Q(2,0) = 48.000 + 184.041 = 232.041\n",
      "  Действие 1 (доставка):\n",
      "    r(2,1) = P[1][2][0]·R[1][2][0] = 0.100·100 = 10.000 + P[1][2][1]·R[1][2][1] = 0.200·70 = 14.000 + P[1][2][2]·R[1][2][2] = 0.700·60 = 42.000 = 66.000\n",
      "    γ·∑_j P[1][2][j]·V[j] = γ·P[1][2][0]·V[0] = 0.7·0.100·300.119 = 21.008 + γ·P[1][2][1]·V[1] = 0.7·0.200·284.707 = 39.859 + γ·P[1][2][2]·V[2] = 0.7·0.700·251.374 = 123.173 = 184.041\n",
      "    Q(2,1) = 66.000 + 184.041 = 250.041\n",
      "    Лучше предыдущего действия 0 с Q=232.041, обновляем.\n",
      "  Действие 2 (ничего):\n",
      "    r(2,2) = P[2][2][0]·R[2][2][0] = 0.100·80 = 8.000 + P[2][2][1]·R[2][2][1] = 0.300·70 = 21.000 + P[2][2][2]·R[2][2][2] = 0.600·60 = 36.000 = 65.000\n",
      "    γ·∑_j P[2][2][j]·V[j] = γ·P[2][2][0]·V[0] = 0.7·0.100·300.119 = 21.008 + γ·P[2][2][1]·V[1] = 0.7·0.300·284.707 = 59.789 + γ·P[2][2][2]·V[2] = 0.7·0.600·251.374 = 105.577 = 186.374\n",
      "    Q(2,2) = 65.000 + 186.374 = 251.374\n",
      "    Лучше предыдущего действия 1 с Q=250.041, обновляем.\n",
      "  Лучшее действие для состояния 2: 2 (ничего), Q=251.374\n",
      "  Q-значения всех действий: [232.041 250.041 251.374]\n",
      "  >>> Политика НЕ ИЗМЕНЕНА для состояния 2: остается 2\n",
      "\n",
      "Итог улучшения политики:\n",
      "  Было: [1, 1, 2] (['доставка', 'доставка', 'ничего'])\n",
      "  Стало: [1, 1, 2] (['доставка', 'доставка', 'ничего'])\n",
      "  Политика НЕ ИЗМЕНИЛАСЬ - достигнута оптимальная политика.\n",
      "\n",
      "================================================================================\n",
      "Политика не изменилась. Алгоритм завершён.\n",
      "================================================================================\n",
      "\n",
      "======================================== Ответ ========================================\n",
      "\n",
      "Оптимальная политика:\n",
      "  Состояние 0 (Отл.) → Действие 1 (доставка)\n",
      "  Состояние 1 (Хор.) → Действие 1 (доставка)\n",
      "  Состояние 2 (Уд.) → Действие 2 (ничего)\n",
      "\n",
      "Оптимальная функция ценности:\n",
      "  V*[0] (Отл.) = 300.119474\n",
      "  V*[1] (Хор.) = 284.707288\n",
      "  V*[2] (Уд.) = 251.373955\n",
      "\n",
      "Стоимость состояний V* в округлённом виде:\n",
      "  [300.119 284.707 251.374]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Задаём P и R по условию\n",
    "P = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "R = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "num_states = 3\n",
    "actions = [0, 1, 2]  # 0=скидка, 1=доставка, 2=ничего\n",
    "state_names = [\"Отл.\", \"Хор.\", \"Уд.\"]\n",
    "action_names = [\"3% скидка\", \"доставка\", \"ничего\"]\n",
    "\n",
    "γ = 0.7  # коэффициент дисконтирования\n",
    "\n",
    "def evaluate_policy_discount(policy, gamma=γ, verbose=True):\n",
    "    \"\"\"\n",
    "    Решаем (I - γ Pπ) V = rπ\n",
    "    возвращаем вектор V размера m.\n",
    "    \"\"\"\n",
    "    m = num_states\n",
    "    Pπ = np.zeros((m, m))\n",
    "    rπ = np.zeros(m)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*40 + \"ОЦЕНКА ПОЛИТИКИ\" + \"=\"*40)\n",
    "        print(f\"Действия политики: {[action_names[a] for a in policy]}\")\n",
    "        print(\"\\n1) Создаём матрицу переходов Pπ и вектор вознаграждений rπ для текущей политики:\")\n",
    "    \n",
    "    for i in range(m):\n",
    "        a = policy[i]\n",
    "        if verbose:\n",
    "            print(f\"\\n  Состояние {i} ({state_names[i]}) → действие {a} ({action_names[a]}):\")\n",
    "        \n",
    "        for j in range(m):\n",
    "            Pπ[i, j] = P[a][i][j]\n",
    "            rπ[i] += P[a][i][j] * R[a][i][j]\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"    Переход в {j} ({state_names[j]}): P[{a}][{i}][{j}] = {P[a][i][j]:.3f}, \" +\n",
    "                      f\"R[{a}][{i}][{j}] = {R[a][i][j]}, вклад в rπ[{i}]: {P[a][i][j] * R[a][i][j]:.3f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n  Итоговая матрица переходов Pπ:\")\n",
    "        for i in range(m):\n",
    "            print(f\"    {Pπ[i, :]}\")\n",
    "            \n",
    "        print(\"\\n  Итоговый вектор вознаграждений rπ:\")\n",
    "        print(f\"    {rπ}\")\n",
    "        \n",
    "        print(\"\\n2) Формируем систему уравнений (I - γ·Pπ)·V = rπ:\")\n",
    "    \n",
    "    # матрица I - γ·Pπ\n",
    "    A = np.eye(m) - gamma * Pπ\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n  Матрица γ·Pπ (γ = {gamma}):\")\n",
    "        for i in range(m):\n",
    "            print(f\"    {gamma * Pπ[i, :]}\")\n",
    "            \n",
    "        print(\"\\n  Матрица системы A = I - γ·Pπ:\")\n",
    "        for i in range(m):\n",
    "            print(f\"    {A[i, :]}\")\n",
    "            \n",
    "        print(\"\\n  Система уравнений:\")\n",
    "        for i in range(m):\n",
    "            eq_parts = []\n",
    "            for j in range(m):\n",
    "                if abs(A[i, j]) > 1e-10:\n",
    "                    sign = \"+\" if A[i, j] > 0 and j > 0 else \"\"\n",
    "                    eq_parts.append(f\"{sign}{A[i, j]:.3f}·V[{j}]\")\n",
    "            \n",
    "            eq = \" \".join(eq_parts)\n",
    "            print(f\"    {eq} = {rπ[i]:.3f}\")\n",
    "    \n",
    "    # Решаем систему\n",
    "    V = np.linalg.solve(A, rπ)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n3) Решение системы дает значения функции ценности V:\")\n",
    "        for i in range(m):\n",
    "            print(f\"    V[{i}] ({state_names[i]}) = {V[i]:.6f}\")\n",
    "            \n",
    "        # Проверка решения\n",
    "        print(\"\\n4) Проверка решения (A·V должно быть ≈ rπ):\")\n",
    "        for i in range(m):\n",
    "            check_val = 0\n",
    "            for j in range(m):\n",
    "                check_val += A[i, j] * V[j]\n",
    "            error = abs(check_val - rπ[i])\n",
    "            print(f\"    Уравнение {i+1}: {check_val:.6f} ≈ {rπ[i]:.6f}, ошибка = {error:.6e}\")\n",
    "    \n",
    "    return V\n",
    "\n",
    "def improve_policy_discount(policy, V, gamma=γ, verbose=True):\n",
    "    \"\"\"\n",
    "    Для каждого состояния i находим действие a, максимизирующее\n",
    "    Q(i,a) = r(i,a) + γ ∑_j P[a][i][j]·V[j]\n",
    "    \"\"\"\n",
    "    m = num_states\n",
    "    new_pol = policy.copy()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*40 + \"Улучшение политики\" + \"=\"*40)\n",
    "        print(f\"\\nТекущая политика: {policy} ({[action_names[a] for a in policy]})\")\n",
    "        print(f\"Текущие значения V: {np.round(V, 4)}\")\n",
    "    \n",
    "    for i in range(m):\n",
    "        if verbose:\n",
    "            print(f\"\\nДля состояния {i} ({state_names[i]}) ищем оптимальное действие:\")\n",
    "        \n",
    "        best_q, best_a = -float('inf'), None\n",
    "        q_values = []\n",
    "        \n",
    "        for a in actions:\n",
    "            # Вычисляем Q(i,a) = r(i,a) + γ·∑_j P[a][i][j]·V[j]\n",
    "            # 1. Мгновенное вознаграждение r(i,a)\n",
    "            r_ia = 0.0\n",
    "            for j in range(m):\n",
    "                r_ia += P[a][i][j] * R[a][i][j]\n",
    "            \n",
    "            # 2. Дисконтированное будущее вознаграждение\n",
    "            future_val = 0.0\n",
    "            for j in range(m):\n",
    "                future_val += gamma * P[a][i][j] * V[j]\n",
    "            \n",
    "            # 3. Суммарное Q-значение\n",
    "            q = r_ia + future_val\n",
    "            q_values.append(q)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Действие {a} ({action_names[a]}):\")\n",
    "                \n",
    "                # Детально показываем вычисление r(i,a)\n",
    "                print(f\"    r({i},{a}) = \", end=\"\")\n",
    "                for j in range(m):\n",
    "                    print(f\"P[{a}][{i}][{j}]·R[{a}][{i}][{j}] = {P[a][i][j]:.3f}·{R[a][i][j]} = {P[a][i][j] * R[a][i][j]:.3f}\", end=\"\")\n",
    "                    if j < m-1:\n",
    "                        print(\" + \", end=\"\")\n",
    "                print(f\" = {r_ia:.3f}\")\n",
    "                \n",
    "                # Детально показываем вычисление дисконтированного будущего\n",
    "                print(f\"    γ·∑_j P[{a}][{i}][j]·V[j] = \", end=\"\")\n",
    "                for j in range(m):\n",
    "                    print(f\"γ·P[{a}][{i}][{j}]·V[{j}] = {gamma:.1f}·{P[a][i][j]:.3f}·{V[j]:.3f} = {gamma * P[a][i][j] * V[j]:.3f}\", end=\"\")\n",
    "                    if j < m-1:\n",
    "                        print(\" + \", end=\"\")\n",
    "                print(f\" = {future_val:.3f}\")\n",
    "                \n",
    "                # Итоговое Q-значение\n",
    "                print(f\"    Q({i},{a}) = {r_ia:.3f} + {future_val:.3f} = {q:.3f}\")\n",
    "            \n",
    "            if q > best_q:\n",
    "                if verbose and best_a is not None:\n",
    "                    print(f\"    Лучше предыдущего действия {best_a} с Q={best_q:.3f}, обновляем.\")\n",
    "                best_q, best_a = q, a\n",
    "            elif verbose:\n",
    "                print(f\"    Хуже текущего лучшего действия {best_a} с Q={best_q:.3f}, пропускаем.\")\n",
    "        \n",
    "        # Определяем лучшее действие для этого состояния\n",
    "        new_pol[i] = best_a\n",
    "        \n",
    "        if verbose:\n",
    "            action_changed = new_pol[i] != policy[i]\n",
    "            print(f\"  Лучшее действие для состояния {i}: {best_a} ({action_names[best_a]}), Q={best_q:.3f}\")\n",
    "            print(f\"  Q-значения всех действий: {np.round(q_values, 3)}\")\n",
    "            \n",
    "            if action_changed:\n",
    "                print(f\"  >>> Политика ИЗМЕНЕНА для состояния {i}: {policy[i]} → {new_pol[i]}\")\n",
    "            else:\n",
    "                print(f\"  >>> Политика НЕ ИЗМЕНЕНА для состояния {i}: остается {policy[i]}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nИтог улучшения политики:\")\n",
    "        print(f\"  Было: {policy} ({[action_names[a] for a in policy]})\")\n",
    "        print(f\"  Стало: {new_pol} ({[action_names[a] for a in new_pol]})\")\n",
    "        if policy == new_pol:\n",
    "            print(\"  Политика НЕ ИЗМЕНИЛАСЬ - достигнута оптимальная политика.\")\n",
    "        else:\n",
    "            print(\"  Политика ИЗМЕНИЛАСЬ - требуется продолжить итерации.\")\n",
    "    \n",
    "    return new_pol\n",
    "\n",
    "def policy_iteration_discount():\n",
    "    # стартуем, например, всегда «3% скидка»\n",
    "    policy = [0] * num_states\n",
    "    it = 0\n",
    "    \n",
    "    print(f\"\\nКоэффициент дисконтирования γ = {γ}\")\n",
    "    print(\"\\nНачальная политика:\")\n",
    "    for i, a in enumerate(policy):\n",
    "        print(f\"  Состояние {i} ({state_names[i]}) → Действие {a} ({action_names[a]})\")\n",
    "    \n",
    "    while True:\n",
    "        print(f\"\\n{'*'*40 } Итерация {it+1} {'*'*40}\")\n",
    "        \n",
    "        print(f\"\\nТекущая политика:\")\n",
    "        for i, a in enumerate(policy):\n",
    "            print(f\"  {i} ({state_names[i]}) → {a} ({action_names[a]})\")\n",
    "        \n",
    "        # оценка\n",
    "        V = evaluate_policy_discount(policy)\n",
    "        print(f\"\\nРезультат оценки политики:\")\n",
    "        print(f\"  V = {np.round(V, 3)}\")\n",
    "        \n",
    "        # улучшение\n",
    "        new_pol = improve_policy_discount(policy, V)\n",
    "        \n",
    "        # Проверка на сходимость\n",
    "        if new_pol == policy:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"Политика не изменилась. Алгоритм завершён.\")\n",
    "            print(\"=\"*80)\n",
    "            break\n",
    "        \n",
    "        # Информация об изменении политики\n",
    "        print(\"\\nИзменения в политике:\")\n",
    "        for i in range(num_states):\n",
    "            if new_pol[i] != policy[i]:\n",
    "                print(f\"  Состояние {i} ({state_names[i]}): {policy[i]} ({action_names[policy[i]]}) → {new_pol[i]} ({action_names[new_pol[i]]})\")\n",
    "        \n",
    "        policy = new_pol\n",
    "        it += 1\n",
    "    \n",
    "    return policy, V\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt_pol, opt_V = policy_iteration_discount()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40 + \" Ответ \" + \"=\"*40)\n",
    "    \n",
    "    print(\"\\nОптимальная политика:\")\n",
    "    for i, a in enumerate(opt_pol):\n",
    "        print(f\"  Состояние {i} ({state_names[i]}) → Действие {a} ({action_names[a]})\")\n",
    "    \n",
    "    print(\"\\nОптимальная функция ценности:\")\n",
    "    for i, v in enumerate(opt_V):\n",
    "        print(f\"  V*[{i}] ({state_names[i]}) = {v:.6f}\")\n",
    "    \n",
    "    print(\"\\nСтоимость состояний V* в округлённом виде:\")\n",
    "    print(f\"  {np.round(opt_V, 3)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50345f",
   "metadata": {},
   "source": [
    "___ \n",
    "## 5 решение методами линейного программирования (Без дисконтирования)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ccf7a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ФОРМИРОВАНИЕ ВЕКТОРА ВОЗНАГРАЖДЕНИЙ\n",
      "------------------------------------------------------------\n",
      "\n",
      "Расчет ожидаемых моментальных вознаграждений r(s,a):\n",
      "\n",
      "Состояние 0 (Отличный):\n",
      "  Действие 0 (3% скидка):\n",
      "    Переход в 0 (Отличный): P=0.300, R=110, вклад = 33.000\n",
      "    Переход в 1 (Хороший): P=0.500, R=100, вклад = 50.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.200, R=70, вклад = 14.000\n",
      "  Итоговое r(0,0) = 97.000 (индекс в векторе reward_vector: 0)\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    Переход в 0 (Отличный): P=0.200, R=120, вклад = 24.000\n",
      "    Переход в 1 (Хороший): P=0.700, R=100, вклад = 70.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.100, R=70, вклад = 7.000\n",
      "  Итоговое r(0,1) = 101.000 (индекс в векторе reward_vector: 1)\n",
      "  Действие 2 (Ничего):\n",
      "    Переход в 0 (Отличный): P=0.300, R=110, вклад = 33.000\n",
      "    Переход в 1 (Хороший): P=0.400, R=80, вклад = 32.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.300, R=50, вклад = 15.000\n",
      "  Итоговое r(0,2) = 80.000 (индекс в векторе reward_vector: 2)\n",
      "\n",
      "Состояние 1 (Хороший):\n",
      "  Действие 0 (3% скидка):\n",
      "    Переход в 0 (Отличный): P=0.200, R=100, вклад = 20.000\n",
      "    Переход в 1 (Хороший): P=0.600, R=80, вклад = 48.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.200, R=50, вклад = 10.000\n",
      "  Итоговое r(1,0) = 78.000 (индекс в векторе reward_vector: 3)\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    Переход в 0 (Отличный): P=0.100, R=110, вклад = 11.000\n",
      "    Переход в 1 (Хороший): P=0.400, R=100, вклад = 40.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.500, R=90, вклад = 45.000\n",
      "  Итоговое r(1,1) = 96.000 (индекс в векторе reward_vector: 4)\n",
      "  Действие 2 (Ничего):\n",
      "    Переход в 0 (Отличный): P=0.200, R=100, вклад = 20.000\n",
      "    Переход в 1 (Хороший): P=0.600, R=60, вклад = 36.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.200, R=40, вклад = 8.000\n",
      "  Итоговое r(1,2) = 64.000 (индекс в векторе reward_vector: 5)\n",
      "\n",
      "Состояние 2 (Удовлетворительный):\n",
      "  Действие 0 (3% скидка):\n",
      "    Переход в 0 (Отличный): P=0.100, R=80, вклад = 8.000\n",
      "    Переход в 1 (Хороший): P=0.200, R=60, вклад = 12.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.700, R=40, вклад = 28.000\n",
      "  Итоговое r(2,0) = 48.000 (индекс в векторе reward_vector: 6)\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    Переход в 0 (Отличный): P=0.100, R=100, вклад = 10.000\n",
      "    Переход в 1 (Хороший): P=0.200, R=70, вклад = 14.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.700, R=60, вклад = 42.000\n",
      "  Итоговое r(2,1) = 66.000 (индекс в векторе reward_vector: 7)\n",
      "  Действие 2 (Ничего):\n",
      "    Переход в 0 (Отличный): P=0.100, R=80, вклад = 8.000\n",
      "    Переход в 1 (Хороший): P=0.300, R=70, вклад = 21.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.600, R=60, вклад = 36.000\n",
      "  Итоговое r(2,2) = 65.000 (индекс в векторе reward_vector: 8)\n",
      "\n",
      "Итоговый вектор вознаграждений reward_vector:\n",
      "  reward_vector[0] = r(0,0) = 97.000\n",
      "  reward_vector[1] = r(0,1) = 101.000\n",
      "  reward_vector[2] = r(0,2) = 80.000\n",
      "  reward_vector[3] = r(1,0) = 78.000\n",
      "  reward_vector[4] = r(1,1) = 96.000\n",
      "  reward_vector[5] = r(1,2) = 64.000\n",
      "  reward_vector[6] = r(2,0) = 48.000\n",
      "  reward_vector[7] = r(2,1) = 66.000\n",
      "  reward_vector[8] = r(2,2) = 65.000\n",
      "\n",
      "2. ФОРМИРОВАНИЕ СИСТЕМЫ ОГРАНИЧЕНИЙ\n",
      "------------------------------------------------------------\n",
      "\n",
      "Формирование условий балансировки потоков:\n",
      "\n",
      "Для состояния 0 (Отличный):\n",
      "  x(0,0) с коэффициентом 0.700\n",
      "    = 1.0 (потому что это текущее состояние) - 0.300 (P[0][0][0])\n",
      "  x(0,1) с коэффициентом 0.800\n",
      "    = 1.0 (потому что это текущее состояние) - 0.200 (P[1][0][0])\n",
      "  x(0,2) с коэффициентом 0.700\n",
      "    = 1.0 (потому что это текущее состояние) - 0.300 (P[2][0][0])\n",
      "  x(1,0) с коэффициентом -0.200\n",
      "    = 0.0 (не текущее состояние) - 0.200 (P[0][1][0])\n",
      "  x(1,1) с коэффициентом -0.100\n",
      "    = 0.0 (не текущее состояние) - 0.100 (P[1][1][0])\n",
      "  x(1,2) с коэффициентом -0.200\n",
      "    = 0.0 (не текущее состояние) - 0.200 (P[2][1][0])\n",
      "  x(2,0) с коэффициентом -0.100\n",
      "    = 0.0 (не текущее состояние) - 0.100 (P[0][2][0])\n",
      "  x(2,1) с коэффициентом -0.100\n",
      "    = 0.0 (не текущее состояние) - 0.100 (P[1][2][0])\n",
      "  x(2,2) с коэффициентом -0.100\n",
      "    = 0.0 (не текущее состояние) - 0.100 (P[2][2][0])\n",
      "  Полное уравнение баланса (в матричном виде): ∑ коэффициентов · x(s,a) = 0\n",
      "\n",
      "Для состояния 1 (Хороший):\n",
      "  x(0,0) с коэффициентом -0.500\n",
      "    = 0.0 (не текущее состояние) - 0.500 (P[0][0][1])\n",
      "  x(0,1) с коэффициентом -0.700\n",
      "    = 0.0 (не текущее состояние) - 0.700 (P[1][0][1])\n",
      "  x(0,2) с коэффициентом -0.400\n",
      "    = 0.0 (не текущее состояние) - 0.400 (P[2][0][1])\n",
      "  x(1,0) с коэффициентом 0.400\n",
      "    = 1.0 (потому что это текущее состояние) - 0.600 (P[0][1][1])\n",
      "  x(1,1) с коэффициентом 0.600\n",
      "    = 1.0 (потому что это текущее состояние) - 0.400 (P[1][1][1])\n",
      "  x(1,2) с коэффициентом 0.400\n",
      "    = 1.0 (потому что это текущее состояние) - 0.600 (P[2][1][1])\n",
      "  x(2,0) с коэффициентом -0.200\n",
      "    = 0.0 (не текущее состояние) - 0.200 (P[0][2][1])\n",
      "  x(2,1) с коэффициентом -0.200\n",
      "    = 0.0 (не текущее состояние) - 0.200 (P[1][2][1])\n",
      "  x(2,2) с коэффициентом -0.300\n",
      "    = 0.0 (не текущее состояние) - 0.300 (P[2][2][1])\n",
      "  Полное уравнение баланса (в матричном виде): ∑ коэффициентов · x(s,a) = 0\n",
      "\n",
      "Для состояния 2 (Удовлетворительный):\n",
      "  x(0,0) с коэффициентом -0.200\n",
      "    = 0.0 (не текущее состояние) - 0.200 (P[0][0][2])\n",
      "  x(0,1) с коэффициентом -0.100\n",
      "    = 0.0 (не текущее состояние) - 0.100 (P[1][0][2])\n",
      "  x(0,2) с коэффициентом -0.300\n",
      "    = 0.0 (не текущее состояние) - 0.300 (P[2][0][2])\n",
      "  x(1,0) с коэффициентом -0.200\n",
      "    = 0.0 (не текущее состояние) - 0.200 (P[0][1][2])\n",
      "  x(1,1) с коэффициентом -0.500\n",
      "    = 0.0 (не текущее состояние) - 0.500 (P[1][1][2])\n",
      "  x(1,2) с коэффициентом -0.200\n",
      "    = 0.0 (не текущее состояние) - 0.200 (P[2][1][2])\n",
      "  x(2,0) с коэффициентом 0.300\n",
      "    = 1.0 (потому что это текущее состояние) - 0.700 (P[0][2][2])\n",
      "  x(2,1) с коэффициентом 0.300\n",
      "    = 1.0 (потому что это текущее состояние) - 0.700 (P[1][2][2])\n",
      "  x(2,2) с коэффициентом 0.400\n",
      "    = 1.0 (потому что это текущее состояние) - 0.600 (P[2][2][2])\n",
      "  Полное уравнение баланса (в матричном виде): ∑ коэффициентов · x(s,a) = 0\n",
      "\n",
      "Условие нормировки:\n",
      "  Сумма всех x(s,a) = 1.0:\n",
      "    + x(0,0)\n",
      "    + x(0,1)\n",
      "    + x(0,2)\n",
      "    + x(1,0)\n",
      "    + x(1,1)\n",
      "    + x(1,2)\n",
      "    + x(2,0)\n",
      "    + x(2,1)\n",
      "    + x(2,2)\n",
      "\n",
      "Итоговая матрица коэффициентов A_eq:\n",
      "  Строка 0 (Уравнение баланса для состояния 0 (Отличный)):\n",
      "    A_eq[0,0] (для x(0,0)) = 0.7000\n",
      "    A_eq[0,1] (для x(0,1)) = 0.8000\n",
      "    A_eq[0,2] (для x(0,2)) = 0.7000\n",
      "    A_eq[0,3] (для x(1,0)) = -0.2000\n",
      "    A_eq[0,4] (для x(1,1)) = -0.1000\n",
      "    A_eq[0,5] (для x(1,2)) = -0.2000\n",
      "    A_eq[0,6] (для x(2,0)) = -0.1000\n",
      "    A_eq[0,7] (для x(2,1)) = -0.1000\n",
      "    A_eq[0,8] (для x(2,2)) = -0.1000\n",
      "  Строка 1 (Уравнение баланса для состояния 1 (Хороший)):\n",
      "    A_eq[1,0] (для x(0,0)) = -0.5000\n",
      "    A_eq[1,1] (для x(0,1)) = -0.7000\n",
      "    A_eq[1,2] (для x(0,2)) = -0.4000\n",
      "    A_eq[1,3] (для x(1,0)) = 0.4000\n",
      "    A_eq[1,4] (для x(1,1)) = 0.6000\n",
      "    A_eq[1,5] (для x(1,2)) = 0.4000\n",
      "    A_eq[1,6] (для x(2,0)) = -0.2000\n",
      "    A_eq[1,7] (для x(2,1)) = -0.2000\n",
      "    A_eq[1,8] (для x(2,2)) = -0.3000\n",
      "  Строка 2 (Уравнение баланса для состояния 2 (Удовлетворительный)):\n",
      "    A_eq[2,0] (для x(0,0)) = -0.2000\n",
      "    A_eq[2,1] (для x(0,1)) = -0.1000\n",
      "    A_eq[2,2] (для x(0,2)) = -0.3000\n",
      "    A_eq[2,3] (для x(1,0)) = -0.2000\n",
      "    A_eq[2,4] (для x(1,1)) = -0.5000\n",
      "    A_eq[2,5] (для x(1,2)) = -0.2000\n",
      "    A_eq[2,6] (для x(2,0)) = 0.3000\n",
      "    A_eq[2,7] (для x(2,1)) = 0.3000\n",
      "    A_eq[2,8] (для x(2,2)) = 0.4000\n",
      "  Строка 3 (Условие нормировки):\n",
      "    A_eq[3,0] (для x(0,0)) = 1.0000\n",
      "    A_eq[3,1] (для x(0,1)) = 1.0000\n",
      "    A_eq[3,2] (для x(0,2)) = 1.0000\n",
      "    A_eq[3,3] (для x(1,0)) = 1.0000\n",
      "    A_eq[3,4] (для x(1,1)) = 1.0000\n",
      "    A_eq[3,5] (для x(1,2)) = 1.0000\n",
      "    A_eq[3,6] (для x(2,0)) = 1.0000\n",
      "    A_eq[3,7] (для x(2,1)) = 1.0000\n",
      "    A_eq[3,8] (для x(2,2)) = 1.0000\n",
      "\n",
      "Итоговый вектор правых частей b_eq:\n",
      "  b_eq[0] (Уравнение баланса для состояния 0 (Отличный)) = 0.0\n",
      "  b_eq[1] (Уравнение баланса для состояния 1 (Хороший)) = 0.0\n",
      "  b_eq[2] (Уравнение баланса для состояния 2 (Удовлетворительный)) = 0.0\n",
      "  b_eq[3] (Условие нормировки) = 1.0\n",
      "\n",
      "Система уравнений в развернутом виде:\n",
      "  0.7000·y(0,0) +0.8000·y(0,1) +0.7000·y(0,2) -0.2000·y(1,0) -0.1000·y(1,1) -0.2000·y(1,2) -0.1000·y(2,0) -0.1000·y(2,1) -0.1000·y(2,2) = 0.0\n",
      "  -0.5000·y(0,0) -0.7000·y(0,1) -0.4000·y(0,2) +0.4000·y(1,0) +0.6000·y(1,1) +0.4000·y(1,2) -0.2000·y(2,0) -0.2000·y(2,1) -0.3000·y(2,2) = 0.0\n",
      "  -0.2000·y(0,0) -0.1000·y(0,1) -0.3000·y(0,2) -0.2000·y(1,0) -0.5000·y(1,1) -0.2000·y(1,2) +0.3000·y(2,0) +0.3000·y(2,1) +0.4000·y(2,2) = 0.0\n",
      "\n",
      "3. РЕШЕНИЕ ЗАДАЧИ ЛИНЕЙНОГО ПРОГРАММИРОВАНИЯ\n",
      "------------------------------------------------------------\n",
      "\n",
      "Задача: максимизировать r^T·x при ограничениях A_eq·x = b_eq, x ≥ 0\n",
      "  где x - вектор переменных x(s,a), r - вектор моментальных вознаграждений\n",
      "  При этом для решения задачи максимизации через linprog (который минимизирует),\n",
      "  мы минимизируем -r^T·x\n",
      "\n",
      "Вектор коэффициентов целевой функции (-r):\n",
      "  c[0] для x(0,0) = -97.0000\n",
      "  c[1] для x(0,1) = -101.0000\n",
      "  c[2] для x(0,2) = -80.0000\n",
      "  c[3] для x(1,0) = -78.0000\n",
      "  c[4] для x(1,1) = -96.0000\n",
      "  c[5] для x(1,2) = -64.0000\n",
      "  c[6] для x(2,0) = -48.0000\n",
      "  c[7] для x(2,1) = -66.0000\n",
      "  c[8] для x(2,2) = -65.0000\n",
      "\n",
      "Задача успешно решена за 3 итераций.\n",
      "Статус: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n",
      "\n",
      "Оптимальные значения переменных x(s,a):\n",
      "  x(0,0) = 0.000000\n",
      "  x(0,1) = 0.111111\n",
      "  x(0,2) = 0.000000\n",
      "  x(1,0) = 0.000000\n",
      "  x(1,1) = 0.382716\n",
      "  x(1,2) = 0.000000\n",
      "  x(2,0) = 0.000000\n",
      "  x(2,1) = 0.000000\n",
      "  x(2,2) = 0.506173\n",
      "\n",
      "Оптимальное значение целевой функции (средний доход): 80.864198\n",
      "\n",
      "4. ВОССТАНОВЛЕНИЕ ОПТИМАЛЬНОЙ ПОЛИТИКИ\n",
      "------------------------------------------------------------\n",
      "\n",
      "Для каждого состояния выбираем действие с наибольшим значением x(s,a):\n",
      "\n",
      "Состояние 0 (Отличный):\n",
      "  Значения x(s,a) для разных действий:\n",
      "    x(0,0) = 0.000000\n",
      "    x(0,1) = 0.111111\n",
      "    x(0,2) = 0.000000\n",
      "  Максимальное значение: x(0,1) = 0.111111\n",
      "  Выбранное оптимальное действие: 1 (Бесплатная доставка)\n",
      "\n",
      "Состояние 1 (Хороший):\n",
      "  Значения x(s,a) для разных действий:\n",
      "    x(1,0) = 0.000000\n",
      "    x(1,1) = 0.382716\n",
      "    x(1,2) = 0.000000\n",
      "  Максимальное значение: x(1,1) = 0.382716\n",
      "  Выбранное оптимальное действие: 1 (Бесплатная доставка)\n",
      "\n",
      "Состояние 2 (Удовлетворительный):\n",
      "  Значения x(s,a) для разных действий:\n",
      "    x(2,0) = 0.000000\n",
      "    x(2,1) = 0.000000\n",
      "    x(2,2) = 0.506173\n",
      "  Максимальное значение: x(2,2) = 0.506173\n",
      "  Выбранное оптимальное действие: 2 (Ничего)\n",
      "\n",
      "5. ИТОГОВЫЕ РЕЗУЛЬТАТЫ\n",
      "------------------------------------------------------------\n",
      "\n",
      "Оптимальное среднее вознаграждение g* = 80.8642\n",
      "\n",
      "Оптимальная стационарная детерминированная политика:\n",
      "  Состояние 0 (Отличный) → Действие 1 (Бесплатная доставка)\n",
      "  Состояние 1 (Хороший) → Действие 1 (Бесплатная доставка)\n",
      "  Состояние 2 (Удовлетворительный) → Действие 2 (Ничего)\n",
      "\n",
      "Стационарное распределение по состояниям:\n",
      "  Состояние 0 (Отличный): 0.111111\n",
      "  Состояние 1 (Хороший): 0.382716\n",
      "  Состояние 2 (Удовлетворительный): 0.506173\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "\n",
    "# 1. Определяем матрицы вероятностей P и вознаграждений R по условию задачи\n",
    "\n",
    "# P[a][s][s'] - вероятность перехода из состояния s в s' при действии a\n",
    "P = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "# R[a][s][s'] - вознаграждение за переход из состояния s в s' при действии a\n",
    "R = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "# Состояния и действия\n",
    "state_names = [\"Отличный\", \"Хороший\", \"Удовлетворительный\"]\n",
    "action_names = [\"3% скидка\", \"Бесплатная доставка\", \"Ничего\"]\n",
    "\n",
    "# Параметры задачи\n",
    "num_states = 3  # Количество состояний\n",
    "num_actions = 3  # Количество действий\n",
    "num_variables = num_states * num_actions  # Общее количество переменных (9)\n",
    "\n",
    "\n",
    "# 2. Формируем вектор вознаграждений r для каждого состояния и действия\n",
    "print(\"\\n1. ФОРМИРОВАНИЕ ВЕКТОРА ВОЗНАГРАЖДЕНИЙ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "reward_vector = np.zeros(num_variables)\n",
    "print(\"\\nРасчет ожидаемых моментальных вознаграждений r(s,a):\")\n",
    "\n",
    "for state in range(num_states):\n",
    "    print(f\"\\nСостояние {state} ({state_names[state]}):\")\n",
    "    \n",
    "    for action in range(num_actions):\n",
    "        idx = state * num_actions + action\n",
    "        # Вычисляем детальное ожидаемое вознаграждение\n",
    "        reward = 0\n",
    "        print(f\"  Действие {action} ({action_names[action]}):\")\n",
    "        \n",
    "        for next_state in range(num_states):\n",
    "            contribution = P[action][state][next_state] * R[action][state][next_state]\n",
    "            reward += contribution\n",
    "            print(f\"    Переход в {next_state} ({state_names[next_state]}): \" + \n",
    "                  f\"P={P[action][state][next_state]:.3f}, R={R[action][state][next_state]}, \" + \n",
    "                  f\"вклад = {contribution:.3f}\")\n",
    "        \n",
    "        reward_vector[idx] = reward\n",
    "        print(f\"  Итоговое r({state},{action}) = {reward:.3f} (индекс в векторе reward_vector: {idx})\")\n",
    "\n",
    "print(\"\\nИтоговый вектор вознаграждений reward_vector:\")\n",
    "for i in range(num_variables):\n",
    "    state = i // num_actions\n",
    "    action = i % num_actions\n",
    "    print(f\"  reward_vector[{i}] = r({state},{action}) = {reward_vector[i]:.3f}\")\n",
    "\n",
    "# 3. Формируем матрицу A_eq и вектор b_eq для решения задачи линейного программирования\n",
    "print(\"\\n2. ФОРМИРОВАНИЕ СИСТЕМЫ ОГРАНИЧЕНИЙ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# A_eq и b_eq для условия балансировки потоков и нормировки\n",
    "A_eq = np.zeros((num_states + 1, num_variables))\n",
    "b_eq = np.zeros(num_states + 1)\n",
    "\n",
    "print(\"\\nФормирование условий балансировки потоков:\")\n",
    "# а) Баланс потоков: для каждого состояния j, учитываем все действия a\n",
    "for next_state in range(num_states):\n",
    "    print(f\"\\nДля состояния {next_state} ({state_names[next_state]}):\")\n",
    "    equation_terms = []\n",
    "    \n",
    "    for state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            idx = state * num_actions + action\n",
    "            coef = 0\n",
    "            \n",
    "            # Если это состояние j, добавляем +1*x(j,a)\n",
    "            if state == next_state:\n",
    "                A_eq[next_state, idx] += 1.0\n",
    "                coef += 1.0\n",
    "                equation_terms.append(f\"+1.0·x({state},{action})\")\n",
    "            \n",
    "            # Вычитаем вероятность перехода в j из любого состояния\n",
    "            transition_prob = P[action][state][next_state]\n",
    "            A_eq[next_state, idx] -= transition_prob\n",
    "            coef -= transition_prob\n",
    "            \n",
    "            if abs(coef) > 1e-10:  # Показываем только значимые коэффициенты\n",
    "                print(f\"  x({state},{action}) с коэффициентом {coef:.3f}\")\n",
    "                if state == next_state:\n",
    "                    print(f\"    = 1.0 (потому что это текущее состояние) - {transition_prob:.3f} (P[{action}][{state}][{next_state}])\")\n",
    "                else:\n",
    "                    print(f\"    = 0.0 (не текущее состояние) - {transition_prob:.3f} (P[{action}][{state}][{next_state}])\")\n",
    "\n",
    "    print(f\"  Полное уравнение баланса (в матричном виде): ∑ коэффициентов · x(s,a) = 0\")\n",
    "\n",
    "print(\"\\nУсловие нормировки:\")\n",
    "# б) Нормировка: сумма всех переменных x_{s,a} равна 1\n",
    "A_eq[num_states, :] = 1.0\n",
    "b_eq[num_states] = 1.0\n",
    "\n",
    "print(\"  Сумма всех x(s,a) = 1.0:\")\n",
    "for state in range(num_states):\n",
    "    for action in range(num_actions):\n",
    "        print(f\"    + x({state},{action})\")\n",
    "\n",
    "print(\"\\nИтоговая матрица коэффициентов A_eq:\")\n",
    "for i in range(A_eq.shape[0]):\n",
    "    if i < num_states:\n",
    "        row_desc = f\"Уравнение баланса для состояния {i} ({state_names[i]})\"\n",
    "    else:\n",
    "        row_desc = \"Условие нормировки\"\n",
    "    print(f\"  Строка {i} ({row_desc}):\")\n",
    "    \n",
    "    # Выводим значения по частям для лучшей читаемости\n",
    "    for j in range(num_variables):\n",
    "        state_j = j // num_actions\n",
    "        action_j = j % num_actions\n",
    "        if abs(A_eq[i, j]) > 1e-10:  # Только значимые коэффициенты\n",
    "            print(f\"    A_eq[{i},{j}] (для x({state_j},{action_j})) = {A_eq[i, j]:.4f}\")\n",
    "\n",
    "print(\"\\nИтоговый вектор правых частей b_eq:\")\n",
    "for i in range(len(b_eq)):\n",
    "    if i < num_states:\n",
    "        row_desc = f\"Уравнение баланса для состояния {i} ({state_names[i]})\"\n",
    "    else:\n",
    "        row_desc = \"Условие нормировки\"\n",
    "    print(f\"  b_eq[{i}] ({row_desc}) = {b_eq[i]}\")\n",
    "\n",
    "print(\"\\nСистема уравнений в развернутом виде:\")\n",
    "for i in range(num_states):\n",
    "    equation_parts = []\n",
    "    for j in range(num_states * num_actions):\n",
    "        if abs(A_eq[i, j]) > 1e-10:\n",
    "            state_j = j // num_actions\n",
    "            action_j = j % num_actions\n",
    "            sign = \"+\" if A_eq[i, j] > 0 and equation_parts else \"\"\n",
    "            equation_parts.append(f\"{sign}{A_eq[i, j]:.4f}·y({state_j},{action_j})\")\n",
    "    \n",
    "    equation = \" \".join(equation_parts)\n",
    "    print(f\"  {equation} = {b_eq[i]}\")\n",
    "\n",
    "\n",
    "# 4. Решаем задачу линейного программирования для максимизации r^T * x\n",
    "print(\"\\n3. РЕШЕНИЕ ЗАДАЧИ ЛИНЕЙНОГО ПРОГРАММИРОВАНИЯ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"\\nЗадача: максимизировать r^T·x при ограничениях A_eq·x = b_eq, x ≥ 0\")\n",
    "print(\"  где x - вектор переменных x(s,a), r - вектор моментальных вознаграждений\")\n",
    "print(\"  При этом для решения задачи максимизации через linprog (который минимизирует),\")\n",
    "print(\"  мы минимизируем -r^T·x\")\n",
    "\n",
    "print(\"\\nВектор коэффициентов целевой функции (-r):\")\n",
    "for i in range(num_variables):\n",
    "    state = i // num_actions\n",
    "    action = i % num_actions\n",
    "    print(f\"  c[{i}] для x({state},{action}) = {-reward_vector[i]:.4f}\")\n",
    "\n",
    "# Задача сводится к минимизации -r^T * x\n",
    "result = linprog(\n",
    "    c=-reward_vector,  # Минмизируем -r^T * x\n",
    "    A_eq=A_eq,\n",
    "    b_eq=b_eq,\n",
    "    bounds=[(0, None)] * num_variables,\n",
    "    method='highs'  # Используем метод 'highs', возможен также 'revised simplex'\n",
    ")\n",
    "\n",
    "# Проверка успешности решения задачи\n",
    "if not result.success:\n",
    "    print(f\"\\nОшибка в решении LP задачи: {result.message}\")\n",
    "else:\n",
    "    print(f\"\\nЗадача успешно решена за {result.nit} итераций.\")\n",
    "    print(f\"Статус: {result.message}\")\n",
    "\n",
    "# Оптимальное решение\n",
    "optimal_x = result.x\n",
    "optimal_reward = reward_vector.dot(optimal_x)\n",
    "\n",
    "print(\"\\nОптимальные значения переменных x(s,a):\")\n",
    "for i in range(num_variables):\n",
    "    state = i // num_actions\n",
    "    action = i % num_actions\n",
    "    print(f\"  x({state},{action}) = {optimal_x[i]:.6f}\")\n",
    "\n",
    "print(f\"\\nОптимальное значение целевой функции (средний доход): {optimal_reward:.6f}\")\n",
    "\n",
    "# 5. Восстанавливаем политику для каждого состояния\n",
    "print(\"\\n4. ВОССТАНОВЛЕНИЕ ОПТИМАЛЬНОЙ ПОЛИТИКИ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"\\nДля каждого состояния выбираем действие с наибольшим значением x(s,a):\")\n",
    "optimal_policy = []\n",
    "\n",
    "for state in range(num_states):\n",
    "    print(f\"\\nСостояние {state} ({state_names[state]}):\")\n",
    "    values = [optimal_x[state * num_actions + action] for action in range(num_actions)]\n",
    "    \n",
    "    print(\"  Значения x(s,a) для разных действий:\")\n",
    "    for action in range(num_actions):\n",
    "        print(f\"    x({state},{action}) = {values[action]:.6f}\")\n",
    "    \n",
    "    best_action = int(np.argmax(values))  # Действие с максимальной вероятностью\n",
    "    optimal_policy.append(best_action)\n",
    "    \n",
    "    print(f\"  Максимальное значение: x({state},{best_action}) = {values[best_action]:.6f}\")\n",
    "    print(f\"  Выбранное оптимальное действие: {best_action} ({action_names[best_action]})\")\n",
    "\n",
    "# 6. Выводим итоговые результаты\n",
    "print(\"\\n5. ИТОГОВЫЕ РЕЗУЛЬТАТЫ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(f\"\\nОптимальное среднее вознаграждение g* = {optimal_reward:.4f}\")\n",
    "print(\"\\nОптимальная стационарная детерминированная политика:\")\n",
    "for state in range(num_states):\n",
    "    print(f\"  Состояние {state} ({state_names[state]}) → Действие {optimal_policy[state]} ({action_names[optimal_policy[state]]})\")\n",
    "\n",
    "print(\"\\nСтационарное распределение по состояниям:\")\n",
    "for state in range(num_states):\n",
    "    state_sum = sum(optimal_x[state * num_actions + action] for action in range(num_actions))\n",
    "    print(f\"  Состояние {state} ({state_names[state]}): {state_sum:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b26f56",
   "metadata": {},
   "source": [
    "___\n",
    "## 5 решение методами линейного программирования (С дисконтированием)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660f2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ФОРМИРОВАНИЕ ВЕКТОРА ВОЗНАГРАЖДЕНИЙ\n",
      "------------------------------------------------------------\n",
      "\n",
      "Расчет ожидаемых моментальных вознаграждений r(s,a):\n",
      "\n",
      "Состояние 0 (Отличный):\n",
      "  Действие 0 (3% скидка):\n",
      "    Переход в 0 (Отличный): P=0.300, R=110, вклад = 33.000\n",
      "    Переход в 1 (Хороший): P=0.500, R=100, вклад = 50.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.200, R=70, вклад = 14.000\n",
      "  Итоговое r(0,0) = 97.000 (индекс в векторе reward_vector: 0)\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    Переход в 0 (Отличный): P=0.200, R=120, вклад = 24.000\n",
      "    Переход в 1 (Хороший): P=0.700, R=100, вклад = 70.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.100, R=70, вклад = 7.000\n",
      "  Итоговое r(0,1) = 101.000 (индекс в векторе reward_vector: 1)\n",
      "  Действие 2 (Ничего):\n",
      "    Переход в 0 (Отличный): P=0.300, R=110, вклад = 33.000\n",
      "    Переход в 1 (Хороший): P=0.400, R=80, вклад = 32.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.300, R=50, вклад = 15.000\n",
      "  Итоговое r(0,2) = 80.000 (индекс в векторе reward_vector: 2)\n",
      "\n",
      "Состояние 1 (Хороший):\n",
      "  Действие 0 (3% скидка):\n",
      "    Переход в 0 (Отличный): P=0.200, R=100, вклад = 20.000\n",
      "    Переход в 1 (Хороший): P=0.600, R=80, вклад = 48.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.200, R=50, вклад = 10.000\n",
      "  Итоговое r(1,0) = 78.000 (индекс в векторе reward_vector: 3)\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    Переход в 0 (Отличный): P=0.100, R=110, вклад = 11.000\n",
      "    Переход в 1 (Хороший): P=0.400, R=100, вклад = 40.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.500, R=90, вклад = 45.000\n",
      "  Итоговое r(1,1) = 96.000 (индекс в векторе reward_vector: 4)\n",
      "  Действие 2 (Ничего):\n",
      "    Переход в 0 (Отличный): P=0.200, R=100, вклад = 20.000\n",
      "    Переход в 1 (Хороший): P=0.600, R=60, вклад = 36.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.200, R=40, вклад = 8.000\n",
      "  Итоговое r(1,2) = 64.000 (индекс в векторе reward_vector: 5)\n",
      "\n",
      "Состояние 2 (Удовлетворительный):\n",
      "  Действие 0 (3% скидка):\n",
      "    Переход в 0 (Отличный): P=0.100, R=80, вклад = 8.000\n",
      "    Переход в 1 (Хороший): P=0.200, R=60, вклад = 12.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.700, R=40, вклад = 28.000\n",
      "  Итоговое r(2,0) = 48.000 (индекс в векторе reward_vector: 6)\n",
      "  Действие 1 (Бесплатная доставка):\n",
      "    Переход в 0 (Отличный): P=0.100, R=100, вклад = 10.000\n",
      "    Переход в 1 (Хороший): P=0.200, R=70, вклад = 14.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.700, R=60, вклад = 42.000\n",
      "  Итоговое r(2,1) = 66.000 (индекс в векторе reward_vector: 7)\n",
      "  Действие 2 (Ничего):\n",
      "    Переход в 0 (Отличный): P=0.100, R=80, вклад = 8.000\n",
      "    Переход в 1 (Хороший): P=0.300, R=70, вклад = 21.000\n",
      "    Переход в 2 (Удовлетворительный): P=0.600, R=60, вклад = 36.000\n",
      "  Итоговое r(2,2) = 65.000 (индекс в векторе reward_vector: 8)\n",
      "\n",
      "Итоговый вектор вознаграждений reward_vector:\n",
      "  reward_vector[0] = r(0,0) = 97.000\n",
      "  reward_vector[1] = r(0,1) = 101.000\n",
      "  reward_vector[2] = r(0,2) = 80.000\n",
      "  reward_vector[3] = r(1,0) = 78.000\n",
      "  reward_vector[4] = r(1,1) = 96.000\n",
      "  reward_vector[5] = r(1,2) = 64.000\n",
      "  reward_vector[6] = r(2,0) = 48.000\n",
      "  reward_vector[7] = r(2,1) = 66.000\n",
      "  reward_vector[8] = r(2,2) = 65.000\n",
      "\n",
      "2. ФОРМИРОВАНИЕ СИСТЕМЫ УРАВНЕНИЙ ДЛЯ ДИСКОНТИРОВАННОГО СЛУЧАЯ\n",
      "------------------------------------------------------------\n",
      "\n",
      "Формирование системы уравнений для дисконтированного MDP:\n",
      "  Для каждого состояния j: ∑_a y(j,a) - γ·∑_s,a P[a][s][j]·y(s,a) = d₀[j]\n",
      "  где γ = 0.7, d₀ = [1. 0. 0.]\n",
      "\n",
      "Для состояния 0 (Отличный):\n",
      "  d₀[0] = 1.0\n",
      "  Для y(0,0): +1.0 (т.к. это текущее состояние 0)\n",
      "  Для y(0,0): -0.7·0.300 = -0.210 (переход в состояние 0)\n",
      "  Итоговый коэффициент для y(0,0): 0.7900\n",
      "  Для y(0,1): +1.0 (т.к. это текущее состояние 0)\n",
      "  Для y(0,1): -0.7·0.200 = -0.140 (переход в состояние 0)\n",
      "  Итоговый коэффициент для y(0,1): 0.8600\n",
      "  Для y(0,2): +1.0 (т.к. это текущее состояние 0)\n",
      "  Для y(0,2): -0.7·0.300 = -0.210 (переход в состояние 0)\n",
      "  Итоговый коэффициент для y(0,2): 0.7900\n",
      "  Для y(1,0): -0.7·0.200 = -0.140 (переход в состояние 0)\n",
      "  Итоговый коэффициент для y(1,0): -0.1400\n",
      "  Для y(1,1): -0.7·0.100 = -0.070 (переход в состояние 0)\n",
      "  Итоговый коэффициент для y(1,1): -0.0700\n",
      "  Для y(1,2): -0.7·0.200 = -0.140 (переход в состояние 0)\n",
      "  Итоговый коэффициент для y(1,2): -0.1400\n",
      "  Для y(2,0): -0.7·0.100 = -0.070 (переход в состояние 0)\n",
      "  Итоговый коэффициент для y(2,0): -0.0700\n",
      "  Для y(2,1): -0.7·0.100 = -0.070 (переход в состояние 0)\n",
      "  Итоговый коэффициент для y(2,1): -0.0700\n",
      "  Для y(2,2): -0.7·0.100 = -0.070 (переход в состояние 0)\n",
      "  Итоговый коэффициент для y(2,2): -0.0700\n",
      "\n",
      "Для состояния 1 (Хороший):\n",
      "  d₀[1] = 0.0\n",
      "  Для y(0,0): -0.7·0.500 = -0.350 (переход в состояние 1)\n",
      "  Итоговый коэффициент для y(0,0): -0.3500\n",
      "  Для y(0,1): -0.7·0.700 = -0.490 (переход в состояние 1)\n",
      "  Итоговый коэффициент для y(0,1): -0.4900\n",
      "  Для y(0,2): -0.7·0.400 = -0.280 (переход в состояние 1)\n",
      "  Итоговый коэффициент для y(0,2): -0.2800\n",
      "  Для y(1,0): +1.0 (т.к. это текущее состояние 1)\n",
      "  Для y(1,0): -0.7·0.600 = -0.420 (переход в состояние 1)\n",
      "  Итоговый коэффициент для y(1,0): 0.5800\n",
      "  Для y(1,1): +1.0 (т.к. это текущее состояние 1)\n",
      "  Для y(1,1): -0.7·0.400 = -0.280 (переход в состояние 1)\n",
      "  Итоговый коэффициент для y(1,1): 0.7200\n",
      "  Для y(1,2): +1.0 (т.к. это текущее состояние 1)\n",
      "  Для y(1,2): -0.7·0.600 = -0.420 (переход в состояние 1)\n",
      "  Итоговый коэффициент для y(1,2): 0.5800\n",
      "  Для y(2,0): -0.7·0.200 = -0.140 (переход в состояние 1)\n",
      "  Итоговый коэффициент для y(2,0): -0.1400\n",
      "  Для y(2,1): -0.7·0.200 = -0.140 (переход в состояние 1)\n",
      "  Итоговый коэффициент для y(2,1): -0.1400\n",
      "  Для y(2,2): -0.7·0.300 = -0.210 (переход в состояние 1)\n",
      "  Итоговый коэффициент для y(2,2): -0.2100\n",
      "\n",
      "Для состояния 2 (Удовлетворительный):\n",
      "  d₀[2] = 0.0\n",
      "  Для y(0,0): -0.7·0.200 = -0.140 (переход в состояние 2)\n",
      "  Итоговый коэффициент для y(0,0): -0.1400\n",
      "  Для y(0,1): -0.7·0.100 = -0.070 (переход в состояние 2)\n",
      "  Итоговый коэффициент для y(0,1): -0.0700\n",
      "  Для y(0,2): -0.7·0.300 = -0.210 (переход в состояние 2)\n",
      "  Итоговый коэффициент для y(0,2): -0.2100\n",
      "  Для y(1,0): -0.7·0.200 = -0.140 (переход в состояние 2)\n",
      "  Итоговый коэффициент для y(1,0): -0.1400\n",
      "  Для y(1,1): -0.7·0.500 = -0.350 (переход в состояние 2)\n",
      "  Итоговый коэффициент для y(1,1): -0.3500\n",
      "  Для y(1,2): -0.7·0.200 = -0.140 (переход в состояние 2)\n",
      "  Итоговый коэффициент для y(1,2): -0.1400\n",
      "  Для y(2,0): +1.0 (т.к. это текущее состояние 2)\n",
      "  Для y(2,0): -0.7·0.700 = -0.490 (переход в состояние 2)\n",
      "  Итоговый коэффициент для y(2,0): 0.5100\n",
      "  Для y(2,1): +1.0 (т.к. это текущее состояние 2)\n",
      "  Для y(2,1): -0.7·0.700 = -0.490 (переход в состояние 2)\n",
      "  Итоговый коэффициент для y(2,1): 0.5100\n",
      "  Для y(2,2): +1.0 (т.к. это текущее состояние 2)\n",
      "  Для y(2,2): -0.7·0.600 = -0.420 (переход в состояние 2)\n",
      "  Итоговый коэффициент для y(2,2): 0.5800\n",
      "\n",
      "Итоговая матрица коэффициентов A_eq:\n",
      "  Строка 0 (уравнение для состояния 0 - Отличный):\n",
      "    Для переменных состояния 0 (Отличный): +0.7900 +0.8600 +0.7900 \n",
      "    Для переменных состояния 1 (Хороший): -0.1400 -0.0700 -0.1400 \n",
      "    Для переменных состояния 2 (Удовлетворительный): -0.0700 -0.0700 -0.0700 \n",
      "  Строка 1 (уравнение для состояния 1 - Хороший):\n",
      "    Для переменных состояния 0 (Отличный): -0.3500 -0.4900 -0.2800 \n",
      "    Для переменных состояния 1 (Хороший): +0.5800 +0.7200 +0.5800 \n",
      "    Для переменных состояния 2 (Удовлетворительный): -0.1400 -0.1400 -0.2100 \n",
      "  Строка 2 (уравнение для состояния 2 - Удовлетворительный):\n",
      "    Для переменных состояния 0 (Отличный): -0.1400 -0.0700 -0.2100 \n",
      "    Для переменных состояния 1 (Хороший): -0.1400 -0.3500 -0.1400 \n",
      "    Для переменных состояния 2 (Удовлетворительный): +0.5100 +0.5100 +0.5800 \n",
      "\n",
      "Итоговый вектор правых частей b_eq:\n",
      "  b_eq[0] (для состояния 0 - Отличный) = 1.0\n",
      "  b_eq[1] (для состояния 1 - Хороший) = 0.0\n",
      "  b_eq[2] (для состояния 2 - Удовлетворительный) = 0.0\n",
      "\n",
      "Система уравнений в развернутом виде:\n",
      "  0.7900·y(0,0) +0.8600·y(0,1) +0.7900·y(0,2) -0.1400·y(1,0) -0.0700·y(1,1) -0.1400·y(1,2) -0.0700·y(2,0) -0.0700·y(2,1) -0.0700·y(2,2) = 1.0\n",
      "  -0.3500·y(0,0) -0.4900·y(0,1) -0.2800·y(0,2) +0.5800·y(1,0) +0.7200·y(1,1) +0.5800·y(1,2) -0.1400·y(2,0) -0.1400·y(2,1) -0.2100·y(2,2) = 0.0\n",
      "  -0.1400·y(0,0) -0.0700·y(0,1) -0.2100·y(0,2) -0.1400·y(1,0) -0.3500·y(1,1) -0.1400·y(1,2) +0.5100·y(2,0) +0.5100·y(2,1) +0.5800·y(2,2) = 0.0\n",
      "\n",
      "3. РЕШЕНИЕ ЗАДАЧИ ЛИНЕЙНОГО ПРОГРАММИРОВАНИЯ\n",
      "------------------------------------------------------------\n",
      "\n",
      "Задача: максимизировать r^T·y при ограничениях A_eq·y = b_eq, y ≥ 0\n",
      "  где y - вектор переменных y(s,a), r - вектор моментальных вознаграждений\n",
      "  При решении через linprog (который минимизирует) используем -r^T·y\n",
      "\n",
      "Вектор коэффициентов целевой функции (-r):\n",
      "  c[0] для y(0,0) = -97.0000\n",
      "  c[1] для y(0,1) = -101.0000\n",
      "  c[2] для y(0,2) = -80.0000\n",
      "  c[3] для y(1,0) = -78.0000\n",
      "  c[4] для y(1,1) = -96.0000\n",
      "  c[5] для y(1,2) = -64.0000\n",
      "  c[6] для y(2,0) = -48.0000\n",
      "  c[7] для y(2,1) = -66.0000\n",
      "  c[8] для y(2,2) = -65.0000\n",
      "\n",
      "Задача успешно решена за 4 итераций.\n",
      "Статус: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n",
      "\n",
      "Оптимальные значения переменных y(s,a):\n",
      "  y(0,0) = 0.000000\n",
      "  y(0,1) = 1.326165\n",
      "  y(0,2) = 0.000000\n",
      "  y(1,0) = 0.000000\n",
      "  y(1,1) = 1.151964\n",
      "  y(1,2) = 0.000000\n",
      "  y(2,0) = 0.000000\n",
      "  y(2,1) = 0.000000\n",
      "  y(2,2) = 0.855205\n",
      "\n",
      "Оптимальное значение целевой функции (дисконтированный доход): 300.119474\n",
      "\n",
      "4. ВОССТАНОВЛЕНИЕ ОПТИМАЛЬНОЙ ПОЛИТИКИ\n",
      "------------------------------------------------------------\n",
      "\n",
      "Для каждого состояния выбираем действие с наибольшим значением y(s,a):\n",
      "\n",
      "Состояние 0 (Отличный):\n",
      "  Значения y(s,a) для разных действий:\n",
      "    y(0,0) = 0.000000\n",
      "    y(0,1) = 1.326165\n",
      "    y(0,2) = 0.000000\n",
      "  Максимальное значение: y(0,1) = 1.326165\n",
      "  Выбранное оптимальное действие: 1 (Бесплатная доставка)\n",
      "\n",
      "Состояние 1 (Хороший):\n",
      "  Значения y(s,a) для разных действий:\n",
      "    y(1,0) = 0.000000\n",
      "    y(1,1) = 1.151964\n",
      "    y(1,2) = 0.000000\n",
      "  Максимальное значение: y(1,1) = 1.151964\n",
      "  Выбранное оптимальное действие: 1 (Бесплатная доставка)\n",
      "\n",
      "Состояние 2 (Удовлетворительный):\n",
      "  Значения y(s,a) для разных действий:\n",
      "    y(2,0) = 0.000000\n",
      "    y(2,1) = 0.000000\n",
      "    y(2,2) = 0.855205\n",
      "  Максимальное значение: y(2,2) = 0.855205\n",
      "  Выбранное оптимальное действие: 2 (Ничего)\n",
      "\n",
      "5. ПРОВЕРКА И ИНТЕРПРЕТАЦИЯ РЕЗУЛЬТАТОВ\n",
      "------------------------------------------------------------\n",
      "\n",
      "Оптимальный дисконтированный доход V* = 300.1195\n",
      "\n",
      "Оптимальная детерминированная политика:\n",
      "  Состояние 0 (Отличный) → Действие 1 (Бесплатная доставка)\n",
      "  Состояние 1 (Хороший) → Действие 1 (Бесплатная доставка)\n",
      "  Состояние 2 (Удовлетворительный) → Действие 2 (Ничего)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "\n",
    "# P[a][s][s'] - вероятность перехода из состояния s в состояние s' при действии a\n",
    "transition_probabilities = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "# R[a][s][s'] - вознаграждение за переход из состояния s в s' при действии a\n",
    "rewards = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "# Параметры задачи\n",
    "num_states = 3  # Количество состояний\n",
    "num_actions = 3  # Количество действий\n",
    "discount_factor = 0.7  # Дисконт-фактор\n",
    "state_names = [\"Отличный\", \"Хороший\", \"Удовлетворительный\"]\n",
    "action_names = [\"3% скидка\", \"Бесплатная доставка\", \"Ничего\"]\n",
    "\n",
    "# Начальное распределение: стартуем из состояния \"Отличный\"\n",
    "initial_distribution = np.array([1.0, 0.0, 0.0])\n",
    "\n",
    "\n",
    "# 1. Формируем вектор вознаграждений r размером num_states * num_actions\n",
    "print(\"\\n1. ФОРМИРОВАНИЕ ВЕКТОРА ВОЗНАГРАЖДЕНИЙ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "reward_vector = np.zeros(num_states * num_actions)\n",
    "print(\"\\nРасчет ожидаемых моментальных вознаграждений r(s,a):\")\n",
    "\n",
    "for state in range(num_states):\n",
    "    print(f\"\\nСостояние {state} ({state_names[state]}):\")\n",
    "    \n",
    "    for action in range(num_actions):\n",
    "        idx = state * num_actions + action\n",
    "        # Вычисляем детальное ожидаемое вознаграждение\n",
    "        reward = 0\n",
    "        print(f\"  Действие {action} ({action_names[action]}):\")\n",
    "        \n",
    "        for next_state in range(num_states):\n",
    "            contribution = transition_probabilities[action][state][next_state] * rewards[action][state][next_state]\n",
    "            reward += contribution\n",
    "            print(f\"    Переход в {next_state} ({state_names[next_state]}): \" + \n",
    "                  f\"P={transition_probabilities[action][state][next_state]:.3f}, R={rewards[action][state][next_state]}, \" + \n",
    "                  f\"вклад = {contribution:.3f}\")\n",
    "        \n",
    "        reward_vector[idx] = reward\n",
    "        print(f\"  Итоговое r({state},{action}) = {reward:.3f} (индекс в векторе reward_vector: {idx})\")\n",
    "\n",
    "print(\"\\nИтоговый вектор вознаграждений reward_vector:\")\n",
    "for i in range(num_states * num_actions):\n",
    "    state = i // num_actions\n",
    "    action = i % num_actions\n",
    "    print(f\"  reward_vector[{i}] = r({state},{action}) = {reward_vector[i]:.3f}\")\n",
    "\n",
    "# 3. Формируем матрицу равенств A_eq и вектор b_eq для линейного программирования\n",
    "print(\"\\n2. ФОРМИРОВАНИЕ СИСТЕМЫ УРАВНЕНИЙ ДЛЯ ДИСКОНТИРОВАННОГО СЛУЧАЯ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# A_eq - матрица коэффициентов, b_eq - вектор правых частей\n",
    "A_eq = np.zeros((num_states, num_states * num_actions))\n",
    "b_eq = initial_distribution.copy()\n",
    "\n",
    "print(\"\\nФормирование системы уравнений для дисконтированного MDP:\")\n",
    "print(f\"  Для каждого состояния j: ∑_a y(j,a) - γ·∑_s,a P[a][s][j]·y(s,a) = d₀[j]\")\n",
    "print(f\"  где γ = {discount_factor}, d₀ = {initial_distribution}\")\n",
    "\n",
    "# Формируем систему уравнений для каждого состояния\n",
    "for next_state in range(num_states):\n",
    "    print(f\"\\nДля состояния {next_state} ({state_names[next_state]}):\")\n",
    "    print(f\"  d₀[{next_state}] = {initial_distribution[next_state]}\")\n",
    "    \n",
    "    for state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            idx = state * num_actions + action\n",
    "            \n",
    "            # Коэффициент при y(s,a) в уравнении для состояния next_state\n",
    "            coef = 0\n",
    "            \n",
    "            # Добавляем +1 для y(j,a) в левой части, если state == next_state\n",
    "            if state == next_state:\n",
    "                A_eq[next_state, idx] += 1.0\n",
    "                coef += 1.0\n",
    "                print(f\"  Для y({state},{action}): +1.0 (т.к. это текущее состояние {next_state})\")\n",
    "            \n",
    "            # Вычитаем γ·P[a][s][j] для всех y(s,a)\n",
    "            transition_prob = transition_probabilities[action][state][next_state]\n",
    "            discounted_prob = discount_factor * transition_prob\n",
    "            A_eq[next_state, idx] -= discounted_prob\n",
    "            coef -= discounted_prob\n",
    "            \n",
    "            if abs(discounted_prob) > 1e-10:\n",
    "                print(f\"  Для y({state},{action}): -{discount_factor:.1f}·{transition_prob:.3f} = -{discounted_prob:.3f} \" +\n",
    "                      f\"(переход в состояние {next_state})\")\n",
    "            \n",
    "            if abs(coef) > 1e-10:\n",
    "                print(f\"  Итоговый коэффициент для y({state},{action}): {coef:.4f}\")\n",
    "\n",
    "print(\"\\nИтоговая матрица коэффициентов A_eq:\")\n",
    "for i in range(A_eq.shape[0]):\n",
    "    print(f\"  Строка {i} (уравнение для состояния {i} - {state_names[i]}):\")\n",
    "    \n",
    "    # Выводим значения по группам для лучшей читаемости\n",
    "    for state in range(num_states):\n",
    "        print(f\"    Для переменных состояния {state} ({state_names[state]}):\", end=\" \")\n",
    "        for action in range(num_actions):\n",
    "            idx = state * num_actions + action\n",
    "            print(f\"{A_eq[i, idx]:+.4f}\", end=\" \")\n",
    "        print()\n",
    "\n",
    "print(\"\\nИтоговый вектор правых частей b_eq:\")\n",
    "for i in range(len(b_eq)):\n",
    "    print(f\"  b_eq[{i}] (для состояния {i} - {state_names[i]}) = {b_eq[i]}\")\n",
    "\n",
    "print(\"\\nСистема уравнений в развернутом виде:\")\n",
    "for i in range(num_states):\n",
    "    equation_parts = []\n",
    "    for j in range(num_states * num_actions):\n",
    "        if abs(A_eq[i, j]) > 1e-10:\n",
    "            state_j = j // num_actions\n",
    "            action_j = j % num_actions\n",
    "            sign = \"+\" if A_eq[i, j] > 0 and equation_parts else \"\"\n",
    "            equation_parts.append(f\"{sign}{A_eq[i, j]:.4f}·y({state_j},{action_j})\")\n",
    "    \n",
    "    equation = \" \".join(equation_parts)\n",
    "    print(f\"  {equation} = {b_eq[i]}\")\n",
    "\n",
    "# 4. Решаем задачу линейного программирования (LP): maximize r^T * y <=> minimize -r^T * y\n",
    "print(\"\\n3. РЕШЕНИЕ ЗАДАЧИ ЛИНЕЙНОГО ПРОГРАММИРОВАНИЯ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"\\nЗадача: максимизировать r^T·y при ограничениях A_eq·y = b_eq, y ≥ 0\")\n",
    "print(\"  где y - вектор переменных y(s,a), r - вектор моментальных вознаграждений\")\n",
    "print(\"  При решении через linprog (который минимизирует) используем -r^T·y\")\n",
    "\n",
    "print(\"\\nВектор коэффициентов целевой функции (-r):\")\n",
    "for i in range(num_states * num_actions):\n",
    "    state = i // num_actions\n",
    "    action = i % num_actions\n",
    "    print(f\"  c[{i}] для y({state},{action}) = {-reward_vector[i]:.4f}\")\n",
    "\n",
    "# Задача сводится к минимизации -r^T * y\n",
    "result = linprog(\n",
    "    c=-reward_vector,  # Минимизируем -r^T * y\n",
    "    A_eq=A_eq,\n",
    "    b_eq=b_eq,\n",
    "    bounds=[(0, None)] * (num_states * num_actions),\n",
    "    method='highs'  # Используем метод 'highs', можно также использовать 'revised simplex'\n",
    ")\n",
    "\n",
    "# Проверка успешности решения задачи\n",
    "if not result.success:\n",
    "    print(f\"\\nОшибка в решении LP задачи: {result.message}\")\n",
    "else:\n",
    "    print(f\"\\nЗадача успешно решена за {result.nit} итераций.\")\n",
    "    print(f\"Статус: {result.message}\")\n",
    "\n",
    "# Оптимальные значения переменных y\n",
    "optimal_y = result.x\n",
    "optimal_value = reward_vector.dot(optimal_y)  # Оптимальный дисконтированный доход\n",
    "\n",
    "print(\"\\nОптимальные значения переменных y(s,a):\")\n",
    "for i in range(num_states * num_actions):\n",
    "    state = i // num_actions\n",
    "    action = i % num_actions\n",
    "    print(f\"  y({state},{action}) = {optimal_y[i]:.6f}\")\n",
    "\n",
    "print(f\"\\nОптимальное значение целевой функции (дисконтированный доход): {optimal_value:.6f}\")\n",
    "\n",
    "# 5. Восстанавливаем стратегию (политику)\n",
    "print(\"\\n4. ВОССТАНОВЛЕНИЕ ОПТИМАЛЬНОЙ ПОЛИТИКИ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"\\nДля каждого состояния выбираем действие с наибольшим значением y(s,a):\")\n",
    "optimal_policy = []\n",
    "\n",
    "for state in range(num_states):\n",
    "    print(f\"\\nСостояние {state} ({state_names[state]}):\")\n",
    "    action_values = [optimal_y[state * num_actions + action] for action in range(num_actions)]\n",
    "    \n",
    "    print(\"  Значения y(s,a) для разных действий:\")\n",
    "    for action in range(num_actions):\n",
    "        print(f\"    y({state},{action}) = {action_values[action]:.6f}\")\n",
    "    \n",
    "    optimal_action = int(np.argmax(action_values))  # Действие с максимальной переменной\n",
    "    optimal_policy.append(optimal_action)\n",
    "    \n",
    "    print(f\"  Максимальное значение: y({state},{optimal_action}) = {action_values[optimal_action]:.6f}\")\n",
    "    print(f\"  Выбранное оптимальное действие: {optimal_action} ({action_names[optimal_action]})\")\n",
    "\n",
    "# 6. Проверка и интерпретация результатов\n",
    "print(\"\\n5. ИТОГОВЫЕ РЕЗУЛЬТАТЫ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(f\"\\nОптимальный дисконтированный доход V* = {optimal_value:.4f}\")\n",
    "print(\"\\nОптимальная детерминированная политика:\")\n",
    "for state in range(num_states):\n",
    "    print(f\"  Состояние {state} ({state_names[state]}) → Действие {optimal_policy[state]} ({action_names[optimal_policy[state]]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf9b85",
   "metadata": {},
   "source": [
    "___\n",
    "## 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02855219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Итоговый вектор правых частей b_eq:\n",
      "  b_eq[0] (Уравнение баланса для состояния 0 (Отличный)) = 0.0\n",
      "  b_eq[1] (Уравнение баланса для состояния 1 (Хороший)) = 0.0\n",
      "  b_eq[2] (Уравнение баланса для состояния 2 (Удовлетворительный)) = 0.0\n",
      "  b_eq[3] (Условие нормировки) = 1.0\n",
      "\n",
      "Система уравнений в развернутом виде:\n",
      "  0.7000·y(0,0) +0.8000·y(0,1) +0.7000·y(0,2) -0.2000·y(1,0) -0.1000·y(1,1) -0.2000·y(1,2) -0.1000·y(2,0) -0.1000·y(2,1) -0.1000·y(2,2) = 0.0\n",
      "  -0.5000·y(0,0) -0.7000·y(0,1) -0.4000·y(0,2) +0.4000·y(1,0) +0.6000·y(1,1) +0.4000·y(1,2) -0.2000·y(2,0) -0.2000·y(2,1) -0.3000·y(2,2) = 0.0\n",
      "  -0.2000·y(0,0) -0.1000·y(0,1) -0.3000·y(0,2) -0.2000·y(1,0) -0.5000·y(1,1) -0.2000·y(1,2) +0.3000·y(2,0) +0.3000·y(2,1) +0.4000·y(2,2) = 0.0\n",
      "\n",
      "3. РЕШЕНИЕ ЗАДАЧИ ЛИНЕЙНОГО ПРОГРАММИРОВАНИЯ\n",
      "------------------------------------------------------------\n",
      "\n",
      "Задача: максимизировать r^T·x при ограничениях A_eq·x = b_eq, x ≥ 0\n",
      "  где x - вектор переменных x(s,a), r - вектор моментальных вознаграждений\n",
      "  При этом для решения задачи максимизации через linprog (который минимизирует),\n",
      "  мы минимизируем -r^T·x\n",
      "\n",
      "Вектор коэффициентов целевой функции (-r):\n",
      "  c[0] для x(0,0) = -97.0000\n",
      "  c[1] для x(0,1) = -101.0000\n",
      "  c[2] для x(0,2) = -80.0000\n",
      "  c[3] для x(1,0) = -78.0000\n",
      "  c[4] для x(1,1) = -96.0000\n",
      "  c[5] для x(1,2) = -64.0000\n",
      "  c[6] для x(2,0) = -48.0000\n",
      "  c[7] для x(2,1) = -66.0000\n",
      "  c[8] для x(2,2) = -65.0000\n",
      "  x(0,0) = 0.000000\n",
      "  x(0,1) = 0.111111\n",
      "  x(0,2) = 0.000000\n",
      "  x(1,0) = 0.000000\n",
      "  x(1,1) = 0.382716\n",
      "  x(1,2) = 0.000000\n",
      "  x(2,0) = 0.000000\n",
      "  x(2,1) = 0.000000\n",
      "  x(2,2) = 0.506173\n",
      "\n",
      "5. ИТОГОВЫЕ РЕЗУЛЬТАТЫ\n",
      "------------------------------------------------------------\n",
      "\n",
      "Оптимальное среднее вознаграждение g* = 80.8642\n",
      "\n",
      "Оптимальная стационарная детерминированная политика:\n",
      "  Состояние 0 (Отличный) → Действие 1 (Бесплатная доставка)\n",
      "  Состояние 1 (Хороший) → Действие 1 (Бесплатная доставка)\n",
      "  Состояние 2 (Удовлетворительный) → Действие 2 (Ничего)\n",
      "\n",
      "Стационарное распределение по состояниям:\n",
      "  Состояние 0 (Отличный): 0.111111\n",
      "  Состояние 1 (Хороший): 0.382716\n",
      "  Состояние 2 (Удовлетворительный): 0.506173\n",
      "\n",
      "Информация о базисных и небазисных переменных:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Базисные переменные (положительные значения):\n",
      "  x(0,1) = 0.111111 (Отличный, Бесплатная доставка)\n",
      "  x(1,1) = 0.382716 (Хороший, Бесплатная доставка)\n",
      "  x(2,2) = 0.506173 (Удовлетворительный, Ничего)\n",
      "\n",
      "Небазисные переменные (нулевые значения):\n",
      "  x(0,0) = 0.000000e+00 (Отличный, 3% скидка)\n",
      "  x(0,2) = 0.000000e+00 (Отличный, Ничего)\n",
      "  x(1,0) = 0.000000e+00 (Хороший, 3% скидка)\n",
      "  x(1,2) = 0.000000e+00 (Хороший, Ничего)\n",
      "  x(2,0) = 0.000000e+00 (Удовлетворительный, 3% скидка)\n",
      "  x(2,1) = 0.000000e+00 (Удовлетворительный, Бесплатная доставка)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "\n",
    "# 1. Определяем матрицы вероятностей P и вознаграждений R по условию задачи\n",
    "\n",
    "# P[a][s][s'] - вероятность перехода из состояния s в s' при действии a\n",
    "P = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "# R[a][s][s'] - вознаграждение за переход из состояния s в s' при действии a\n",
    "R = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "# Состояния и действия\n",
    "state_names = [\"Отличный\", \"Хороший\", \"Удовлетворительный\"]\n",
    "action_names = [\"3% скидка\", \"Бесплатная доставка\", \"Ничего\"]\n",
    "\n",
    "# Параметры задачи\n",
    "num_states = 3  # Количество состояний\n",
    "num_actions = 3  # Количество действий\n",
    "num_variables = num_states * num_actions  # Общее количество переменных (9)\n",
    "\n",
    "\n",
    "# 2. Формируем вектор вознаграждений r для каждого состояния и действия\n",
    "#print(\"\\n1. ФОРМИРОВАНИЕ ВЕКТОРА ВОЗНАГРАЖДЕНИЙ\")\n",
    "#print(\"-\"*60)\n",
    "\n",
    "reward_vector = np.zeros(num_variables)\n",
    "#print(\"\\nРасчет ожидаемых моментальных вознаграждений r(s,a):\")\n",
    "\n",
    "for state in range(num_states):\n",
    "    #print(f\"\\nСостояние {state} ({state_names[state]}):\")\n",
    "    \n",
    "    for action in range(num_actions):\n",
    "        idx = state * num_actions + action\n",
    "        # Вычисляем детальное ожидаемое вознаграждение\n",
    "        reward = 0\n",
    "        #print(f\"  Действие {action} ({action_names[action]}):\")\n",
    "        \n",
    "        for next_state in range(num_states):\n",
    "            contribution = P[action][state][next_state] * R[action][state][next_state]\n",
    "            reward += contribution\n",
    "            #print(f\"    Переход в {next_state} ({state_names[next_state]}): \" + \n",
    "            #      f\"P={P[action][state][next_state]:.3f}, R={R[action][state][next_state]}, \" + \n",
    "            #     f\"вклад = {contribution:.3f}\")\n",
    "        \n",
    "        reward_vector[idx] = reward\n",
    "        #print(f\"  Итоговое r({state},{action}) = {reward:.3f} (индекс в векторе reward_vector: {idx})\")\n",
    "\n",
    "#print(\"\\nИтоговый вектор вознаграждений reward_vector:\")\n",
    "for i in range(num_variables):\n",
    "    state = i // num_actions\n",
    "    action = i % num_actions\n",
    "    #print(f\"  reward_vector[{i}] = r({state},{action}) = {reward_vector[i]:.3f}\")\n",
    "\n",
    "# 3. Формируем матрицу A_eq и вектор b_eq для решения задачи линейного программирования\n",
    "#print(\"\\n2. ФОРМИРОВАНИЕ СИСТЕМЫ ОГРАНИЧЕНИЙ\")\n",
    "#print(\"-\"*60)\n",
    "\n",
    "# A_eq и b_eq для условия балансировки потоков и нормировки\n",
    "A_eq = np.zeros((num_states + 1, num_variables))\n",
    "b_eq = np.zeros(num_states + 1)\n",
    "\n",
    "#print(\"\\nФормирование условий балансировки потоков:\")\n",
    "# а) Баланс потоков: для каждого состояния j, учитываем все действия a\n",
    "for next_state in range(num_states):\n",
    "    #print(f\"\\nДля состояния {next_state} ({state_names[next_state]}):\")\n",
    "    equation_terms = []\n",
    "    \n",
    "    for state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            idx = state * num_actions + action\n",
    "            coef = 0\n",
    "            \n",
    "            # Если это состояние j, добавляем +1*x(j,a)\n",
    "            if state == next_state:\n",
    "                A_eq[next_state, idx] += 1.0\n",
    "                coef += 1.0\n",
    "                equation_terms.append(f\"+1.0·x({state},{action})\")\n",
    "            \n",
    "            # Вычитаем вероятность перехода в j из любого состояния\n",
    "            transition_prob = P[action][state][next_state]\n",
    "            A_eq[next_state, idx] -= transition_prob\n",
    "            coef -= transition_prob\n",
    "            \n",
    "            if abs(coef) > 1e-10:  # Показываем только значимые коэффициенты\n",
    "                #print(f\"  x({state},{action}) с коэффициентом {coef:.3f}\")\n",
    "                if state == next_state:\n",
    "                    #print(f\"    = 1.0 (потому что это текущее состояние) - {transition_prob:.3f} (P[{action}][{state}][{next_state}])\")\n",
    "                    pass\n",
    "                else:\n",
    "                    pass\n",
    "                    #print(f\"    = 0.0 (не текущее состояние) - {transition_prob:.3f} (P[{action}][{state}][{next_state}])\")\n",
    "\n",
    "    #print(f\"  Полное уравнение баланса (в матричном виде): ∑ коэффициентов · x(s,a) = 0\")\n",
    "\n",
    "#print(\"\\nУсловие нормировки:\")\n",
    "# б) Нормировка: сумма всех переменных x_{s,a} равна 1\n",
    "A_eq[num_states, :] = 1.0\n",
    "b_eq[num_states] = 1.0\n",
    "\n",
    "#print(\"  Сумма всех x(s,a) = 1.0:\")\n",
    "for state in range(num_states):\n",
    "    for action in range(num_actions):\n",
    "        #print(f\"    + x({state},{action})\")\n",
    "        pass\n",
    "#print(\"\\nИтоговая матрица коэффициентов A_eq:\")\n",
    "for i in range(A_eq.shape[0]):\n",
    "    if i < num_states:\n",
    "        row_desc = f\"Уравнение баланса для состояния {i} ({state_names[i]})\"\n",
    "    else:\n",
    "        row_desc = \"Условие нормировки\"\n",
    "    #print(f\"  Строка {i} ({row_desc}):\")\n",
    "    \n",
    "    # Выводим значения по частям для лучшей читаемости\n",
    "    for j in range(num_variables):\n",
    "        state_j = j // num_actions\n",
    "        action_j = j % num_actions\n",
    "        #if abs(A_eq[i, j]) > 1e-10:  # Только значимые коэффициенты\n",
    "            #print(f\"    A_eq[{i},{j}] (для x({state_j},{action_j})) = {A_eq[i, j]:.4f}\")\n",
    "\n",
    "print(\"\\nИтоговый вектор правых частей b_eq:\")\n",
    "for i in range(len(b_eq)):\n",
    "    if i < num_states:\n",
    "        row_desc = f\"Уравнение баланса для состояния {i} ({state_names[i]})\"\n",
    "    else:\n",
    "        row_desc = \"Условие нормировки\"\n",
    "    print(f\"  b_eq[{i}] ({row_desc}) = {b_eq[i]}\")\n",
    "\n",
    "print(\"\\nСистема уравнений в развернутом виде:\")\n",
    "for i in range(num_states):\n",
    "    equation_parts = []\n",
    "    for j in range(num_states * num_actions):\n",
    "        if abs(A_eq[i, j]) > 1e-10:\n",
    "            state_j = j // num_actions\n",
    "            action_j = j % num_actions\n",
    "            sign = \"+\" if A_eq[i, j] > 0 and equation_parts else \"\"\n",
    "            equation_parts.append(f\"{sign}{A_eq[i, j]:.4f}·y({state_j},{action_j})\")\n",
    "    \n",
    "    equation = \" \".join(equation_parts)\n",
    "    print(f\"  {equation} = {b_eq[i]}\")\n",
    "\n",
    "\n",
    "# 4. Решаем задачу линейного программирования для максимизации r^T * x\n",
    "print(\"\\n3. РЕШЕНИЕ ЗАДАЧИ ЛИНЕЙНОГО ПРОГРАММИРОВАНИЯ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(\"\\nЗадача: максимизировать r^T·x при ограничениях A_eq·x = b_eq, x ≥ 0\")\n",
    "print(\"  где x - вектор переменных x(s,a), r - вектор моментальных вознаграждений\")\n",
    "print(\"  При этом для решения задачи максимизации через linprog (который минимизирует),\")\n",
    "print(\"  мы минимизируем -r^T·x\")\n",
    "\n",
    "print(\"\\nВектор коэффициентов целевой функции (-r):\")\n",
    "for i in range(num_variables):\n",
    "    state = i // num_actions\n",
    "    action = i % num_actions\n",
    "    print(f\"  c[{i}] для x({state},{action}) = {-reward_vector[i]:.4f}\")\n",
    "\n",
    "# Задача сводится к минимизации -r^T * x\n",
    "result = linprog(\n",
    "    c=-reward_vector,  # Минмизируем -r^T * x\n",
    "    A_eq=A_eq,\n",
    "    b_eq=b_eq,\n",
    "    bounds=[(0, None)] * num_variables,\n",
    "    method='highs'  # Используем метод 'highs', возможен также 'revised simplex'\n",
    ")\n",
    "\n",
    "# Проверка успешности решения задачи\n",
    "if not result.success:\n",
    "    print(f\"\\nОшибка в решении LP задачи: {result.message}\")\n",
    "else:\n",
    "    #print(f\"\\nЗадача успешно решена за {result.nit} итераций.\")\n",
    "    #print(f\"Статус: {result.message}\")\n",
    "    pass\n",
    "\n",
    "# Оптимальное решение\n",
    "optimal_x = result.x\n",
    "optimal_reward = reward_vector.dot(optimal_x)\n",
    "\n",
    "#print(\"\\nОптимальные значения переменных x(s,a):\")\n",
    "for i in range(num_variables):\n",
    "    state = i // num_actions\n",
    "    action = i % num_actions\n",
    "    print(f\"  x({state},{action}) = {optimal_x[i]:.6f}\")\n",
    "\n",
    "#print(f\"\\nОптимальное значение целевой функции (средний доход): {optimal_reward:.6f}\")\n",
    "\n",
    "# 5. Восстанавливаем политику для каждого состояния\n",
    "#print(\"\\n4. ВОССТАНОВЛЕНИЕ ОПТИМАЛЬНОЙ ПОЛИТИКИ\")\n",
    "#print(\"-\"*60)\n",
    "\n",
    "#print(\"\\nДля каждого состояния выбираем действие с наибольшим значением x(s,a):\")\n",
    "optimal_policy = []\n",
    "\n",
    "for state in range(num_states):\n",
    "    #print(f\"\\nСостояние {state} ({state_names[state]}):\")\n",
    "    values = [optimal_x[state * num_actions + action] for action in range(num_actions)]\n",
    "    \n",
    "    #print(\"  Значения x(s,a) для разных действий:\")\n",
    "    for action in range(num_actions):\n",
    "        #print(f\"    x({state},{action}) = {values[action]:.6f}\")\n",
    "        pass\n",
    "    best_action = int(np.argmax(values))  # Действие с максимальной вероятностью\n",
    "    optimal_policy.append(best_action)\n",
    "    \n",
    "    #print(f\"  Максимальное значение: x({state},{best_action}) = {values[best_action]:.6f}\")\n",
    "    #print(f\"  Выбранное оптимальное действие: {best_action} ({action_names[best_action]})\")\n",
    "\n",
    "# 6. Выводим итоговые результаты\n",
    "# 6. Выводим итоговые результаты\n",
    "print(\"\\n5. ИТОГОВЫЕ РЕЗУЛЬТАТЫ\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "print(f\"\\nОптимальное среднее вознаграждение g* = {optimal_reward:.4f}\")\n",
    "print(\"\\nОптимальная стационарная детерминированная политика:\")\n",
    "for state in range(num_states):\n",
    "    print(f\"  Состояние {state} ({state_names[state]}) → Действие {optimal_policy[state]} ({action_names[optimal_policy[state]]})\")\n",
    "\n",
    "print(\"\\nСтационарное распределение по состояниям:\")\n",
    "for state in range(num_states):\n",
    "    state_sum = sum(optimal_x[state * num_actions + action] for action in range(num_actions))\n",
    "    print(f\"  Состояние {state} ({state_names[state]}): {state_sum:.6f}\")\n",
    "\n",
    "# Дополнительная информация о базисных и небазисных переменных\n",
    "print(\"\\nИнформация о базисных и небазисных переменных:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Определяем порог для идентификации базисных переменных\n",
    "threshold = 1e-10\n",
    "\n",
    "# Находим базисные и небазисные переменные\n",
    "basic_vars = []\n",
    "nonbasic_vars = []\n",
    "\n",
    "for i in range(num_variables):\n",
    "    state = i // num_actions\n",
    "    action = i % num_actions\n",
    "    value = optimal_x[i]\n",
    "    \n",
    "    if abs(value) > threshold:\n",
    "        basic_vars.append((i, state, action, value))\n",
    "    else:\n",
    "        nonbasic_vars.append((i, state, action, value))\n",
    "\n",
    "# Выводим базисные переменные\n",
    "print(\"\\nБазисные переменные (положительные значения):\")\n",
    "for idx, state, action, value in basic_vars:\n",
    "    print(f\"  x({state},{action}) = {value:.6f} ({state_names[state]}, {action_names[action]})\")\n",
    "\n",
    "# Выводим небазисные переменные\n",
    "print(\"\\nНебазисные переменные (нулевые значения):\")\n",
    "for idx, state, action, value in nonbasic_vars:\n",
    "    print(f\"  x({state},{action}) = {value:.6e} ({state_names[state]}, {action_names[action]})\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
