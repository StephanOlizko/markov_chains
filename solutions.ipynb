{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96ec2a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начальное состояние «Отличный»:\n",
      "  Лучшая стратегия действий: (1, 1, 1)\n",
      "  Ожидаемое вознаграждение: 278.40\n",
      "\n",
      "Начальное состояние «Хороший»:\n",
      "  Лучшая стратегия действий: (1, 1, 1)\n",
      "  Ожидаемое вознаграждение: 257.25\n",
      "\n",
      "Начальное состояние «Удовлетворительный»:\n",
      "  Лучшая стратегия действий: (2, 1, 1)\n",
      "  Ожидаемое вознаграждение: 222.65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# Матрицы доходов: R[action][state_from][state_to]\n",
    "P = {\n",
    "    0: [[0.3,0.5,0.2],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.2,0.7]],\n",
    "    1: [[0.2,0.7,0.1],\n",
    "        [0.1,0.4,0.5],\n",
    "        [0.1,0.2,0.7]],\n",
    "    2: [[0.3,0.4,0.3],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.3,0.6]],\n",
    "}\n",
    "\n",
    "R = {\n",
    "    0: [[110,100,70],\n",
    "        [100, 80,50],\n",
    "        [ 80, 60,40]],\n",
    "    1: [[120,100,70],\n",
    "        [110,100,90],\n",
    "        [100, 70,60]],\n",
    "    2: [[110, 80,50],\n",
    "        [100, 60,40],\n",
    "        [ 80, 70,60]],\n",
    "}\n",
    "\n",
    "actions = [0,1,2]      # индексы действий\n",
    "states  = [0,1,2]      # 0=«Отличный»,1=«Хороший»,2=«Удовлетворительный»\n",
    "months = 3\n",
    "state_names = [\"Отличный\",\"Хороший\",\"Удовлетворительный\"]\n",
    "\n",
    "def expected_reward(strategy, init_state):\n",
    "    # d[i] — вероятность быть в состоянии i\n",
    "    d = [0.0,0.0,0.0]\n",
    "    d[init_state] = 1.0\n",
    "    total = 0.0\n",
    "    for a in strategy:\n",
    "        d_new = [0.0,0.0,0.0]\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                pij = P[a][i][j]\n",
    "                rij = R[a][i][j]\n",
    "                total += d[i] * pij * rij\n",
    "                d_new[j] += d[i] * pij\n",
    "        d = d_new\n",
    "    return total\n",
    "\n",
    "for init in range(3):\n",
    "    best = (None, -1e9)\n",
    "    for strat in itertools.product(actions, repeat=months):\n",
    "        er = expected_reward(strat, init_state=init)\n",
    "        if er > best[1]:\n",
    "            best = (strat, er)\n",
    "    print(f\"Начальное состояние «{state_names[init]}»:\")\n",
    "    print(f\"  Лучшая стратегия действий: {best[0]}\")\n",
    "    print(f\"  Ожидаемое вознаграждение: {best[1]:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e245d2b6",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "905e4aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшая стационарная стратегия (0, 1, 2) -> (1, 1, 2)\n",
      "Стационарное распределение μ: [0.111 0.383 0.506]\n",
      "Среднее вознаграждение g: 80.86\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# Словари P и R по условию\n",
    "P = {\n",
    "    0: [[0.3,0.5,0.2],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.2,0.7]],\n",
    "    1: [[0.2,0.7,0.1],\n",
    "        [0.1,0.4,0.5],\n",
    "        [0.1,0.2,0.7]],\n",
    "    2: [[0.3,0.4,0.3],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.3,0.6]],\n",
    "}\n",
    "\n",
    "R = {\n",
    "    0: [[110,100,70],\n",
    "        [100, 80,50],\n",
    "        [ 80, 60,40]],\n",
    "    1: [[120,100,70],\n",
    "        [110,100,90],\n",
    "        [100, 70,60]],\n",
    "    2: [[110, 80,50],\n",
    "        [100, 60,40],\n",
    "        [ 80, 70,60]],\n",
    "}\n",
    "\n",
    "actions = [0,1,2]      # индексы действий\n",
    "states  = [0,1,2]      # 0=«Отличный»,1=«Хороший»,2=«Удовлетворительный»\n",
    "\n",
    "def evaluate_policy(policy):\n",
    "    # policy — кортеж длины 3: policy[s] = действие в состоянии s\n",
    "    # 1) Собираем P_pi и r_pi\n",
    "    P_pi = np.zeros((3,3))\n",
    "    r_pi = np.zeros(3)\n",
    "    for s in states:\n",
    "        a = policy[s]\n",
    "        P_pi[s,:] = P[a][s]\n",
    "        # среднее вознаграждение при s и действии a\n",
    "        for s2 in states:\n",
    "            r_pi[s] += P[a][s][s2] * R[a][s][s2]\n",
    "\n",
    "    # 2) Ищем стационарное распределение μ: μ = μ P_pi, sum(μ)=1\n",
    "    #    Находим собственный вектор P_pi^T с собственным числом 1\n",
    "    vals, vecs = np.linalg.eig(P_pi.T)\n",
    "    idx = np.argmin(np.abs(vals - 1.0))\n",
    "    mu = np.real(vecs[:, idx])\n",
    "    mu = mu / mu.sum()\n",
    "\n",
    "    # 3) Среднее вознаграждение по стратегии\n",
    "    g = float(np.dot(mu, r_pi))\n",
    "    return g, mu\n",
    "\n",
    "best_policy = None\n",
    "best_gain   = -1e9\n",
    "best_mu     = None\n",
    "\n",
    "for policy in itertools.product(actions, repeat=3):\n",
    "    g, mu = evaluate_policy(policy)\n",
    "    if g > best_gain:\n",
    "        best_gain   = g\n",
    "        best_policy = policy\n",
    "        best_mu     = mu\n",
    "\n",
    "print(\"Лучшая стационарная стратегия (0, 1, 2) ->\", best_policy)\n",
    "print(\"Стационарное распределение μ:\", np.round(best_mu,3))\n",
    "print(\"Среднее вознаграждение g:\", round(best_gain,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaadd10",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "69cfd249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Итерация 0. Текущая политика:\n",
      "  0 -> 0\n",
      "  1 -> 0\n",
      "  2 -> 0\n",
      "  Оценка: средний доход g = 69.3778\n",
      "\n",
      "Итерация 1. Текущая политика:\n",
      "  0 -> 1\n",
      "  1 -> 0\n",
      "  2 -> 2\n",
      "  Оценка: средний доход g = 77.9322\n",
      "\n",
      "Итерация 2. Текущая политика:\n",
      "  0 -> 1\n",
      "  1 -> 1\n",
      "  2 -> 2\n",
      "  Оценка: средний доход g = 80.8642\n",
      "\n",
      "Политика не изменилась. Алгоритм завершён.\n",
      "\n",
      "=== Итоговая оптимальная политика ===\n",
      "0 -> 1\n",
      "1 -> 1\n",
      "2 -> 2\n",
      "Оптимальный средний доход g* = 80.8642\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Задаём P и R по условию задачи\n",
    "P = {\n",
    "    0: [[0.3,0.5,0.2],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.2,0.7]],\n",
    "    1: [[0.2,0.7,0.1],\n",
    "        [0.1,0.4,0.5],\n",
    "        [0.1,0.2,0.7]],\n",
    "    2: [[0.3,0.4,0.3],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.3,0.6]],\n",
    "}\n",
    "\n",
    "R = {\n",
    "    0: [[110,100,70],\n",
    "        [100, 80,50],\n",
    "        [ 80, 60,40]],\n",
    "    1: [[120,100,70],\n",
    "        [110,100,90],\n",
    "        [100, 70,60]],\n",
    "    2: [[110, 80,50],\n",
    "        [100, 60,40],\n",
    "        [ 80, 70,60]],\n",
    "}\n",
    "\n",
    "num_states    = 3\n",
    "actions       = [0,1,2]    # 0=скидка,1=доставка,2=ничего\n",
    "state_names   = [\"Отличный\",\"Хороший\",\"Удовлетворительный\"]\n",
    "action_names  = [\"3% скидка\",\"Бесплатная доставка\",\"Ничего\"]\n",
    "\n",
    "def evaluate_policy(policy):\n",
    "    \"\"\"Policy evaluation (gain g и bias h)\"\"\"\n",
    "    m = num_states\n",
    "    # 1) Собираем P_pi и r_pi\n",
    "    P_pi = np.zeros((m,m))\n",
    "    r_pi = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        a = policy[i]\n",
    "        for j in range(m):\n",
    "            P_pi[i,j] = P[a][i][j]\n",
    "            r_pi[i] += P[a][i][j] * R[a][i][j]\n",
    "\n",
    "    # 2) Строим и решаем систему (m+1)x(m+1) на [h0..h_{m-1}, g]\n",
    "    A = np.zeros((m+1, m+1))\n",
    "    b = np.zeros(m+1)\n",
    "    for i in range(m):\n",
    "        A[i,i]     = 1.0\n",
    "        A[i,:m]   -= P_pi[i,:]\n",
    "        A[i,m]     = 1.0\n",
    "        b[i]       = r_pi[i]\n",
    "    # фиксация h[m-1] = 0\n",
    "    A[m, m-1] = 1.0\n",
    "    b[m]      = 0.0\n",
    "\n",
    "    x = np.linalg.solve(A, b)\n",
    "    h = x[:m]\n",
    "    g = x[m]\n",
    "    return g, h\n",
    "\n",
    "def improve_policy(policy, h):\n",
    "    \"\"\"Policy improvement\"\"\"\n",
    "    m = num_states\n",
    "    new_pol = policy.copy()\n",
    "    for i in range(m):\n",
    "        best_q, best_a = -1e9, None\n",
    "        for a in actions:\n",
    "            # считаем Q(i,a) = r(i,a) + sum_j P[a][i][j]·h[j]\n",
    "            q = 0.0\n",
    "            for j in range(m):\n",
    "                q += P[a][i][j] * R[a][i][j]   # r(i,a)\n",
    "                q += P[a][i][j] * h[j]         # влияние bias\n",
    "            if q > best_q:\n",
    "                best_q, best_a = q, a\n",
    "        new_pol[i] = best_a\n",
    "    return new_pol\n",
    "\n",
    "def policy_iteration():\n",
    "    policy = [0]*num_states     # стартуем, например, всегда со \"скидки\"\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        #  -- вывод текущей политики --\n",
    "        print(f\"\\nИтерация {iteration}. Текущая политика:\")\n",
    "        for i, a in enumerate(policy):\n",
    "            print(f\"  {i} -> {a}\")\n",
    "\n",
    "        # Оценка этой политики\n",
    "        g, h = evaluate_policy(policy)\n",
    "        print(f\"  Оценка: средний доход g = {g:.4f}\")\n",
    "\n",
    "        # Улучшаем политику\n",
    "        new_pol = improve_policy(policy, h)\n",
    "\n",
    "        # Если не изменилось — готово\n",
    "        if new_pol == policy:\n",
    "            print(\"\\nПолитика не изменилась. Алгоритм завершён.\")\n",
    "            break\n",
    "\n",
    "        policy = new_pol\n",
    "        iteration += 1\n",
    "\n",
    "    return policy, g, h\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt_policy, opt_gain, opt_h = policy_iteration()\n",
    "    print(\"\\n=== Итоговая оптимальная политика ===\")\n",
    "    for i, a in enumerate(opt_policy):\n",
    "        print(f\"{i} -> {a}\")\n",
    "    print(f\"Оптимальный средний доход g* = {opt_gain:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e1ec5",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5969d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Итерация 0, текущая политика:\n",
      "  0 -> 0\n",
      "  1 -> 0\n",
      "  2 -> 0\n",
      "   V = [734.13  713.251 655.289]\n",
      "\n",
      "-- Итерация 1, текущая политика:\n",
      "  0 -> 1\n",
      "  1 -> 1\n",
      "  2 -> 2\n",
      "   V = [842.748 823.777 789.711]\n",
      "\n",
      "Политика не изменилась, алгоритм завершён.\n",
      "\n",
      "=== Оптимальная политика ===\n",
      "0 -> 1\n",
      "1 -> 1\n",
      "2 -> 2\n",
      "Стоимость состояний V* = [842.748 823.777 789.711]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Задаём P и R по условию\n",
    "P = {\n",
    "    0: [[0.3,0.5,0.2],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.2,0.7]],\n",
    "    1: [[0.2,0.7,0.1],\n",
    "        [0.1,0.4,0.5],\n",
    "        [0.1,0.2,0.7]],\n",
    "    2: [[0.3,0.4,0.3],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.3,0.6]],\n",
    "}\n",
    "\n",
    "R = {\n",
    "    0: [[110,100,70],\n",
    "        [100, 80,50],\n",
    "        [ 80, 60,40]],\n",
    "    1: [[120,100,70],\n",
    "        [110,100,90],\n",
    "        [100, 70,60]],\n",
    "    2: [[110, 80,50],\n",
    "        [100, 60,40],\n",
    "        [ 80, 70,60]],\n",
    "}\n",
    "\n",
    "num_states   = 3\n",
    "actions      = [0,1,2]     # 0=скидка,1=доставка,2=ничего\n",
    "state_names  = [\"Отл.\",\"Хор.\",\"Уд.\"]\n",
    "action_names = [\"3% скидка\",\"доставка\",\"ничего\"]\n",
    "\n",
    "γ = 0.9  # коэффициент дисконтирования\n",
    "\n",
    "def evaluate_policy_discount(policy, gamma=γ):\n",
    "    \"\"\"\n",
    "    Решаем (I - γ Pπ) V = rπ\n",
    "    возвращаем вектор V размера m.\n",
    "    \"\"\"\n",
    "    m = num_states\n",
    "    Pπ = np.zeros((m,m))\n",
    "    rπ = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        a = policy[i]\n",
    "        for j in range(m):\n",
    "            Pπ[i,j] = P[a][i][j]\n",
    "            rπ[i]  += P[a][i][j] * R[a][i][j]\n",
    "    # матрица I - γ·Pπ\n",
    "    A = np.eye(m) - gamma * Pπ\n",
    "    V = np.linalg.solve(A, rπ)\n",
    "    return V\n",
    "\n",
    "def improve_policy_discount(policy, V, gamma=γ):\n",
    "    \"\"\"\n",
    "    Для каждого состояния i находим a, максимизирующее\n",
    "       Q(i,a) = r(i,a) + γ ∑_j P[a][i][j]·V[j]\n",
    "    \"\"\"\n",
    "    m = num_states\n",
    "    new_pol = policy.copy()\n",
    "    for i in range(m):\n",
    "        best_q, best_a = -1e9, None\n",
    "        for a in actions:\n",
    "            # r(i,a) + γ P[a][i]·V\n",
    "            q = 0.0\n",
    "            # мгновенное ожидание r(i,a)\n",
    "            for j in range(m):\n",
    "                q += P[a][i][j] * R[a][i][j]\n",
    "            # плюс дисконтированное будущее\n",
    "            for j in range(m):\n",
    "                q += gamma * P[a][i][j] * V[j]\n",
    "            if q > best_q:\n",
    "                best_q, best_a = q, a\n",
    "        new_pol[i] = best_a\n",
    "    return new_pol\n",
    "\n",
    "def policy_iteration_discount():\n",
    "    # стартуем, например, всегда «3% скидка»\n",
    "    policy = [0]*num_states\n",
    "    it = 0\n",
    "    while True:\n",
    "        print(f\"\\n-- Итерация {it}, текущая политика:\")\n",
    "        for i, a in enumerate(policy):\n",
    "            print(f\"  {i} -> {a}\")\n",
    "        # оценка\n",
    "        V = evaluate_policy_discount(policy)\n",
    "        print(\"   V =\", np.round(V,3))\n",
    "        # улучшение\n",
    "        new_pol = improve_policy_discount(policy, V)\n",
    "        if new_pol == policy:\n",
    "            print(\"\\nПолитика не изменилась, алгоритм завершён.\")\n",
    "            break\n",
    "        policy = new_pol\n",
    "        it += 1\n",
    "    return policy, V\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt_pol, opt_V = policy_iteration_discount()\n",
    "    print(\"\\n=== Оптимальная политика ===\")\n",
    "    for i, a in enumerate(opt_policy):\n",
    "        print(f\"{i} -> {a}\")\n",
    "    print(\"Стоимость состояний V* =\", np.round(opt_V,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50345f",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bd3fdd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оптимальное среднее вознаграждение g* = 80.8642\n",
      "\n",
      "Оптимальная стан. детермин. политика:\n",
      "  0 → 1\n",
      "  1 → 1\n",
      "  2 → 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# 1) Задаём матрицы P и R по условию\n",
    "P = {\n",
    "    0: [[0.3,0.5,0.2],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.2,0.7]],\n",
    "    1: [[0.2,0.7,0.1],\n",
    "        [0.1,0.4,0.5],\n",
    "        [0.1,0.2,0.7]],\n",
    "    2: [[0.3,0.4,0.3],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.3,0.6]],\n",
    "}\n",
    "\n",
    "R = {\n",
    "    0: [[110,100,70],\n",
    "        [100, 80,50],\n",
    "        [ 80, 60,40]],\n",
    "    1: [[120,100,70],\n",
    "        [110,100,90],\n",
    "        [100, 70,60]],\n",
    "    2: [[110, 80,50],\n",
    "        [100, 60,40],\n",
    "        [ 80, 70,60]],\n",
    "}\n",
    "\n",
    "state_names  = [\"Отличный\",\"Хороший\",\"Удовлетворительный\"]\n",
    "action_names = [\"3% скидка\",\"Бесплатная доставка\",\"Ничего\"]\n",
    "\n",
    "m = 3               # число состояний\n",
    "A = 3               # число действий\n",
    "n_vars = m * A      # 9\n",
    "\n",
    "# 2) Формируем вектор r (размер 9)\n",
    "r = np.zeros(n_vars)\n",
    "for s in range(m):\n",
    "    for a in range(A):\n",
    "        idx = s*A + a\n",
    "        # ожидаемый доход при (s,a)\n",
    "        r[idx] = sum(P[a][s][s2] * R[a][s][s2] for s2 in range(m))\n",
    "\n",
    "# 3) Составляем матрицу A_eq и вектор b_eq\n",
    "#    Первые m строк — flow-balance, последняя строка — нормировка\n",
    "A_eq = np.zeros((m+1, n_vars))\n",
    "b_eq = np.zeros(m+1)\n",
    "\n",
    "# a) flow-balance: для каждого j: sum_a x_{j,a} - sum_{s,a} P[a][s][j]*x_{s,a} = 0\n",
    "for j in range(m):\n",
    "    for s in range(m):\n",
    "        for a in range(A):\n",
    "            idx = s*A + a\n",
    "            if s == j:\n",
    "                A_eq[j, idx] += 1.0\n",
    "            A_eq[j, idx] -= P[a][s][j]\n",
    "\n",
    "# b) нормировка sum x_{s,a} = 1\n",
    "A_eq[m, :] = 1.0\n",
    "b_eq[m]    = 1.0\n",
    "\n",
    "# 4) решаем LP: maximize r^T x  <=>  minimize -r^T x\n",
    "res = linprog(\n",
    "    c = -r,\n",
    "    A_eq = A_eq,\n",
    "    b_eq = b_eq,\n",
    "    bounds = [(0, None)] * n_vars,\n",
    "    method = 'highs'      # можно попробовать 'revised simplex'\n",
    ")\n",
    "\n",
    "if not res.success:\n",
    "    raise RuntimeError(\"LP не сошлось: \" + res.message)\n",
    "\n",
    "x_opt = res.x\n",
    "g_opt = r.dot(x_opt)\n",
    "\n",
    "# 5) Восстанавливаем политику: для каждого состояния берем аргмакс по x_{s,a}\n",
    "policy = []\n",
    "for s in range(m):\n",
    "    values = [ x_opt[s*A + a] for a in range(A) ]\n",
    "    best_a = int(np.argmax(values))\n",
    "    policy.append(best_a)\n",
    "\n",
    "# 6) Выводим результат\n",
    "print(f\"Оптимальное среднее вознаграждение g* = {g_opt:.4f}\\n\")\n",
    "print(\"Оптимальная стан. детермин. политика:\")\n",
    "for s in range(m):\n",
    "    print(f\"  {s} → {policy[s]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b26f56",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "deeeb4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оптимальный дисконтированный доход V* = 842.7485\n",
      "\n",
      "Оптимальная политика:\n",
      "0 → 1\n",
      "1 → 1\n",
      "2 → 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# 1) Данные задачи\n",
    "P = {\n",
    "    0: [[0.3,0.5,0.2],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.2,0.7]],\n",
    "    1: [[0.2,0.7,0.1],\n",
    "        [0.1,0.4,0.5],\n",
    "        [0.1,0.2,0.7]],\n",
    "    2: [[0.3,0.4,0.3],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.3,0.6]],\n",
    "}\n",
    "R = {\n",
    "    0: [[110,100,70],\n",
    "        [100, 80,50],\n",
    "        [ 80, 60,40]],\n",
    "    1: [[120,100,70],\n",
    "        [110,100,90],\n",
    "        [100, 70,60]],\n",
    "    2: [[110, 80,50],\n",
    "        [100, 60,40],\n",
    "        [ 80, 70,60]],\n",
    "}\n",
    "\n",
    "m = 3       # число состояний\n",
    "A = 3       # число действий\n",
    "gamma = 0.9 # дисконт-фактор\n",
    "# стартуем из состояния \"Отличный\"\n",
    "d0 = np.array([1.0, 0.0, 0.0])\n",
    "\n",
    "# 2) Построим вектор r размером m*A\n",
    "r = np.zeros(m*A)\n",
    "for s in range(m):\n",
    "    for a in range(A):\n",
    "        idx = s*A + a\n",
    "        # ожидаемый доход при паре (s,a)\n",
    "        r[idx] = sum(P[a][s][j] * R[a][s][j] for j in range(m))\n",
    "\n",
    "# 3) Матрица равенств A_eq y = b_eq\n",
    "#    m уравнений — по каждому состоянию j\n",
    "A_eq = np.zeros((m, m*A))\n",
    "b_eq = d0.copy()\n",
    "\n",
    "for j in range(m):\n",
    "    for s in range(m):\n",
    "        for a in range(A):\n",
    "            idx = s*A + a\n",
    "            # левая часть: + y[j,a]\n",
    "            if s == j:\n",
    "                A_eq[j, idx] += 1.0\n",
    "            # и −γ·P[a][s][j]·y[s,a]\n",
    "            A_eq[j, idx] -= gamma * P[a][s][j]\n",
    "\n",
    "# 4) Решаем LP: maximize r^T y  <=>  minimize -r^T y\n",
    "res = linprog(\n",
    "    c      = -r,\n",
    "    A_eq   = A_eq,\n",
    "    b_eq   = b_eq,\n",
    "    bounds = [(0, None)] * (m*A),\n",
    "    method = 'highs'\n",
    ")\n",
    "if not res.success:\n",
    "    raise RuntimeError(\"LP не сошлось: \" + res.message)\n",
    "\n",
    "y_opt = res.x\n",
    "V_opt = r.dot(y_opt)  # оптимальный дисконтированный доход\n",
    "\n",
    "# 5) Восстанавливаем стратегию: для каждого s берём argmax_a y_opt[s,a]\n",
    "policy = []\n",
    "for s in range(m):\n",
    "    arr = [y_opt[s*A + a] for a in range(A)]\n",
    "    policy.append(int(np.argmax(arr)))\n",
    "\n",
    "# 6) Выводим\n",
    "state_names  = [\"Отличный\",\"Хороший\",\"Удовлетворительный\"]\n",
    "action_names = [\"3% скидка\",\"Бесплатная доставка\",\"Ничего\"]\n",
    "print(f\"Оптимальный дисконтированный доход V* = {V_opt:.4f}\\n\")\n",
    "print(\"Оптимальная политика:\")\n",
    "for s,a in enumerate(policy):\n",
    "    print(f\"{s} → {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf9b85",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "21a7a47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Результат LP ===\n",
      "Status: Optimal\n",
      "Оптимальный V* =  842.7484618000001\n",
      "\n",
      "Оптимальная политика:\n",
      "  Состояние 0 → действие 1  (y=2.0879)\n",
      "  Состояние 1 → действие 1  (y=3.7930)\n",
      "  Состояние 2 → действие 2  (y=4.1191)\n",
      "\n",
      "=== Dual variables (shadow prices) для уравнений потока ===\n",
      "  flow_state_0     π =  842.7485\n",
      "  flow_state_1     π =  823.7773\n",
      "  flow_state_2     π =  789.7114\n",
      "\n",
      "=== Reduced costs для y[s,a] ===\n",
      "  y[0,0] = 0.0000    reduced cost = -5.3585\n",
      "  y[0,1] = 2.0879    reduced cost =  0.0000\n",
      "  y[0,2] = 0.0000    reduced cost = -25.4245\n",
      "  y[1,0] = 0.0000    reduced cost = -7.0948\n",
      "  y[1,1] = 3.7930    reduced cost = -0.0000\n",
      "  y[1,2] = 0.0000    reduced cost = -21.0948\n",
      "  y[2,0] = 0.0000    reduced cost = -20.0659\n",
      "  y[2,1] = 0.0000    reduced cost = -2.0659\n",
      "  y[2,2] = 4.1191    reduced cost = -0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pulp\n",
    "\n",
    "# 1) Данные MDP\n",
    "P = {\n",
    "    0: [[0.3,0.5,0.2],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.2,0.7]],\n",
    "    1: [[0.2,0.7,0.1],\n",
    "        [0.1,0.4,0.5],\n",
    "        [0.1,0.2,0.7]],\n",
    "    2: [[0.3,0.4,0.3],\n",
    "        [0.2,0.6,0.2],\n",
    "        [0.1,0.3,0.6]],\n",
    "}\n",
    "R = {\n",
    "    0: [[110,100,70],\n",
    "        [100, 80,50],\n",
    "        [ 80, 60,40]],\n",
    "    1: [[120,100,70],\n",
    "        [110,100,90],\n",
    "        [100, 70,60]],\n",
    "    2: [[110, 80,50],\n",
    "        [100, 60,40],\n",
    "        [ 80, 70,60]],\n",
    "}\n",
    "\n",
    "m     = 3               # состояний\n",
    "A     = 3               # действий\n",
    "gamma = 0.9             # дисконт\n",
    "d0    = [1.0, 0.0, 0.0]  # стартуем из «Отличного»\n",
    "\n",
    "# 2) Предсчитаем «моментный» r[s,a]\n",
    "r = {}\n",
    "for s in range(m):\n",
    "    for a in range(A):\n",
    "        r[s,a] = sum(P[a][s][j] * R[a][s][j] for j in range(m))\n",
    "\n",
    "# 3) Формируем LP через PuLP\n",
    "model = pulp.LpProblem(\"Discounted_MDP\", pulp.LpMaximize)\n",
    "\n",
    "# переменные y[s,a] >= 0\n",
    "y = {(s,a): pulp.LpVariable(f\"y_{s}_{a}\", lowBound=0) \n",
    "     for s in range(m) for a in range(A)}\n",
    "\n",
    "# --- целевая функция: max sum r[s,a]*y[s,a]\n",
    "model += pulp.lpSum(r[s,a] * y[s,a] for s in range(m) for a in range(A)), \"Obj\"\n",
    "\n",
    "# --- баланс: для каждого j: sum_a y[j,a] - γ sum_{s,a} P[a][s][j]*y[s,a] == d0[j]\n",
    "for j in range(m):\n",
    "    expr = pulp.lpSum(y[j,a] for a in range(A)) \\\n",
    "           - gamma * pulp.lpSum(P[a][s][j] * y[s,a] \n",
    "                               for s in range(m) for a in range(A))\n",
    "    model += (expr == d0[j], f\"flow_state_{j}\")\n",
    "\n",
    "# --- решаем\n",
    "model.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "\n",
    "# 4) Собираем результаты\n",
    "print(\"=== Результат LP ===\")\n",
    "print(\"Status:\", pulp.LpStatus[model.status])\n",
    "print(\"Оптимальный V* = \", pulp.value(model.objective))\n",
    "print()\n",
    "\n",
    "# оптимальная политика\n",
    "print(\"Оптимальная политика:\")\n",
    "for s in range(m):\n",
    "    best_a, best_val = None, -1e9\n",
    "    for a in range(A):\n",
    "        val = y[s,a].value()\n",
    "        if val > best_val:\n",
    "            best_val, best_a = val, a\n",
    "    print(f\"  Состояние {s} → действие {best_a}  (y={best_val:.4f})\")\n",
    "print()\n",
    "\n",
    "# 5) Анализ чувствительности\n",
    "print(\"=== Dual variables (shadow prices) для уравнений потока ===\")\n",
    "for cname, con in model.constraints.items():\n",
    "    # .pi — дуальная переменная (shadow price)\n",
    "    print(f\"  {cname:15s}  π = {con.pi: .4f}\")\n",
    "\n",
    "print()\n",
    "print(\"=== Reduced costs для y[s,a] ===\")\n",
    "for (s,a), var in y.items():\n",
    "    # var.dj — reduced cost\n",
    "    print(f\"  y[{s},{a}] = {var.value():.4f}    reduced cost = {var.dj: .4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
