{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "96ec2a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для начального состояния «Отличный»:\n",
      "  Лучшая стратегия действий: (1, 1, 1)\n",
      "  Ожидаемое вознаграждение: 278.40\n",
      "\n",
      "Для начального состояния «Хороший»:\n",
      "  Лучшая стратегия действий: (1, 1, 1)\n",
      "  Ожидаемое вознаграждение: 257.25\n",
      "\n",
      "Для начального состояния «Удовлетворительный»:\n",
      "  Лучшая стратегия действий: (2, 1, 1)\n",
      "  Ожидаемое вознаграждение: 222.65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# Матрицы вероятностей переходов P[action][state_from][state_to]\n",
    "transition_probabilities = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "# Матрицы вознаграждений R[action][state_from][state_to]\n",
    "rewards = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "actions = [0, 1, 2]  # Доступные действия (индексы)\n",
    "states = [0, 1, 2]   # Индексы состояний: 0 - «Отличный», 1 - «Хороший», 2 - «Удовлетворительный»\n",
    "months = 3  # Количество месяцев (длительность стратегии)\n",
    "state_names = [\"Отличный\", \"Хороший\", \"Удовлетворительный\"]  # Человекочитаемые имена состояний\n",
    "\n",
    "def calculate_expected_reward(strategy, initial_state):\n",
    "    \"\"\"\n",
    "    Вычисление ожидаемого вознаграждения для заданной стратегии действий.\n",
    "    \n",
    "    Параметры:\n",
    "    strategy (list[int]): Стратегия действий (порядок действий на каждый месяц).\n",
    "    initial_state (int): Начальное состояние системы.\n",
    "    \n",
    "    Возвращает:\n",
    "    float: Ожидаемое вознаграждение для стратегии.\n",
    "    \"\"\"\n",
    "    # Инициализация вероятностей состояний: на старте мы находимся в начальном состоянии\n",
    "    state_probabilities = [0.0, 0.0, 0.0]\n",
    "    state_probabilities[initial_state] = 1.0\n",
    "    \n",
    "    total_reward = 0.0  # Общее ожидаемое вознаграждение\n",
    "    \n",
    "    # Процесс для каждого действия в стратегии\n",
    "    for action in strategy:\n",
    "        new_state_probabilities = [0.0, 0.0, 0.0]  # Вероятности для нового состояния\n",
    "        # Проходим по всем возможным переходам между состояниями\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                transition_probability = transition_probabilities[action][i][j]\n",
    "                reward = rewards[action][i][j]\n",
    "                total_reward += state_probabilities[i] * transition_probability * reward\n",
    "                new_state_probabilities[j] += state_probabilities[i] * transition_probability\n",
    "        \n",
    "        # Обновляем вероятности состояний для следующего шага\n",
    "        state_probabilities = new_state_probabilities\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "# Поиск наилучшей стратегии для каждого начального состояния\n",
    "for initial_state in range(3):\n",
    "    best_strategy = None\n",
    "    best_reward = -1e9  # Начальная очень низкая оценка\n",
    "\n",
    "    # Генерация всех возможных стратегий на протяжении заданного количества месяцев\n",
    "    for strategy in itertools.product(actions, repeat=months):\n",
    "        expected_reward = calculate_expected_reward(strategy, initial_state)\n",
    "        # Обновляем лучшую стратегию, если текущая дает большее вознаграждение\n",
    "        if expected_reward > best_reward:\n",
    "            best_strategy = strategy\n",
    "            best_reward = expected_reward\n",
    "    \n",
    "    # Выводим результаты для каждого начального состояния\n",
    "    print(f\"Для начального состояния «{state_names[initial_state]}»:\")\n",
    "    print(f\"  Лучшая стратегия действий: {best_strategy}\")\n",
    "    print(f\"  Ожидаемое вознаграждение: {best_reward:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e245d2b6",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "905e4aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшая стационарная стратегия для состояний (0, 1, 2): (1, 1, 2)\n",
      "Стационарное распределение состояний μ: [0.111 0.383 0.506]\n",
      "Средний доход g при лучшей стратегии: 80.86\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# Матрицы переходов и доходов для каждого действия\n",
    "transition_matrix = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "reward_matrix = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "actions = [0, 1, 2]  # Индексы доступных действий\n",
    "states = [0, 1, 2]   # Состояния: 0 - «Отличный», 1 - «Хороший», 2 - «Удовлетворительный»\n",
    "\n",
    "def evaluate_policy(policy):\n",
    "    \"\"\"\n",
    "    Оценка политики: вычисление среднего дохода и стационарного распределения состояний.\n",
    "    \n",
    "    Параметры:\n",
    "    policy (list[int]): Стратегия действий для каждого состояния.\n",
    "    \n",
    "    Возвращает:\n",
    "    float: Средний доход от применения политики.\n",
    "    numpy.ndarray: Стационарное распределение состояний.\n",
    "    \"\"\"\n",
    "    # Инициализация матрицы переходов и вектора вознаграждений для данной политики\n",
    "    transition_matrix_policy = np.zeros((3, 3))\n",
    "    reward_vector_policy = np.zeros(3)\n",
    "    \n",
    "    # Для каждого состояния, вычисляем ожидаемое вознаграждение и вероятности перехода\n",
    "    for state in states:\n",
    "        action = policy[state]\n",
    "        transition_matrix_policy[state, :] = transition_matrix[action][state]\n",
    "        for next_state in states:\n",
    "            reward_vector_policy[state] += transition_matrix[action][state][next_state] * reward_matrix[action][state][next_state]\n",
    "\n",
    "    # Решение задачи с собственными значениями для нахождения стационарного распределения\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(transition_matrix_policy.T)\n",
    "    stationary_state_idx = np.argmin(np.abs(eigenvalues - 1.0))\n",
    "    stationary_distribution = np.real(eigenvectors[:, stationary_state_idx])\n",
    "    \n",
    "    # Нормировка стационарного распределения\n",
    "    stationary_distribution /= stationary_distribution.sum()\n",
    "\n",
    "    # Рассчитываем средний доход\n",
    "    average_reward = float(np.dot(stationary_distribution, reward_vector_policy))\n",
    "    return average_reward, stationary_distribution\n",
    "\n",
    "# Инициализация переменных для поиска лучшей политики\n",
    "best_policy = None\n",
    "best_gain = -1e9  # Начальное значение для максимального дохода\n",
    "best_stationary_distribution = None\n",
    "\n",
    "# Перебор всех возможных политик и выбор лучшей\n",
    "for policy in itertools.product(actions, repeat=3):\n",
    "    gain, stationary_distribution = evaluate_policy(policy)\n",
    "    if gain > best_gain:\n",
    "        best_gain = gain\n",
    "        best_policy = policy\n",
    "        best_stationary_distribution = stationary_distribution\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Лучшая стационарная стратегия для состояний (0, 1, 2):\", best_policy)\n",
    "print(\"Стационарное распределение состояний μ:\", np.round(best_stationary_distribution, 3))\n",
    "print(\"Средний доход g при лучшей стратегии:\", round(best_gain, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaadd10",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "69cfd249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Итерация 0. Текущая политика:\n",
      "  0 -> 0\n",
      "  1 -> 0\n",
      "  2 -> 0\n",
      "  Оценка: средний доход g = 69.3778\n",
      "\n",
      "Итерация 1. Текущая политика:\n",
      "  0 -> 1\n",
      "  1 -> 0\n",
      "  2 -> 2\n",
      "  Оценка: средний доход g = 77.9322\n",
      "\n",
      "Итерация 2. Текущая политика:\n",
      "  0 -> 1\n",
      "  1 -> 1\n",
      "  2 -> 2\n",
      "  Оценка: средний доход g = 80.8642\n",
      "\n",
      "Политика не изменилась. Алгоритм завершён.\n",
      "\n",
      "=== Итоговая оптимальная политика ===\n",
      "0 -> 1\n",
      "1 -> 1\n",
      "2 -> 2\n",
      "Оптимальный средний доход g* = 80.8642\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Задаём P и R по условию задачи\n",
    "P = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "R = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "num_states = 3\n",
    "actions = [0, 1, 2]  # 0=скидка, 1=доставка, 2=ничего\n",
    "state_names = [\"Отличный\", \"Хороший\", \"Удовлетворительный\"]\n",
    "action_names = [\"3% скидка\", \"Бесплатная доставка\", \"Ничего\"]\n",
    "\n",
    "def evaluate_policy(policy):\n",
    "    \"\"\"Policy evaluation (gain g и bias h)\"\"\"\n",
    "    m = num_states\n",
    "    # 1) Собираем P_pi и r_pi\n",
    "    P_pi = np.zeros((m, m))\n",
    "    r_pi = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        a = policy[i]\n",
    "        for j in range(m):\n",
    "            P_pi[i, j] = P[a][i][j]\n",
    "            r_pi[i] += P[a][i][j] * R[a][i][j]\n",
    "\n",
    "    # 2) Строим и решаем систему (m+1)x(m+1) на [h0..h_{m-1}, g]\n",
    "    A = np.zeros((m + 1, m + 1))\n",
    "    b = np.zeros(m + 1)\n",
    "    for i in range(m):\n",
    "        A[i, i] = 1.0\n",
    "        A[i, :m] -= P_pi[i, :]\n",
    "        A[i, m] = 1.0\n",
    "        b[i] = r_pi[i]\n",
    "    # фиксация h[m-1] = 0\n",
    "    A[m, m - 1] = 1.0\n",
    "    b[m] = 0.0\n",
    "\n",
    "    x = np.linalg.solve(A, b)\n",
    "    h = x[:m]\n",
    "    g = x[m]\n",
    "    return g, h\n",
    "\n",
    "def improve_policy(policy, h):\n",
    "    \"\"\"Policy improvement\"\"\"\n",
    "    m = num_states\n",
    "    new_pol = policy.copy()\n",
    "    for i in range(m):\n",
    "        best_q, best_a = -1e9, None\n",
    "        for a in actions:\n",
    "            # считаем Q(i,a) = r(i,a) + sum_j P[a][i][j]·h[j]\n",
    "            q = 0.0\n",
    "            for j in range(m):\n",
    "                q += P[a][i][j] * R[a][i][j]  # r(i,a)\n",
    "                q += P[a][i][j] * h[j]  # влияние bias\n",
    "            if q > best_q:\n",
    "                best_q, best_a = q, a\n",
    "        new_pol[i] = best_a\n",
    "    return new_pol\n",
    "\n",
    "def policy_iteration():\n",
    "    policy = [0] * num_states  # стартуем, например, всегда со \"скидки\"\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        #  -- вывод текущей политики --\n",
    "        print(f\"\\nИтерация {iteration}. Текущая политика:\")\n",
    "        for i, a in enumerate(policy):\n",
    "            print(f\"  {i} -> {a}\")\n",
    "\n",
    "        # Оценка этой политики\n",
    "        g, h = evaluate_policy(policy)\n",
    "        print(f\"  Оценка: средний доход g = {g:.4f}\")\n",
    "\n",
    "        # Улучшаем политику\n",
    "        new_pol = improve_policy(policy, h)\n",
    "\n",
    "        # Если не изменилось — готово\n",
    "        if new_pol == policy:\n",
    "            print(\"\\nПолитика не изменилась. Алгоритм завершён.\")\n",
    "            break\n",
    "\n",
    "        policy = new_pol\n",
    "        iteration += 1\n",
    "\n",
    "    return policy, g, h\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt_policy, opt_gain, opt_h = policy_iteration()\n",
    "    print(\"\\n=== Итоговая оптимальная политика ===\")\n",
    "    for i, a in enumerate(opt_policy):\n",
    "        print(f\"{i} -> {a}\")\n",
    "    print(f\"Оптимальный средний доход g* = {opt_gain:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e1ec5",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f5969d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Итерация 0, текущая политика:\n",
      "  0 -> 0\n",
      "  1 -> 0\n",
      "  2 -> 0\n",
      "   V = [734.13  713.251 655.289]\n",
      "\n",
      "-- Итерация 1, текущая политика:\n",
      "  0 -> 1\n",
      "  1 -> 1\n",
      "  2 -> 2\n",
      "   V = [842.748 823.777 789.711]\n",
      "\n",
      "Политика не изменилась, алгоритм завершён.\n",
      "\n",
      "=== Оптимальная политика ===\n",
      "0 -> 1\n",
      "1 -> 1\n",
      "2 -> 2\n",
      "Стоимость состояний V* = [842.748 823.777 789.711]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Задаём P и R по условию\n",
    "P = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "R = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "num_states = 3\n",
    "actions = [0, 1, 2]  # 0=скидка, 1=доставка, 2=ничего\n",
    "state_names = [\"Отл.\", \"Хор.\", \"Уд.\"]\n",
    "action_names = [\"3% скидка\", \"доставка\", \"ничего\"]\n",
    "\n",
    "γ = 0.9  # коэффициент дисконтирования\n",
    "\n",
    "def evaluate_policy_discount(policy, gamma=γ):\n",
    "    \"\"\"\n",
    "    Решаем (I - γ Pπ) V = rπ\n",
    "    возвращаем вектор V размера m.\n",
    "    \"\"\"\n",
    "    m = num_states\n",
    "    Pπ = np.zeros((m, m))\n",
    "    rπ = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        a = policy[i]\n",
    "        for j in range(m):\n",
    "            Pπ[i, j] = P[a][i][j]\n",
    "            rπ[i] += P[a][i][j] * R[a][i][j]\n",
    "    # матрица I - γ·Pπ\n",
    "    A = np.eye(m) - gamma * Pπ\n",
    "    V = np.linalg.solve(A, rπ)\n",
    "    return V\n",
    "\n",
    "def improve_policy_discount(policy, V, gamma=γ):\n",
    "    \"\"\"\n",
    "    Для каждого состояния i находим a, максимизирующее\n",
    "       Q(i,a) = r(i,a) + γ ∑_j P[a][i][j]·V[j]\n",
    "    \"\"\"\n",
    "    m = num_states\n",
    "    new_pol = policy.copy()\n",
    "    for i in range(m):\n",
    "        best_q, best_a = -1e9, None\n",
    "        for a in actions:\n",
    "            # r(i,a) + γ P[a][i]·V\n",
    "            q = 0.0\n",
    "            # мгновенное ожидание r(i,a)\n",
    "            for j in range(m):\n",
    "                q += P[a][i][j] * R[a][i][j]\n",
    "            # плюс дисконтированное будущее\n",
    "            for j in range(m):\n",
    "                q += gamma * P[a][i][j] * V[j]\n",
    "            if q > best_q:\n",
    "                best_q, best_a = q, a\n",
    "        new_pol[i] = best_a\n",
    "    return new_pol\n",
    "\n",
    "def policy_iteration_discount():\n",
    "    # стартуем, например, всегда «3% скидка»\n",
    "    policy = [0] * num_states\n",
    "    it = 0\n",
    "    while True:\n",
    "        print(f\"\\n-- Итерация {it}, текущая политика:\")\n",
    "        for i, a in enumerate(policy):\n",
    "            print(f\"  {i} -> {a}\")\n",
    "        # оценка\n",
    "        V = evaluate_policy_discount(policy)\n",
    "        print(\"   V =\", np.round(V, 3))\n",
    "        # улучшение\n",
    "        new_pol = improve_policy_discount(policy, V)\n",
    "        if new_pol == policy:\n",
    "            print(\"\\nПолитика не изменилась, алгоритм завершён.\")\n",
    "            break\n",
    "        policy = new_pol\n",
    "        it += 1\n",
    "    return policy, V\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    opt_pol, opt_V = policy_iteration_discount()\n",
    "    print(\"\\n=== Оптимальная политика ===\")\n",
    "    for i, a in enumerate(opt_pol):\n",
    "        print(f\"{i} -> {a}\")\n",
    "    print(\"Стоимость состояний V* =\", np.round(opt_V, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50345f",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bd3fdd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оптимальное среднее вознаграждение g* = 80.8642\n",
      "\n",
      "Оптимальная стационарная детерминированная политика:\n",
      "  0 → 1\n",
      "  1 → 1\n",
      "  2 → 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# 1. Определяем матрицы вероятностей P и вознаграждений R по условию задачи\n",
    "\n",
    "# P[a][s][s'] - вероятность перехода из состояния s в s' при действии a\n",
    "P = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "# R[a][s][s'] - вознаграждение за переход из состояния s в s' при действии a\n",
    "R = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "# Состояния и действия\n",
    "state_names = [\"Отличный\", \"Хороший\", \"Удовлетворительный\"]\n",
    "action_names = [\"3% скидка\", \"Бесплатная доставка\", \"Ничего\"]\n",
    "\n",
    "# Параметры задачи\n",
    "num_states = 3  # Количество состояний\n",
    "num_actions = 3  # Количество действий\n",
    "num_variables = num_states * num_actions  # Общее количество переменных (9)\n",
    "\n",
    "# 2. Формируем вектор вознаграждений r для каждого состояния и действия\n",
    "reward_vector = np.zeros(num_variables)\n",
    "for state in range(num_states):\n",
    "    for action in range(num_actions):\n",
    "        idx = state * num_actions + action\n",
    "        # Ожидаемое вознаграждение для пары (состояние, действие)\n",
    "        reward_vector[idx] = sum(P[action][state][next_state] * R[action][state][next_state] for next_state in range(num_states))\n",
    "\n",
    "# 3. Формируем матрицу A_eq и вектор b_eq для решения задачи линейного программирования\n",
    "\n",
    "# A_eq и b_eq для условия балансировки потоков и нормировки\n",
    "A_eq = np.zeros((num_states + 1, num_variables))\n",
    "b_eq = np.zeros(num_states + 1)\n",
    "\n",
    "# а) Баланс потоков: для каждого состояния j, учитываем все действия a\n",
    "for next_state in range(num_states):\n",
    "    for state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            idx = state * num_actions + action\n",
    "            if state == next_state:\n",
    "                A_eq[next_state, idx] += 1.0\n",
    "            A_eq[next_state, idx] -= P[action][state][next_state]\n",
    "\n",
    "# б) Нормировка: сумма всех переменных x_{s,a} равна 1\n",
    "A_eq[num_states, :] = 1.0\n",
    "b_eq[num_states] = 1.0\n",
    "\n",
    "# 4. Решаем задачу линейного программирования для максимизации r^T * x\n",
    "# Задача сводится к минимизации -r^T * x\n",
    "result = linprog(\n",
    "    c=-reward_vector,  # Минмизируем -r^T * x\n",
    "    A_eq=A_eq,\n",
    "    b_eq=b_eq,\n",
    "    bounds=[(0, None)] * num_variables,\n",
    "    method='highs'  # Используем метод 'highs', возможен также 'revised simplex'\n",
    ")\n",
    "\n",
    "# Проверка успешности решения задачи\n",
    "if not result.success:\n",
    "    raise RuntimeError(f\"Ошибка в решении LP задачи: {result.message}\")\n",
    "\n",
    "# Оптимальное решение\n",
    "optimal_x = result.x\n",
    "optimal_reward = reward_vector.dot(optimal_x)\n",
    "\n",
    "# 5. Восстанавливаем политику для каждого состояния: выбираем действие с максимальной вероятностью\n",
    "optimal_policy = []\n",
    "for state in range(num_states):\n",
    "    values = [optimal_x[state * num_actions + action] for action in range(num_actions)]\n",
    "    best_action = int(np.argmax(values))  # Действие с максимальной вероятностью\n",
    "    optimal_policy.append(best_action)\n",
    "\n",
    "# 6. Выводим результаты\n",
    "print(f\"Оптимальное среднее вознаграждение g* = {optimal_reward:.4f}\\n\")\n",
    "print(\"Оптимальная стационарная детерминированная политика:\")\n",
    "for state in range(num_states):\n",
    "    print(f\"  {state} → {optimal_policy[state]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b26f56",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "deeeb4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оптимальный дисконтированный доход V* = 842.7485\n",
      "\n",
      "Оптимальная стратегия (политика):\n",
      "0 → 1\n",
      "1 → 1\n",
      "2 → 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# 1. Данные задачи\n",
    "\n",
    "# P[a][s][s'] - вероятность перехода из состояния s в состояние s' при действии a\n",
    "transition_probabilities = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "\n",
    "# R[a][s][s'] - вознаграждение за переход из состояния s в s' при действии a\n",
    "rewards = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "# Количество состояний и действий\n",
    "num_states = 3  # Количество состояний\n",
    "num_actions = 3  # Количество действий\n",
    "discount_factor = 0.9  # Дисконт-фактор\n",
    "\n",
    "# Начальное распределение: стартуем из состояния \"Отличный\"\n",
    "initial_distribution = np.array([1.0, 0.0, 0.0])\n",
    "\n",
    "# 2. Формируем вектор вознаграждений r размером num_states * num_actions\n",
    "reward_vector = np.zeros(num_states * num_actions)\n",
    "for state in range(num_states):\n",
    "    for action in range(num_actions):\n",
    "        idx = state * num_actions + action\n",
    "        # Ожидаемое вознаграждение для состояния и действия (s, a)\n",
    "        reward_vector[idx] = sum(transition_probabilities[action][state][next_state] * rewards[action][state][next_state] \n",
    "                                 for next_state in range(num_states))\n",
    "\n",
    "# 3. Формируем матрицу равенств A_eq и вектор b_eq для линейного программирования\n",
    "\n",
    "# A_eq - матрица коэффициентов, b_eq - вектор правых частей\n",
    "A_eq = np.zeros((num_states, num_states * num_actions))\n",
    "b_eq = initial_distribution.copy()\n",
    "\n",
    "# Формируем систему уравнений для каждого состояния\n",
    "for next_state in range(num_states):\n",
    "    for state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            idx = state * num_actions + action\n",
    "            # Добавляем коэффициент для y[j, a] на левой части\n",
    "            if state == next_state:\n",
    "                A_eq[next_state, idx] += 1.0\n",
    "            # Добавляем коэффициент для -γ·P[a][s][j]·y[s, a] на правой части\n",
    "            A_eq[next_state, idx] -= discount_factor * transition_probabilities[action][state][next_state]\n",
    "\n",
    "# 4. Решаем задачу линейного программирования (LP): maximize r^T * y <=> minimize -r^T * y\n",
    "result = linprog(\n",
    "    c=-reward_vector,  # Минмизируем -r^T * y\n",
    "    A_eq=A_eq,\n",
    "    b_eq=b_eq,\n",
    "    bounds=[(0, None)] * (num_states * num_actions),\n",
    "    method='highs'  # Используем метод 'highs', можно также использовать 'revised simplex'\n",
    ")\n",
    "\n",
    "# Проверяем успешность решения задачи\n",
    "if not result.success:\n",
    "    raise RuntimeError(f\"Ошибка в решении LP задачи: {result.message}\")\n",
    "\n",
    "# Оптимальные значения переменных y\n",
    "optimal_y = result.x\n",
    "optimal_value = reward_vector.dot(optimal_y)  # Оптимальный дисконтированный доход\n",
    "\n",
    "# 5. Восстанавливаем стратегию (политику): для каждого состояния выбираем действие с максимальным y\n",
    "optimal_policy = []\n",
    "for state in range(num_states):\n",
    "    action_values = [optimal_y[state * num_actions + action] for action in range(num_actions)]\n",
    "    optimal_action = int(np.argmax(action_values))  # Действие с максимальной вероятностью\n",
    "    optimal_policy.append(optimal_action)\n",
    "\n",
    "# 6. Выводим результаты\n",
    "state_labels = [\"Отличный\", \"Хороший\", \"Удовлетворительный\"]\n",
    "action_labels = [\"3% скидка\", \"Бесплатная доставка\", \"Ничего\"]\n",
    "\n",
    "# Вывод оптимального дисконтированного дохода\n",
    "print(f\"Оптимальный дисконтированный доход V* = {optimal_value:.4f}\\n\")\n",
    "\n",
    "# Вывод оптимальной стратегии для каждого состояния\n",
    "print(\"Оптимальная стратегия (политика):\")\n",
    "for state, action in enumerate(optimal_policy):\n",
    "    print(f\"{state} → {action}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf9b85",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "21a7a47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Результат LP ===\n",
      "Status: Optimal\n",
      "Оптимальный V* =  842.7484618000001\n",
      "\n",
      "Оптимальная политика:\n",
      "  Состояние 0 → действие 1  (y=2.0879)\n",
      "  Состояние 1 → действие 1  (y=3.7930)\n",
      "  Состояние 2 → действие 2  (y=4.1191)\n",
      "\n",
      "=== Dual variables (shadow prices) для уравнений потока ===\n",
      "  flow_state_0     π =  842.7485\n",
      "  flow_state_1     π =  823.7773\n",
      "  flow_state_2     π =  789.7114\n",
      "\n",
      "=== Reduced costs для y[s,a] ===\n",
      "  y[0,0] = 0.0000    reduced cost = -5.3585\n",
      "  y[0,1] = 2.0879    reduced cost =  0.0000\n",
      "  y[0,2] = 0.0000    reduced cost = -25.4245\n",
      "  y[1,0] = 0.0000    reduced cost = -7.0948\n",
      "  y[1,1] = 3.7930    reduced cost = -0.0000\n",
      "  y[1,2] = 0.0000    reduced cost = -21.0948\n",
      "  y[2,0] = 0.0000    reduced cost = -20.0659\n",
      "  y[2,1] = 0.0000    reduced cost = -2.0659\n",
      "  y[2,2] = 4.1191    reduced cost = -0.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pulp\n",
    "\n",
    "# 1) Данные MDP\n",
    "P = {\n",
    "    0: [[0.3, 0.5, 0.2],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    1: [[0.2, 0.7, 0.1],\n",
    "        [0.1, 0.4, 0.5],\n",
    "        [0.1, 0.2, 0.7]],\n",
    "    2: [[0.3, 0.4, 0.3],\n",
    "        [0.2, 0.6, 0.2],\n",
    "        [0.1, 0.3, 0.6]],\n",
    "}\n",
    "R = {\n",
    "    0: [[110, 100, 70],\n",
    "        [100,  80, 50],\n",
    "        [ 80,  60, 40]],\n",
    "    1: [[120, 100, 70],\n",
    "        [110, 100, 90],\n",
    "        [100,  70, 60]],\n",
    "    2: [[110,  80, 50],\n",
    "        [100,  60, 40],\n",
    "        [ 80,  70, 60]],\n",
    "}\n",
    "\n",
    "m = 3  # состояний\n",
    "A = 3  # действий\n",
    "gamma = 0.9  # дисконт\n",
    "d0 = [1.0, 0.0, 0.0]  # стартуем из «Отличного»\n",
    "\n",
    "# 2) Предсчитаем «моментный» r[s,a]\n",
    "r = {}\n",
    "for s in range(m):\n",
    "    for a in range(A):\n",
    "        r[s, a] = sum(P[a][s][j] * R[a][s][j] for j in range(m))\n",
    "\n",
    "# 3) Формируем LP через PuLP\n",
    "model = pulp.LpProblem(\"Discounted_MDP\", pulp.LpMaximize)\n",
    "\n",
    "# переменные y[s,a] >= 0\n",
    "y = {(s, a): pulp.LpVariable(f\"y_{s}_{a}\", lowBound=0)\n",
    "     for s in range(m) for a in range(A)}\n",
    "\n",
    "# --- целевая функция: max sum r[s,a]*y[s,a]\n",
    "model += pulp.lpSum(r[s, a] * y[s, a] for s in range(m) for a in range(A)), \"Obj\"\n",
    "\n",
    "# --- баланс: для каждого j: sum_a y[j,a] - γ sum_{s,a} P[a][s][j]*y[s,a] == d0[j]\n",
    "for j in range(m):\n",
    "    expr = pulp.lpSum(y[j, a] for a in range(A)) \\\n",
    "           - gamma * pulp.lpSum(P[a][s][j] * y[s, a]\n",
    "                               for s in range(m) for a in range(A))\n",
    "    model += (expr == d0[j], f\"flow_state_{j}\")\n",
    "\n",
    "# --- решаем\n",
    "model.solve(pulp.PULP_CBC_CMD(msg=False))\n",
    "\n",
    "# 4) Собираем результаты\n",
    "print(\"=== Результат LP ===\")\n",
    "print(\"Status:\", pulp.LpStatus[model.status])\n",
    "print(\"Оптимальный V* = \", pulp.value(model.objective))\n",
    "print()\n",
    "\n",
    "# оптимальная политика\n",
    "print(\"Оптимальная политика:\")\n",
    "for s in range(m):\n",
    "    best_a, best_val = None, -1e9\n",
    "    for a in range(A):\n",
    "        val = y[s, a].value()\n",
    "        if val > best_val:\n",
    "            best_val, best_a = val, a\n",
    "    print(f\"  Состояние {s} → действие {best_a}  (y={best_val:.4f})\")\n",
    "print()\n",
    "\n",
    "# 5) Анализ чувствительности\n",
    "print(\"=== Dual variables (shadow prices) для уравнений потока ===\")\n",
    "for cname, con in model.constraints.items():\n",
    "    # .pi — дуальная переменная (shadow price)\n",
    "    print(f\"  {cname:15s}  π = {con.pi: .4f}\")\n",
    "\n",
    "print()\n",
    "print(\"=== Reduced costs для y[s,a] ===\")\n",
    "for (s, a), var in y.items():\n",
    "    # var.dj — reduced cost\n",
    "    print(f\"  y[{s},{a}] = {var.value():.4f}    reduced cost = {var.dj: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f51f0c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Промежутки дисконтирования и политики:\n",
      "γ ∈ [0.10, 0.35] → политика (1, 1, 1)\n",
      "γ ∈ [0.35, 0.99] → политика (1, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# policy_iteration_discount взяли из предыдущего примера, \n",
    "# но сделали функцию, принимающую gamma\n",
    "def policy_iteration_discount(P, R, gamma):\n",
    "    m = len(P[0])\n",
    "    actions = list(P.keys())\n",
    "    policy = [actions[0]]*m\n",
    "    while True:\n",
    "        # evaluate\n",
    "        Pπ = np.zeros((m,m))\n",
    "        rπ = np.zeros(m)\n",
    "        for i in range(m):\n",
    "            a = policy[i]\n",
    "            for j in range(m):\n",
    "                Pπ[i,j] = P[a][i][j]\n",
    "                rπ[i]  += P[a][i][j] * R[a][i][j]\n",
    "        V = np.linalg.solve(np.eye(m) - gamma*Pπ, rπ)\n",
    "        # improve\n",
    "        new_pol = policy.copy()\n",
    "        for i in range(m):\n",
    "            qs = []\n",
    "            for a in actions:\n",
    "                q = sum(P[a][i][j]*R[a][i][j] for j in range(m)) \\\n",
    "                    + gamma*sum(P[a][i][j]*V[j] for j in range(m))\n",
    "                qs.append(q)\n",
    "            new_pol[i] = actions[int(np.argmax(qs))]\n",
    "        if new_pol == policy:\n",
    "            break\n",
    "        policy = new_pol\n",
    "    return policy\n",
    "\n",
    "# Матрицы P и R — как раньше\n",
    "P = {\n",
    "    0: [[0.3,0.5,0.2],[0.2,0.6,0.2],[0.1,0.2,0.7]],\n",
    "    1: [[0.2,0.7,0.1],[0.1,0.4,0.5],[0.1,0.2,0.7]],\n",
    "    2: [[0.3,0.4,0.3],[0.2,0.6,0.2],[0.1,0.3,0.6]],\n",
    "}\n",
    "R = {\n",
    "    0: [[110,100,70],[100,80,50],[80,60,40]],\n",
    "    1: [[120,100,70],[110,100,90],[100,70,60]],\n",
    "    2: [[110,80,50],[100,60,40],[80,70,60]],\n",
    "}\n",
    "\n",
    "# Сканируем γ и фиксируем, какая политика в итоге получилась\n",
    "gammas = np.linspace(0.1,0.99,19)\n",
    "policies = []\n",
    "for γ in gammas:\n",
    "    pol = tuple(policy_iteration_discount(P, R, γ))\n",
    "    policies.append(pol)\n",
    "\n",
    "# Выводим интервалы γ, на которых политика стабильна\n",
    "intervals = []\n",
    "start = gammas[0]\n",
    "prev = policies[0]\n",
    "for g, pol in zip(gammas[1:], policies[1:]):\n",
    "    if pol != prev:\n",
    "        intervals.append((start, prev, g))\n",
    "        start = g\n",
    "        prev = pol\n",
    "# последний кусок\n",
    "intervals.append((start, prev, gammas[-1]))\n",
    "\n",
    "print(\"Промежутки дисконтирования и политики:\")\n",
    "for low, pol, high in intervals:\n",
    "    print(f\"γ ∈ [{low:.2f}, {high:.2f}] → политика {pol}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
